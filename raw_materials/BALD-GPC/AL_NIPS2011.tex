\documentclass{article}

\usepackage{nips11submit_e,times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{array}
\usepackage{microtype}
\usepackage{subfigure}
%\usepackage{natbib}
\usepackage{xfrac}

\newcommand{\x}{\bm{x}}
\newcommand{\y}{\bm{y}}
\newcommand{\upref}{\bm{u}}
\newcommand{\vpref}{\bm{v}}
\newcommand{\data}{\mathcal{D}}
\newcommand{\param}{\bm{\theta}}
\newcommand{\argmax}{ \operatorname*{arg \max}} 
\newcommand{\argmin}{ \operatorname*{arg \min}} 
\newcommand{\ourmethod}{BALD } % nb if change this wll need to change initial ref
\newcommand{\E}{\mathbb{E}}

\definecolor{mycolor1}{rgb}{0.8,0.8,0}
\definecolor{mycolor2}{rgb}{0,1,1}
\definecolor{mycolor3}{rgb}{1,0,1}
\definecolor{mycolor4}{rgb}{1,0.8,0.5}
\definecolor{mycolor5}{rgb}{0.7,0.4,0.01}

\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}   
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}


\title{Information Theoretic Active Learning for Classification and Preference Modelling}

\author{
Neil M. T. Houlsby OR Ferenc Husz\'{a}r tbd\\
Department of Engineering\\
University of Cambridge\\
\texttt{} \\
\And
Neil M. T. Houlsby OR Ferenc Husz\'{a}r tbd\\
Department of Engineering\\
University of Cambridge\\
\texttt{} \\
\AND
Zoubin Ghahramani\\
Department of Engineering\\
University of Cambridge\\
\texttt{zoubin@eng.cam.ac.uk} \\
\And
M\'{a}t\'{e} Lengyel\\
Department of Engineering\\
University of Cambridge\\
\texttt{m.lengyel@eng.cam.ac.uk} \\
}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}

Information theoretic active learning has been widely studied for probabilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other common tasks, such as classification, the optimal solution is more complex. Many approaches have been proposed that include computing approximate posterior entropies, sampling, or using related quantities in non-probabilistic models. The contributions of this paper are threefold: Firstly, we propose an approach that expresses information gain in terms of predictive entropies and discuss the computational advantages this offers compared to other methods. Secondly, we propose a novel algorithm for active learning for the popular Gaussian Process classifier (GPC). Notably our algorithm works with all known approximate inference methods for GPC and allows for active learning of hyperparameters too. Finally we extend the the algorithm to Gaussian process-based binary preference learning.

\end{abstract}

\section{Introduction}
In most machine learning applications, the learner passively collects data with which it makes inferences about its environment. In active learning, however, the learner can seek out the most useful measurements to be trained on. The goal of active learning is to produce the most accurate model with the least possible data; this is closely related to the statistical field of optimal experimental design. With the advent of the internet and expansion in storage facilities vast quantities of unlabelled data have become available, but it is often costly to obtain labels; searching for the most useful data in this vast space calls for efficient active learning algorithms.

Two approaches to active learning utilise decision and information theory \cite{kapoor2007,lindley1956}. The former minimizes the expected losses encountered after making decisions based on the data collected i.e. minimize the Bayes posterior risk \cite{roy2001}. Maximising performance under test is the ultimate objective of most learners, however, evaluating this objective can be very hard. For example the methods proposed in \cite{kapoor2007,zhu2003} for classification are in general very expensive to compute. Furthermore, we may not know the loss function or test distribution in advance, we may want our model to perform well on a wide variety of loss functions. In extreme scenarios, such as exploratory data analysis, or visualisation, losses may be very hard to quantify. This motivates the use of information theoretic active learning which is agnostic to the decision task at hand, but tries to increase model certainty as quickly as possible, usually using Shannon's entropy as a measure of uncertainty. 

In this paper we focus on probabilistic classification, we present an algorithm that applies the full information theoretic criterion to Gaussian Processes Classification (GPC). GPC is powerful, non-parametric kernel-based model, and poses an interesting problem for information-theoretic active learning because parameter space is infinite dimensional and the posterior distribution is analytically intractable. We present the information theoretic approach to active learning in Section 2. In Section 3 we briefly review other approaches and their suitability for classification relative to our information-theoretic approach. We detail our algorithm and show how to extend our GPC approach to yield a novel method for active preference learning (Section 4). We present results on a variety of datasets in Section 5 and conclude the paper in Section 6.

\section{Bayesian Information Theoretic Active Learning}

We consider a fully discriminative model where the goal of active learning is to discover the dependence of some variable $\y\in\mathcal{Y}$ on an input variable $\x\in\mathcal{X}$ by interactively querying the system with inputs $\x_i\in\mathcal{X}$ and observing the system's response $\y_i$. 

Within a Bayesian framework we assume existence of some latent parameters, $\param$, that control the dependence between inputs and outputs, $p(\y\vert\x,\param)$. After having observed data $\data = \{(\x_i,\y_i)\}_{i=1}^n$, we have a posterior distribution over the parameters, $p(\param|\data)$. The goal of information theoretic active learning is to minimize the uncertainty about the parameters using the well studied Shannon's entropy \cite{coverandthomas}, i.e. select a new set of datapoints $\mathcal{D}'$ that satisfy $\argmin_{\mathcal{D}'}H[\param|\mathcal{D}']=-\int p(\theta|\mathcal{D}')\log p(\theta|\mathcal{D}') \mathrm{d}\theta$. Unfortunately, solving this problem in general in NP-hard; however, it has been shown that a myopic policy can perform near-optimally \cite{golovin2010,dasgupta2005}. The myopic strategy sequentially selects points to greedily minimize the objective. Therefore, the objective function, first proposed in \cite{lindley1956} is to seek the a data point $\x$ that satisfies:

\begin{align}	
	\label{eqn:ent_change}
	\argmax_{\x} H[\param | \data] - \E_{\y\sim p(\y|\x\data)} \left[ H[\param| \y, \x,\data] \right] 
\end{align}

Note that expectation over the unseen output $\y$ is required. Eqn. \eqref{eqn:ent_change} poses two difficulties: firstly,  if we search $k$ potential queries,  $\x$, and the output, $\y$, may take on $l$ values, each $kl$ posterior updates are required to compute the objective for each $\x$ in question. Secondly, calculating entropies in parameter space may be hard. Often we may only be able to estimate entropies in parameter space using samples from the posterior, which is notoriously difficult \cite{panzeri2007}; or by gridding up parameter space which scales exponentially with dimensionality of $\param$. Worse still, for non-parametric processes parameter space is infinite dimensional so Eqn. \eqref{eqn:ent_change} becomes poorly defined. \cite{mackay1992, krishnapuram2004, lawrence2004} use this objective but must make approximations to the complicated entropy term. However, if we note that the objective in Eqn. \eqref{eqn:ent_change} is equivalent to the conditional mutual information between the unknown output and the parameters, $I[\theta,\y\vert\x,\data]$ then it is simple to show that the objective can be rearranged to compute entropies in $\y$ space:

\begin{align}
\argmax_{\x} H[\y \vert \x, \data] - \E_{\param\sim p(\param|\data)} \left[ H[\y \vert \x,\param] \right] \label{eqn:rearrangement} 
\end{align}

Eqn. \eqref{eqn:rearrangement} overcomes the aforementioned challenges. Entropies are now calculated in, usually low dimensional, output space. Also $\param$ is now conditioned only on $\data$, so we do not need to update the posterior for every possible outcome, saving a factor of $kl$ posterior updates. Equation \eqref{eqn:rearrangement} provides us with an intuition about the objective; we seek the $\x$ for which the model is marginally most uncertain about $\y$ (high $H[\y \vert \x, \data]$), but for which individual setting of the parameters are confident (low $\E_{\param\sim p(\param|\data)} \left[ H[\y \vert \x,\param] \right]$). This can be interpreted as seeking the $\x$ for which the parameters under the posterior disagree about the outcome the most, so refer to this objective as Bayesian Active Learning by Disagreement (BALD). We present a way to apply Eqn. \eqref{eqn:rearrangement} directly to GPC and preference learning. This method is inductive, i.e. we do not assume anything about the test set as opposed to transductive algorithms which know the distribution of the test data.

\section{Related Methodologies}

In this section we briefly review some of the very many related algorithms that are applicable to classification and relate them to the full information theoretic objective \eqref{eqn:rearrangement}.

\paragraph{Information Theoretic:} Other work that uses rearrangement to data space (Eqn. \eqref{eqn:rearrangement}) include Maximum Entropy Sampling (MES) \cite{sebastiani2000}. MES was proposed for regression models with input-independent observation noise. Although Eqn. \eqref{eqn:rearrangement} is used, the second term is constant because of input independent noise. and so can be ignored. For heteroscedastic regression or classification, MES is inappropriate; it fails to differentiate between model uncertainty and observation uncertainty (about which our model may be confident). Many toy demonstrations show the `information based' active learning criterion performing pathologically in classification by repeatedly querying points close the decision boundary or in regions of high observation uncertainty  e.g. those presented in \cite{dasgupta2008, huang2010}. This is because MES is inappropriate, \ourmethod distinguishes between observatio and model uncertainty and will eliminate these problems as we will show.

Further mutual-information based objective functions are presented in \cite{ertin2003,fuhrmann2003}, who seek to maximise mutual information between the variable being measured and the variable of interest. Fuhrmann \cite{fuhrmann2003} applies this to linear Gaussian models and acoustic arrays, Ertin \emph{et al.} \cite{ertin2003} to a communications channel. Although closely related, these objectives do not work with the model parameters and are not applied to classification. \cite{guestrin2005, krause2006, krause2007} also use mutual information. They specify points of interest in advance and maximise the expected mutual information between the points of interest at the observed locations. Although this is a objective is promising for regression, it is not computable for models with input-dependent observation noise, such as classification; furthermore, it is not inductive.

Finally, the IVM \cite{lawrence2004} algorithm was designed for sub-sampling a dataset to be used to train a GP. It may not fall under the term `active learning' because all $\y$ values are available a priori. Their objective is Eqn. \eqref{eqn:ent_change}, however the algorithm is not based on a rearrangement to data space (Eqn. \eqref{eqn:rearrangement}). Therefore, posterior entropy calculations are made approximately on the $n$ dimensional subspace corresponding to the $n$ observed datapoints using the GP covariance matrix and $kl$ posterior updates are required (\cite{lawrence2004} proposes using Assumed Density Filtering to do this quickly).

We have briefly reviewed several information-theoretic based algorithms, but as far as the authors are aware our paper is the first to develop an efficient algorithm applying the fulll information theoretic criterion (Eqn. \eqref{eqn:rearrangement}) to probabilistic classification.

\paragraph{Decision theoretic:} We briefly mention a few decision theoretic approaches to classification. Two closely related algorithms, \cite{kapoor2007, zhu2003}, seek to minimize the expected misclassification probability on all seen and future data (sometimes with costs associated). These methods observe the locations of the test points and their objective functions are monotonic in the predictive entropies at the test points. \cite{kapoor2007} also includes an empirical error term that can yield pathological behaviour (we investigate this experimentally). These approaches are computationally expensive, requiring $kl$ posterior updates. They are also they are transductive because they look at the locations of the test data; designing an inductive, decision-theoretic algorithm  is an open, hard problem as it must require potentially expensive integration over possible test data distributions.

\paragraph{Non-probabilistic} Certain non-probabilistic methods have close analogues to information theoretic active learning. Perhaps the most ubiquitous is active learning for SVMs \cite{tong2001,seung1992} where the volume of version space is used as a proxy for the posterior entropy. If a uniform (improper) prior is used with a deterministic classification likelihood it is easy to show that the log volume of version space and Bayesian posterior entropy are equivalent. However, just as Bayesian posteriors become intractable after observing many datapoints, version space too can become very complicated. \cite{tong2001} proposes approximating version space with a simple shape, such as a hypersphere. This closely resembles approximating a Bayesian posterior using a Gaussian distribution via the Laplace or EP approximations. \cite{seung1992} sidesteps the problem by working with predictions. The algorithm, Query by Committee (QBC), samples parameters from version space (committee members), they vote on the outcome of each $\x$ in question. The $\x$ with the most balanced vote is selected; this is termed the `principle of maximal disagreement'. If \ourmethod is used with a sampled posterior, query by committee is implemented but with a probabilistic measure of disagreement. QBC's deterministic vote criterion discards confidence in the predictions and so can exhibit the same pathologies as MES. We present a summary of the methods discussed in this section in the Supplementary material.

\section{Gaussian Processes for Classification and Preference Learning\label{sec:GPC}}

Here we present the application of \ourmethod to Gaussian process classification (GPC). GPs are powerful and highly popular non-parametric tools for regression and classification. GPC appears to be an especially challenging problem for information-theoretic active learning because the parameter space is infinite, however, by using \eqref{eqn:rearrangement} we are able to fully calculate the relevant information quantities without having to work out entropies of infinite dimensional objects. The probabilistic model underlying GPC is as follows:
\begin{align}
	f \sim \mathrm{GP}(\mu(\cdot),k(\cdot,\cdot)) \qquad \y\vert\x,f \sim\mathrm{Bernoulli}(\Phi(f(\x))) 
\end{align}
The latent parameter, now called $f$ (previously denoted as $\param$), is a function $\mathcal{X}\rightarrow\mathbb{R}$, and is assigned a Gaussian process prior with mean $\mu(\cdot)$ and covariance function $k(\cdot,\cdot)$. We consider the probit case where given the value of $f$, $y$ takes a Bernoulli distribution with probability $\Phi(f(\x))$, and $\Phi$ is the cumulative distribution function of the normal distribution. For further details on GPC see \cite{rasmussen2005}.

Inference in the GPC model is intractable; given some observations $\data$, the posterior over $f$ becomes non-Gaussian and complicated. The most commonly used approximate inference methods -- EP,  Laplace approximation, Assumed Density Filtering and sparse methods -- all approximate the posterior by a Gaussian \cite{rasmussen2005}. Throughout this section we will assume that we are provided with such a Gaussian approximation from one of these methods, though the active learning algorithm does not care which. In our derivation we will use {\scriptsize$\stackrel{1}{\approx}$} to indicate where such an approximation is exploited.

Now, we will compute the informativeness of a query $\x$ using Eqn.  \eqref{eqn:rearrangement}.  The entropy of the binary output variable $y$ given a fixed $f$ can be expressed in terms of the binary entropy function $h$: 
\begin{align}
H[y\vert\x,f] = h\left(\Phi(f(\x)\right), && h(p)=- p\log p - (1-p)\log(1-p)
\end{align}
We now have to compute expectations over the posterior. Using a Gaussian approximation to the posterior, for each $\x$, $f_{\x} = f(\x)$ will follow a Gaussian distribution with mean $\mu_{\x,\data}$ and variance $\sigma_{\x,\data}^2$. To compute the two terms in Eqn. \eqref{eqn:rearrangement} we have to compute two entropy quantities. The first term in Eqn. \eqref{eqn:rearrangement}, $H[y\vert\x,\data]$ can be handled analytically:
\begin{align}
	H[y\vert\x,\data] &\stackrel{1}{\approx} h\left( \int \Phi( f_{\x} )  \mathcal{N}(f_{\x}\vert \mu_{\x,\data},\sigma_{\x,\data}^2) df_{\x} \right)  = h \left( \Phi\left( \frac{\mu_{\x,\data}}{\sqrt{\sigma^2_{\x,\data} + 1}} \right)\right)\label{ent_mean}
\end{align}
The second term, $\E_{f \sim p(f\vert\data)} \left[ H[\y\vert f] \right]$ can be computed approximately as follows
\begin{align}
	\E_{f \sim p(f\vert\data)} \left[ H[\y\vert f] \right] &\stackrel{1}{\approx}\int h(\Phi(f_{\x})) \mathcal{N}(f_{\x}\vert \mu_{\x,\data},\sigma_{\x,\data}^2)df_{\x}\label{eqn:mean_entropy}\\
	&\stackrel{2}{\approx} \int \exp\left(-\frac{f_{\x}^2}{\pi\ln2}\right) \mathcal{N}(f_{\x}\vert \mu_{\x,\data},\sigma_{\x,\data}^2)df_{\x}\notag\\	
	&= \frac{C}{\sqrt{\sigma_{\x,\data}^2 + C^2}}\exp\left(-\frac{\mu_{\x,\data}^2}{2\left(\sigma_{\x,\data}^2 + C^2\right)}\right)\notag
\end{align}

where $C=\sqrt{\frac{\pi\ln2}{2}}$. The first approximation, {\scriptsize $\stackrel{1}{\approx}$}, reflects the Gaussian approximation to the posterior. The integral in the left hand side of Eqn. \eqref{eqn:mean_entropy} is hard to compute; $h(\Phi(f_{\x}))$ must be integrated against a Gaussian distribution. However, if we perform a Taylor expansion on $\ln h(\Phi(f_{\x}))$ (see supplementary material) we can see that it can be approximated up to $\mathcal{O}(f_{\x}^3)$ by a squared exponential curve, $\exp(-f_{\x}^2/\pi\ln2)$. We will refer to this approximation as {\scriptsize $\stackrel{2}{\approx}$}. Now we can apply the standard convolution formula for Gaussians to finally get a closed form expression for both terms of  Eqn. \eqref{eqn:rearrangement}.

Fig. \ref{fig:trick} depicts the striking accuracy of this simple approximation. The maximum possible error that will be incurred when using this approximation is if $\mathcal{N}(f_{\x}\vert \mu_{\x,\data},\sigma_{\x,\data}^2)$ is centred at $\mu_{\x,\data}=\pm 2.05$  with $\sigma_{\x,\data}^2$ tending to zero (see Fig. \ref{fig:trick}, absolute error \ref{plots:approx_error}); even this yields only a 0.27\% error in the integral in Eqn.\eqref{eqn:mean_entropy}. The authors are unaware of previous use of this simple and useful approximation in this context.  In Section 5 we investigate experimentally the information lost from approximations {\scriptsize $\stackrel{1}{\approx}$} and {\scriptsize $\stackrel{2}{\approx}$} as compared to the golden standard of extensive Monte Carlo simulation.

To summarise, the \ourmethod algorithm for Gaussian process classification consists of two steps. First it applies an approximate inference algorithm to obtain the posterior predictive mean $\mu_{\x,\data}$ and $\sigma_{\x,\data}$ for each point of interest $\x$. Then, it selects a query $\x$ that maximises the following objective function:

\begin{equation}
	\mathrm{h} \left( \Phi\left( \frac{\mu_{\x,\data}}{\sqrt{\sigma^2_{\x,\data} + 1}} \right)\right) - \frac{C \exp\left(-\frac{\mu_{\x,\data}^2}{2\left(\sigma_{\x,\data}^2 + C^2\right)}\right)}{\sqrt{\sigma_{\x,\data}^2 + C^2}} \label{eqn:BALD_GPC}
\end{equation}

For most practically relevant kernels, the objective \eqref{eqn:BALD_GPC} is smooth, and differentiable function of $\x$, so gradient-based optimisation procedures can be used to find the maximally informative query.

\begin{figure}\centering
\begin{tikzpicture}
\node at (0,0) {
\resizebox{.55\columnwidth}{!}{\input{figs/gaussian_approx.tikz}}};
\node at (.47\columnwidth,0){
\resizebox{.45\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
&MCMC&EP ($\stackrel{1}{\approx}$)&Laplace ($\stackrel{1}{\approx}$)\\ \hline
\hline
MC & 0 & $7.51\pm2.51$ & $41.57\pm4.02$ \\
$\stackrel{2}{\approx}$ & $0.16\pm0.05$ & $7.43\pm2.40$ & $40.45\pm3.67$ \\ \hline
\end{tabular}
}};
\end{tikzpicture}
\caption{\emph{Left:} Analytic approximation ({\scriptsize $\stackrel{1}{\approx}$}) to the binary entropy of the error function (\ref{plots:approx_true}) by a squared exponential (\ref{plots:approx_approx}). The absolute error (\ref{plots:approx_error}) remains under $3\cdot 10^{-3}$. \emph{Right:} Percentage approximation error ($\pm$1 s.d.) for different methods of approximate inference (\emph{columns}) and approximation methods for evaluating Eqn.\eqref{eqn:mean_entropy} (\emph{rows}). The results indicate that {\scriptsize $\stackrel{2}{\approx}$} is a very accurate approximation; EP causes some loss and Laplace significantly more, which is in line with the comparison presented in \cite{Kuss05}. }\label{fig:trick}
\end{figure}

\subsection{Learning and Exploring Hyperparameters \label{sec:hyperparameters}}

Suppose we want to perform active learning by minimising information about a subset of parameters $\param^+$ of central interest, but do not care about another set $\param^-$. By integrating Eqn. \eqref{eqn:ent_change} over the nuisance parameters, $\theta^-$, we may re-derive the following \ourmethod objective function:

\begin{align}
H\left[\E_{p(\param^+,\param^-\vert \data)}\left[\y|\x,\param^+,\param^-\right]\right] - \E_{p(\param^+|\data)} \left[ H\left[\E_{p(\param^-|\param^+,\data)}[ \y \vert \x, \param^+,\param^- ]\right] \right]\label{eqn:BALD_bipartite}
\end{align}

In the context of GP models, hyperparameters typically control the smoothness or spatial length-scale of functions. If we maintain a posterior distribution over these hyperparameters, which we can do e.\,g.\ via Hamiltonian Monte Carlo, we can choose either to treat them as nuisance parameters $\theta^-$ and use Eq.\ \ref{eqn:BALD_bipartite}, or to include them in $\theta^+$ and perform active learning over them as well. In certain cases, such as automatic relevance determination\cite{rasmussen2005}, it may even make sense to treat hyperparameters as variables of primary interest, and the function $f$ itself as nuisance parameter $\theta^-$.

\subsection{Preference Learning}

In preference learning our dataset consists for pairs of items $(\upref_i,\vpref_i)\in\mathcal{X}^2$ with binary labels, $y_i\in\{0,1\}$. $y_i=1$ means instance $\upref_i$ is preferred to $\vpref_i$, denoted $\upref_i\succ \vpref_i$. The task is to predict the preference relation between any $(\upref,\vpref)$. Ultimately the problem is a special case of building a classifier $h:\mathcal{X}^2\mapsto\{0,1\}$. We now briefly review the Bayesian approach of Chu \emph{et al.} \cite{chu2005} who use a latent preference function $f$, over which a zero-mean GP prior is defined. When predicting preference,  $\upref_i \succ \vpref_i$ whenever $f(\upref_i)+\delta_{u_i}>f(\vpref_i)+\delta_{v_i}$, where $\delta_{u_i}, \delta_{v_i}$ denote additive Gaussian evaluation noise. Under this model, the likelihood of $f$ becomes:

\begin{align}
	\mathbb{P}[y=1\vert (\upref_i,\vpref_i), f] = \mathbb{P}[\upref_i\succ \vpref_i \vert f] =  \Phi\left(\frac{f(\upref_i) - f(\vpref_i)}{\sqrt{2}\sigma_{noise}}\right)
\end{align}

It can be assumed w.l.o.g. that $\sqrt{2}\sigma_{noise}=1$. Observe, that the likelihood only depends on the difference between $f(\upref)$ and $f(\vpref)$. We therefore define $g(\upref,\vpref)=f(\upref)-f(\vpref)$, and do inference entirely in terms of $g$, for which the likelihood becomes the same as for probit classification: $y|\upref,\vpref,f\sim \mathrm{Bernoulli}(\Phi(g(\upref,\vpref)))$. We observe that a GP prior is induced on $g$ because it is formed by performing a linear operation on $f$, for which we have a GP prior already $f\sim \mathrm{GP}(0,k)$. We can derive the induced covariance function of $g$ as (derivation in the Supplementary material):

\begin{align}
	k_{pref}((\upref_i,\vpref_i),(\upref_j,\vpref_j)) &= k(\upref_i,\upref_j) + k(\vpref_i,\vpref_j) - k(\upref_i,\vpref_j) - k(\vpref_i,\upref_j)
\end{align}

Note, that this kernel $k_{pref}$ respects the anti-symmetry properties desired for a preference learning scenario, i.e. the value $g(u,v)$ is perfectly anti-correlated with $g(v,u)$, ensuring $\mathbb{P}[\upref\succ \vpref] = 1 - \mathbb{P}[\vpref \succ \upref]$ holds. Thus, we can conclude that the GP preference learning framework of \cite{chu2005}, is equivalent to GPC with a particular class of kernels, that we may call the \emph{preference judgement kernels}. Therefore, our active learning algorithm presented in section \ref{sec:GPC} for GPC can readily be applied to pairwise preference learning as well.

\section{Experiments}

\begin{figure*}[t]
\begin{center}
\begin{tabular}{ccc}
\input{figs/blockinthemiddle_dataset.tikz}&
\input{figs/corner_dataset.tikz}&
\input{figs/checkerboard_dataset.tikz}\\
\input{figs/blockinmiddle2.tikz}&
\input{figs/blockincorner2.tikz}&
\input{figs/checkerboard2.tikz} \\
\end{tabular}
\end{center}
\caption{\emph{Top:} Artificial datasets used in our evaluation of active learning methods. Exemplars of the two classes are shown with black squares (\ref{plots:positives}) and red circles (\ref{plots:negatives}). \emph{Bottom:} Results of active learning with nine methods: random query (\ref{plots:rand}), \ourmethod (\ref{plots:BALD}),  MES (\ref{plots:maxent}), QBC with the vote criterion with 2 ($\mbox{QBC}_2$, \ref{plots:QBC2}) and 100 ($\mbox{QBC}_{100}$, \ref{plots:QBC100}) committee members, active SVM (\ref{plots:SVM}), IVM (\ref{plots:IVM}), Kapoor \emph{et al.} \cite{kapoor2007} (\ref{plots:dec}), Zhu \emph{et al.} \cite{zhu2003} (\ref{plots:semi}) and empirical error (\ref{plots:emp}).}
\label{fig:artificial}
\end{figure*}

\paragraph{Quantifying Approximation Losses:} To obtain \eqref{eqn:BALD_GPC} we made two approximations: we perform approximate inference ({\scriptsize $\stackrel{1}{\approx}$}), and we approximated the binary entropy of the Gaussian CDF by a squared exponential ({\scriptsize $\stackrel{2}{\approx}$}). Both of these can be substituted with Monte Carlo approximation, enabling us to compute an asymptotically unbiased estimate of the expected information gain. Using extensive Monte Carlo as the `gold standard', we can evaluate how much we loose by applying these approximations. We quantify approximation error as: 

\begin{align}
\frac{ \max_{\x\in\mathcal{P}} I(\x) - I(\argmax_{\x\in\mathcal{P}}\hat{I}(\x)) }{{\max_{\x\in\mathcal{P}}I(\x) }}\cdot 100\% 
\end{align}

where $I$ is the objective computed using Monte Carlo, $\hat{I}$ is the approximate objective. These experiments were run on the \emph{cancer} dataset, results are shown and discussed in Figure \ref{fig:trick}.



\paragraph{Pool based active learning:} We test \ourmethod for GPC and preference learning in the pool-based setting i.e. selecting $x$ values from a fixed set of data-points. We compare to eight other algorithms discussed in this paper: random sampling, MES, QBC, SVM with version space approximation \cite{tong2001}, decision theoretic approaches in \cite{kapoor2007, zhu2003} and directly minimizing expected empirical error (empirical error is not a widely used method, but is included for analysis of \cite{kapoor2007}).

We consider three artificial, but challenging, datasets. The first of which is similar to the \emph{checkerboard} dataset used in \cite{zhu2003}, and is designed to test the algorithm's capabilities to find multiple disjoint islands of points from one class. The second, \emph{block in the corner}, has a block of uninformative points far from the decision boundary, and the third, \emph{block in the middle}, has a block of noisy points on the decision boundary: a strong active learning algorithm should avoid these uninformative regions. The three datasets and results using each algorithm are depicted in Fig. \ref{fig:artificial}.

In addition to this, we present results on 6 UCI binary classification datasets \emph{australia, crabs, vehicle, isolet, cancer} and \emph{wdbc}. \emph{Letter} is a multiclass dataset for which we select hard-to-distinguish letters E vs. F and D vs. P. For preference learning we use the \emph{cpu, cart} and \emph{kinematics} regression datasets \footnote{Data sets at http://www.liacc.up.pt/~ltorgo/Regression/DataSets.html} processed to yield a preference task as described in \cite{chu2005}. Results are plotted in Fig. \ref{fig:BALD_GPC_results}.


\begin{figure*}
\begin{center}
\begin{tabular}{ccc}
\input{figs/crabs2.tikz}&
\input{figs/vehicle2.tikz}&
\input{figs/wine2.tikz}\\
\input{figs/wdbc2.tikz}&
\input{figs/isolet2.tikz}&
\input{figs/austra2.tikz}\\
\input{figs/letterDP2.tikz}&
\input{figs/letterEF2.tikz}&
\input{figs/prefkinem2.tikz}\\
\input{figs/prefcart2.tikz}&
\input{figs/prefcpu2.tikz}&
\input{figs/cancerB2.tikz}\\
\end{tabular}
\end{center}
\caption{Classification accuracy on classification and preference learning datasets. Methods used are random query (\ref{plots:rand}), \ourmethod (\ref{plots:BALD}),  MES (\ref{plots:maxent}), QBC with 2 ($\mbox{QBC}_2$, \ref{plots:QBC2}) and 100 ($\mbox{QBC}_{100}$, \ref{plots:QBC100}) committee members, active SVM (\ref{plots:SVM}), IVM (\ref{plots:IVM}), decision theoretic \cite{kapoor2007} (\ref{plots:dec}), semi-supervised \cite{zhu2003} (\ref{plots:semi}) and empicial error (\ref{plots:emp}). The decision theoretic methods took a long time to run, so were not completed for all datasets. Plots (a-h) are GPC datasets, (i-k) are preference learning, plot (l) includes \ourmethod with hyperparameter learning (\ref{plots:opthyper})}.
\label{fig:BALD_GPC_results}
\end{figure*}



We can see from Figs \ref{fig:artificial} and \ref{fig:BALD_GPC_results} that by using \ourmethod we make significant gains over naive sampling in both the classification adn preference learning domains. Relative to other active learning algorithms \ourmethod performs consistently well across all datasets, particularly when avoiding the block of points in Fig. \ref{fig:artificial} (a). Occasionally e.g. as in Fig. \ref{fig:BALD_GPC_results} (k,l), it performs poorly on the first couple of queries. In most reporter experiments we have fixed the hyperparamters a priori to the maximum likelihood estimate on the whole pool. This is of course cheating, as it uses information from the whole dataset before starting to select queries, but it provides us with a fair way of comparing various methods, that cannot handle hyperparameter learning. As shown in section \ref{sec:hyperparameters}, BALD can accommodate active learning of hyperparameters. For inference over hyperparameters we used Hybrid Monte Carlo, which is an expensive procedure, therefore we only tried it on a fewer number of datasets. On most datasets including hyperparameters in the \ourmethod objective makes little difference, however, on the \emph{cancer} dataset it helps to overcome the initial poor performance of \ourmethod. This is shown in Fig.\ \ref{fig:BALD_GPC_results}(l).

MES often performs as well as \ourmethod e.g. on Fig. \ref{fig:artificial} (c), where there is zero noise. It never outperforms \ourmethod though and on noisy datasets (e.g. Fig. \ref{fig:artificial}(a)) performs poorly as expected. QBC provides a close approximation to \ourmethod and usually provides a small decrement in performance. However, there is a large decrease in performance on the noisy artificial dataset caused by the vote criterion not maintaining a notion of inherent uncertainty, like MES. The IVM occasionally performs well, but often exhibits highly pathological behaviour; by observing $\y$ values in advance it actively chooses noisy or mislabelled points, thinking them informative. The SVM-based approach exhibits variable performance (it does extremely well on Fig. \ref{fig:BALD_GPC_results} (f), but very poorly on \ref{fig:artificial} (c)). Although we have only presented one possible version space approximation here, we find that the performance is greatly effected by the approximation used.

The decision theoretic approaches sometimes perform well, on \ref{fig:artificial}(c) they choose the first 16 points from the centre of each cluster as they are influenced by the surrounding unlabelled points. \ourmethod, being inductive, does not observe the unlabelled points so may not pick points from the centres. On the real datasets though BALD usually performs as well, if not better, despite not having access to the locations of the test points and having a significantly lower computational cost. The Kapoor \emph{et al.} objective sometimes fails badly, this is likely to be because one term in their objective function is the empirical error. The weighting of this term is determined by the relative sizes of the training and test set. Directly minimizing empirical error usually performs very pathologically, picking only 'safe' points, when the Kapoor \emph{et al.} objective assigns to much weight to this term it also fails.

\section{Conclusions}

We have presented a novel method for applying the full information theoretic active learning criterion to GPC. We present a neat trick that provides a highly accurate analytic approximation to the information theoretic objective. We extend the GPC model to develop a novel preference learning kernel, allowing us to apply our active learning algorithm directly to this domain also. We have shown that the method can naturally handle active learning of kernel hyperparameters, something which is a hard, mostly unsolved problem for example in SVM active learning. One notable feature of our approach is that it is agnostic to the approximate inference methods used. This allows us to choose from a whole range of approximate inference methods, including EP, Laplace approximation, ADF or even sparse online learning, and thereby to trade off between computational complexity and accuracy. We compare favourably to many other active learning methods for classification, even those that have access to the test data and require much greater computational time.



{\small
\bibliographystyle{unsrt}
\bibliography{bib/bibliog}
}



\end{document}
