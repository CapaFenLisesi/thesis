\section{Related Methods \label{sec:relatedWork}}

\paragraph{Multi-task preference Learning} Other approaches to generalising the GP framework of \citep{chu2005} to the multi-user case are presented in \citep{bonilla2010, birlutiu2009}. Bonilla et al. \citep{bonilla2010} use a GP to model both users and items simultaneously using a product kernel. This is a flexible approach, but is not applicable in our domain, where we do not assume access to user features. Birlutiu et al. model similarities between users using a hierarchical GP model. They capture shared behaviour by assuming a common prior mean and covariance matrix over users. The model is less flexible than ours becuase user's are modelled as perturbations about a unimodal prior; by combining distinct auxiliary latent functions we may capture a wider vareith of user behaviour. We compare experimentally to this model, details of the implementation may be found in the Supplementary material.

Our model resembles matrix factorisation methods for recommendation systems where the users' preferences are expressed as the product of two matrices (here $\w$ and $\h$). Our method extends this approach by putting GP prior on the rows of $\h$, incorporating covariates on the items. Recently, matrix factorisation approaches have been extended successfully to large scale recommender systems \citep{stern2009}.

\paragraph{Active Learning} Maximum Entropy Sampling (MES) \citep{sebastiani2000} is a closely related approach that also works explicitly in dataspace (i.e.\, with Eqn.\,\eqref{eqn:rearrangement}). MES was proposed for regression models with input-independent observation noise. Although Eqn.\,\eqref{eqn:rearrangement} is used, the second term is constant because of input independent noise and is ignored. However, when confronted with input-dependent observation noise (as is the case for GPC or out preference model) it fails to differentiate between model uncertainty and observation uncertainty (about which our model may be confident). There is a large host of other information-theoretic based approach that make further approximations to BALD when applied to GPC, and often have much higher computational complexity. We describe the relationship to these algorithms compare BALD experimentally against them for binary GPC in the Supplementary material. We find that BALD consistently performs very well due to the fact that it makes the fewest approximations to the full information theoretic criteria. The closest performing algorithm is MES which has the same computational complexity as BALD; it is therefore used as a comparison for multi-task preference learning.

%The Query by Committee algorithm (QBC) \citep{freund1997} approach performs a sampling approximation to \eqref{eqn:rearrangement} . If BALD is used with a sampled posterior, it is equivalent to QBC but with a probabilistic measure of disagreement. QBC's deterministic vote criterion discards confidence in the predictions and so can exhibit the same pathologies as MES. 
%The Informative Vector Machine (IVM) \citep{lawrence2002} is also motivated by information theory. It was designed for sub-sampling a dataset and cannot be used for online active learning because the alggorithm is privy to the $\y$ values before including a measurement. It cannot therefore work explicitly in output space i.e. with Eqn.\,\eqref{eqn:rearrangement}. The IVM uses Eqn.\,\eqref{eqn:ent_change}, but parameter entropies are calculated approximately in the marginal subspace corresponding to the observed data points. This approach is expensive, requiring $\mathcal{O}(Q_{\x}Q_{\y})$ posterior updates to sample a single new datapoint. To make this computationally feasible, Assumed Density Filtering (ADF) is proposed in \citep{lawrence2002} which is a further approximation to EP. Such an approach would be infeasible with our more complex inference algorithm.

%Another flavour of approaches is proposed for multi-user preference learning is to use Expected Value of Information (EVOI) \citep{bonilla2010}. This is a decision theoretic approach, and the objective is not to learn optimally about the parameters but to seek the data that is expected to improve the maximum of users latent function the most (i.e. find their most preferred item). This of course requires the set or distribution over test items to be known. This approach is well tailored for online active learning with a single user as it seeks to find the best recommendation as fast as possible, whereas the information theoretic approach is more suited when trying to learn the model offline from existing data from many users. By focussing just on areas of high preference one is less likely to discover the true latent functions underlying user behaviour, our approach directly tries to reduce uncertainty about these alongside the user-specific parameters (i.e. the weights). Perhaps more importantly to calculate the EVOI new predictive distributions need to be calculated for all possible data points under consideration, again requiring $\mathcal{O}(Q_{\pair}Q_{\y})$ posterior updates; we only require one from that may calculate the score for all candidates, this offers a very large computational saving when the number of possible item pairs is large.
