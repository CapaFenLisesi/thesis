{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"scoring",
				"sec:scoring_rules"
			],
			[
				"gaussian",
				" Gaussian Processes for Machine Learning"
			],
			[
				"Kr",
				"Kr Experimental Adaptive Bayesian Tomography"
			],
			[
				"quan",
				"quantumparam"
			],
			[
				"scor",
				"sec:scoring_rules"
			],
			[
				"support",
				" Support vector machine active learning with applications to text classifi"
			],
			[
				"Houlsb",
				"Houlsb Bayesian Active Learning for Classification and Preference Learning"
			],
			[
				"Houls",
				"Houlsby2012preference"
			],
			[
				"Houl",
				"Houlsby2011"
			],
			[
				"zhu",
				"Zhu2003"
			],
			[
				"houls",
				"Houlsby2012preference"
			],
			[
				"seung",
				" Selective sampling using the query by committee algorithm"
			],
			[
				"sco",
				"sec:scoring_rules"
			],
			[
				"fig",
				"eqn:def_divergence"
			],
			[
				"B",
				"eqn:BALD_GPC"
			],
			[
				"prefere",
				"sec:prefKernel"
			],
			[
				"log",
				"sec:log_score"
			],
			[
				"sparse",
				"sparse A unifying view of sparse approximate {G}aussian process regression"
			],
			[
				"csato",
				" Improved approximation algorithms for maximum cut and satisfiability prob"
			],
			[
				"expectation",
				" Expectation-propagation for the Generative Aspect Model"
			],
			[
				"gaussien",
				" Gaussian Processes for Machine Learning"
			],
			[
				"mutua",
				"eqn:mutualinfo_as_KLdivergence"
			],
			[
				"ker",
				"eqn:kernel_scoring_rule"
			],
			[
				"Grett",
				"Grett Kernel constrained covariance for dependence measurement"
			],
			[
				"score",
				"def:score_matching"
			],
			[
				"appro",
				"sec:approximate_inference"
			],
			[
				"hilber",
				" A Hilbert space embedding for distributions"
			],
			[
				"Dawi",
				"Dawi Proper local scoring rules on discrete sample spaces"
			],
			[
				"kernel",
				"eqn:kernel_information"
			],
			[
				"Scoring",
				"Scoring Scoring rules, generalized entropy, and utility maximization"
			],
			[
				"Gret",
				"Gret A kernel two-sample test"
			],
			[
				"scorin",
				"sec:scoring_rules"
			],
			[
				"hus",
				"hus Optimally-Weighted Herding is {B}ayesian Quadrature"
			],
			[
				"loss",
				"sec:loss_scoring_rule"
			],
			[
				"pho",
				"eqn:photon_right"
			],
			[
				"reg",
				"regr_az"
			],
			[
				"acto",
				"actor_id"
			],
			[
				"actor",
				"actor_json"
			],
			[
				"hash",
				"hashtag_data"
			],
			[
				"mention",
				"mention_normalised"
			],
			[
				"influe",
				"influence_graph_normalised"
			],
			[
				"infl",
				"influence_graph_normalised"
			],
			[
				"topic_score",
				"topic_score_normalised"
			],
			[
				"resolved",
				"url_resolved"
			],
			[
				"tweet",
				"tweet_retweeted_count"
			],
			[
				"retweet",
				"retweeted_status"
			],
			[
				"raw_te",
				"raw_tweets_sample"
			],
			[
				"retwee",
				"retweet_count"
			],
			[
				"topic",
				"topic2_score"
			],
			[
				"NORM",
				"NORM_GRADIENT"
			],
			[
				"mean",
				"meanoutput"
			]
		]
	},
	"buffers":
	[
		{
			"file": "latex/part1/01_scoring_rules.tex",
			"settings":
			{
				"buffer_size": 61296,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/part1/01_scoring_rules.tex.bak",
			"settings":
			{
				"buffer_size": 60573,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "Searching 87 files for \"\n%!TEX root = main.tex\n%!TEX root = main.tex\n%!TEX root = ../main.tex\"\n\n0 matches across 0 files\n\n\nSearching 87 files for \"\n%!TEX root = main.tex\n%!TEX root = main.tex\nTEX root = ../main.tex\"\n\n0 matches across 0 files\n\n\nSearching 87 files for \"\n%!TEX root = main.tex\n%!TEX root = main.tex\n%!TEX root = main.tex\"\n\n0 matches across 0 files\n\n\nSearching 87 files for \"%!TEX root = main.tex\"\n\n/Users/fhuszar/Dropbox/thesis/latex/conclusions.tex:\n    1: %!TEX root = main.tex\n    2  \n    3  Machine learning is a mess. This thesis finally brings order and clarity to what otherwise appears as a mix of muddled thoughts. Just kidding.\n\n/Users/fhuszar/Dropbox/thesis/latex/introduction.tex:\n    1: %!TEX root = main.tex\n    2  \\lipsum[1-5]\n\n/Users/fhuszar/Dropbox/thesis/latex/notation.tex:\n    1: %!TEX root = main.tex\n    2  \\newcommand{\\TODO}[1]{\\textbf{TODO: #1}}\n    3  \n\n/Users/fhuszar/Dropbox/thesis/latex/part1_main.tex:\n    1: %!TEX root = main.tex\n    2  \\part{Scoring rules, Divergences and Information}\n    3  \n\n/Users/fhuszar/Dropbox/thesis/latex/part2_main.tex:\n    1: %!TEX root = main.tex\n    2  \\part{Approximate Bayesian inference}\n    3  \n\n/Users/fhuszar/Dropbox/thesis/latex/part3_main.tex:\n    1: %!TEX root = main.tex\n    2  \\part{Optimal Experiment Design}\n    3  \n\n/Users/fhuszar/Dropbox/thesis/latex/preamble.tex:\n    1: %!TEX root = main.tex\n    2  \\newtheorem{theorem}{Theorem}\n    3  \\newtheorem{corollary}{Corollary}\n\n7 matches across 7 files\n\n\nSearching 86 files for \"usetikzlibrary\"\n\n/Users/fhuszar/Dropbox/thesis/latex/preamble.tex:\n   11  \\usepackage[paper=a4paper]{geometry}\n   12  \\usepackage{tikz,pgfplots}\n   13: \\usetikzlibrary{external}\\tikzexternalize\n   14  \n   15  \\newtheorem{theorem}{Theorem}\n\n1 match in 1 file\n\n\nSearching 1 file for \"\\label:{fig:\"\n\n0 matches across 0 files\n\n\nSearching 1 file for \"\\label{fig:\"\n\n/Users/fhuszar/Dropbox/thesis/latex/part2/01_approximate_inference.tex:\n  208  \\includegraphics[width=\\columnwidth]{figs/herding/fig1_v2}\n  209  \\caption[Sequential Bayesian quadrature versus kernel herding]{The first 8 samples from sequential Bayesian quadrature, versus the first 20 samples from herding.  Only 8 weighted \\sbq{} samples are needed to give an estimator with the same maximum mean discrepancy as using 20 herding samples with uniform weights.  Relative sizes of samples indicate their relative weights.}\n  210: \\label{fig:fig1}\n  211  \\end{figure} \n  212  \n  ...\n  290  \\includegraphics[width=\\columnwidth]{figs/herding/bq_intro4}\n  291  \\caption[An illustration of Bayesian quadrature]{An illustration of Bayesian Quadrature.  The function $f(x)$ is sampled at a set of input locations.  This induces a Gaussian process posterior distribution on $f$, which is integrated in closed form against the target density, $p(\\vx)$.  Since the amount of volume under $f$ is uncertain, this gives rise to a (Gaussian) posterior distribution over $Z_{f,p}$.}\n  292: \\label{fig:bq_intro} \\label{fig:fig1}\n  293  \\end{figure}\n  294  \n  ...\n  353  	\\includegraphics[width=\\columnwidth]{figs/herding/weights_v1_n100}\n  354  	\\caption[Empirical distribution of weights in sequential Bayesian quadrature]{A set of optimal weights given by \\bq{}, after 100 \\sbq{} samples were selected on the distribution shown in Figure \\ref{fig:fig1}.  Note that the optimal weights are spread away from the uniform weight ($\\frac{1}{N}$), and that some weights are even negative.  The sum of these weights is 0.93.}\n  355: 	\\label{fig:weights100} \\label{fig:bq_intro} \\label{fig:fig1}\n  356  \\end{figure}\n  357  \n  ...\n  365  	\n  366  		\\caption[The concept of shrinkage in Bayesian quadrature]{An example of Bayesian shrinkage in the sample weights.  In this example, the kernel width is approximately $\\nicefrac{1}{20}$ the width of the distribution being considered.  Because the prior over functions is zero mean, in the small sample case the weights are shrunk towards zero.  The weights given by simple Monte Carlo and herding do not exhibit shrinkage. }\n  367: 	\\label{fig:weights_shrinkage} 	\\label{fig:weights100} \\label{fig:bq_intro} \\label{fig:fig1}\n  368  \\end{figure}\n  369  \n  ...\n  495  \\includegraphics[width=\\columnwidth]{figs/herding/expected_variance_v7_400}\n  496  \\caption[Discrepancy of Bayesian quadrature, herding and random sampling]{The maximum mean discrepancy, or expected error of several different quadrature methods.  Herding appears to approach a rate close to $\\mathcal{O}(1/N)$.  \\sbq{} appears to attain a faster, but unknown rate.}\n  497: \\label{fig:mmd_curve}\\label{fig:weights_shrinkage} 	\\label{fig:weights100} \\label{fig:bq_intro} \\label{fig:fig1}\n  498  \\end{figure}\n  499  %\n  ...\n  522  \\includegraphics[width=\\columnwidth]{figs/herding/error_curve_rkhs_400_v4}\n  523  \\caption[Empirical error of Bayesian quadrature, herding and random sampling]{Within-model error: The empirical error rate in estimating $Z_{f,p}$, for several different sampling methods, averaged over 250 functions randomly drawn from the RKHS corresponding to the kernel used.}\n  524: \\label{fig:error_curve}\\label{fig:mmd_curve}\\label{fig:weights_shrinkage} 	\\label{fig:weights100} \\label{fig:bq_intro} \\label{fig:fig1}\n  498  \n  525  \\end{figure}\n  526  %\n  ...\n  532  \\includegraphics[width=\\columnwidth]{figs/herding/bound_curve_rkhs}\n  533  \\caption[Illustrating MMD as an upper bound on empirical error rate]{The empirical error rate in estimating $Z_{f,p}$,  for the \\sbq{} estimator, on 10 random functions drawn from the RKHS corresponding to the kernel used.  Also shown is the upper bound on the error rate implied by the $\\mmd$.}\n  534: \\label{fig:bound_curve}\\label{fig:error_curve}\\label{fig:mmd_curve}\\label{fig:weights_shrinkage} 	\\label{fig:weights100} \\label{fig:bq_intro} \\label{fig:fig1}\n  535  \\end{figure}\n  536  \n  ...\n  548  \\includegraphics[width=\\columnwidth]{figs/herding/error_curve_outmodel_400_v3}\n  549  \\caption[Out-of-model error of Bayeisan quadrature, herding and random sampling]{Out-of-model error: The empirical error rates in estimating $Z_{f,p}$, for several different sampling methods, averaged over 250 functions drawn from outside the RKHS corresponding to the kernels used.}\n  550: \\label{fig:error_curve_outmodel}\\label{fig:bound_curve}\\label{fig:error_curve}\\label{fig:mmd_curve}\\label{fig:weights_shrinkage} 	\\label{fig:weights100} \\label{fig:bq_intro} \\label{fig:fig1}\n  551  \\end{figure}\n  552  %\n\n8 matches in 1 file\n\n\nSearching 86 files for \"asmussen\" (case sensitive)\n\n/Users/fhuszar/Dropbox/thesis/latex/bibliography/thesis.bib:\n  146  \n  147  @article{quinonero2005,\n  148: 	Author = {Qui{\\~n}onero-Candela, Joaquin and Rasmussen, Carl Edward},\n  149  	Date-Added = {2013-05-12 16:04:56 +0000},\n  150  	Date-Modified = {2013-05-12 16:05:03 +0000},\n  ...\n  773  \n  774  @article{nickisch2008gpc,\n  775: 	Author = {Nickisch, H. and C. E. Rasmussen},\n  776  	Date-Added = {2013-04-16 17:25:49 +0000},\n  777  	Date-Modified = {2013-04-16 17:25:49 +0000},\n  ...\n  793  \n  794  @article{candela05sparseGP,\n  795: 	Author = {J. Qui{\\`O}onero-Candela and C. E. Rasmussen},\n  796  	Date-Added = {2013-04-16 17:25:49 +0000},\n  797  	Date-Modified = {2013-04-16 17:25:49 +0000},\n  ...\n  958  	Year = {2002}}\n  959  \n  960: @book{rasmussen06GP,\n  961  	Address = {Cambridge, MA, USA},\n  962: 	Author = {Carl Edward Rasmussen and Christopher K. I. Williams},\n  963  	Date-Added = {2013-04-16 17:25:49 +0000},\n  964  	Date-Modified = {2013-04-16 17:25:49 +0000},\n  ...\n 1158  	Year = {2006}}\n 1159  \n 1160: @book{rasmussen2006,\n 1161: 	Author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},\n 1162  	Date-Added = {2013-04-16 17:12:02 +0000},\n 1163  	Date-Modified = {2013-04-16 17:12:02 +0000},\n ....\n 1192  \n 1193  @article{Nickisch2008,\n 1194: 	Author = {Nickisch, Hannes and Rasmussen, Carl Edward},\n 1195  	Date-Added = {2013-04-16 17:12:02 +0000},\n 1196  	Date-Modified = {2013-04-16 17:14:05 +0000},\n ....\n 1623  @incollection{BZMonteCarlo,\n 1624  	Address = {Cambridge, MA},\n 1625: 	Author = {C. E. Rasmussen and Z. Ghahramani},\n 1626  	Booktitle = {Advances in Neural Information Processing Systems},\n 1627  	Date-Added = {2013-04-16 17:05:02 +0000},\n ....\n 1697  	Year = {2012}}\n 1698  \n 1699: @article{rasmussen38gaussian,\n 1700: 	Author = {Rasmussen, C.E. and Williams, CKI},\n 1701  	Date-Added = {2013-04-16 17:05:02 +0000},\n 1702  	Date-Modified = {2013-04-16 17:05:02 +0000},\n ....\n 1741  	Year = {2005}}\n 1742  \n 1743: @article{Rasmussen2005,\n 1744: 	Author = {Rasmussen, C.E. and Williams, C.K.I.},\n 1745  	Date-Added = {2013-04-16 16:35:16 +0000},\n 1746  	Date-Modified = {2013-04-16 16:35:31 +0000},\n ....\n 1881  \n 1882  @inproceedings{Kuss2005,\n 1883: 	Author = {M. Kuss and C. E. Rasmussen},\n 1884  	Booktitle = {NIPS},\n 1885  	Date-Added = {2013-04-16 16:35:16 +0000},\n\n/Users/fhuszar/Dropbox/thesis/latex/part2/01_approximate_inference.tex:\n   22  The likelihood $p(\\data\\vert\\param)$ describes how data is related to the parameters $\\param$, and $p(\\param)$ is a prior distribution which capture's one a priori expectations about what the value of $\\param$ may be. The posterior distribution $p_\\data = p(\\param\\vert\\data)$ captures all statistically relevant information that the data $\\data$ provides about $\\param$, and it is therefore of central importance.\n   23  \n   24: The marginal likelihood, also called the model evidence $Z = \\int p(\\data\\vert\\param) p(\\param) d\\param$ is also of interest. It is often used to quantify how well a Bayesian model -- the combination of likelihood and prior -- fit the data. When the model involves additinal parameters or hyper-parameters that are not modelled probabilistically via prior distributions, it is common practice to learn their values by maximising model evidence \\citep[see \\eg][Chapter 5]{Rasmussen2006}.\n   25  \n   26  In practically interesting Bayesian models, the posterior distribution and model evidence are often computationally intractable to obtain and therefore one has to resort to approximations. The most popular methods for Bayesian approximate inference are variational inference and Markov chain Monte Carlo.\n\n/Users/fhuszar/Dropbox/thesis/latex/part2/02_herding.tex:\n  362  This means that the estimate converges for any bounded measurable function $f$. The speed of convergence, however, may not be as fast.\n  363  \n  364: Therefore it is crucial that the kernel we choose is representative of the function or functions $f$ we will integrate.  For example, in our experiments, the convergence of herding was sensitive to the width of the Gaussian kernel.  One of the major weaknesses of kernel methods in general is the difficulty of setting kernel parameters.  A key benefit of the Bayesian interpretation of herding and MMD presented in this paper is that it provides a recipe for adapting the Hilbert space to the observations $f(x_n)$.  To be precise, we can fit the kernel parameters by maximizing the marginal likelihood of Gaussian process conditioned on the observations.  Details can be found in \\citep{rasmussen38gaussian}.\n  365  \n  366  \\subsection{Computational Complexity}\n\n/Users/fhuszar/Dropbox/thesis/latex/part3/02_GP_BALD.tex:\n   81  BALD exploits the fact that in many active learning applications the output space $\\Ye$ is often simpler than the parameter space $\\Theta$. Here I consider the problem of active learning for binary classification, when the output takes one of two possible values $y \\in \\{-1,1\\}$. Given the simplicity of the outputs, binary classification is a highly relevant use-case for BALD.\n   82  \n   83: I will use a non-parametric Bayesian classification model, Gaussian process classification \\citep[GPC,][]{rasmussen06GP} to demonstrate the usefulness of BALD: GPC appears to be an especially challenging problem for information-theoretic active learning because its parameter space is infinite. Therefore, computing entropy of the posterior involves nontrivial quantities. However, by using the BALD approach and Eqn.\\ \\eqref{eqn:rearrangement} we are able to fully calculate the relevant information quantities without having to work out entropies of infinite dimensional objects. \n   84  \n   85  \n   ..\n   94  \\TODO{I need an appendix on kernels and Gaussian processes}\n   95  \n   96: We consider the probit case where, given the value of $f$, the binary label $y$ takes a Bernoulli distribution with probability $\\Phi(f(\\x))$, and $\\Phi$ is the cumulative distribution function of the normal distribution. For further details on GPC see \\citep{rasmussen2005}.\n   97  \n   98  Posterior inference in the GPC model is intractable; given some observations $\\data$, the posterior over $f$ becomes non-Gaussian and complicated. This is addressed by using approximate inference methods. The most commonly used approximate inference methods for Gaussian process classification are expectation propagation \\citep[EP,][]{Minka2002}, Laplace's approximation \\citep{williams1998}, assumed density filtering \\citep[ADF,][]{csato2000} and sparse methods \\citep{candela05sparseGP}. These all approximate the non-Gaussian posterior by a Gaussian \\citep{Nickisch2008}, but differ in the optimisation criterion and other restrictions. Throughout this chapter I will assume that we are provided with some Gaussian approximation to the GPC posterior resulting from one of these methods, though the active learning method is agnostic as to which method produced this estimate. Given the sequential nature of active learning, fast on-line methods \\citep{Csato2002} are particularly well suited for the task. In our derivation we will use {\\scriptsize$\\stackrel{1}{\\approx}$} to indicate where approximate inference is exploited.\n\n/Users/fhuszar/Dropbox/thesis/raw_materials/BALD-GPC/AL_NIPS2011.tex:\n  124  	f \\sim \\mathrm{GP}(\\mu(\\cdot),k(\\cdot,\\cdot)) \\qquad \\y\\vert\\x,f \\sim\\mathrm{Bernoulli}(\\Phi(f(\\x))) \n  125  \\end{align}\n  126: The latent parameter, now called $f$ (previously denoted as $\\param$), is a function $\\mathcal{X}\\rightarrow\\mathbb{R}$, and is assigned a Gaussian process prior with mean $\\mu(\\cdot)$ and covariance function $k(\\cdot,\\cdot)$. We consider the probit case where given the value of $f$, $y$ takes a Bernoulli distribution with probability $\\Phi(f(\\x))$, and $\\Phi$ is the cumulative distribution function of the normal distribution. For further details on GPC see \\cite{rasmussen2005}.\n  127  \n  128: Inference in the GPC model is intractable; given some observations $\\data$, the posterior over $f$ becomes non-Gaussian and complicated. The most commonly used approximate inference methods -- EP,  Laplace approximation, Assumed Density Filtering and sparse methods -- all approximate the posterior by a Gaussian \\cite{rasmussen2005}. Throughout this section we will assume that we are provided with such a Gaussian approximation from one of these methods, though the active learning algorithm does not care which. In our derivation we will use {\\scriptsize$\\stackrel{1}{\\approx}$} to indicate where such an approximation is exploited.\n  129  \n  130  Now, we will compute the informativeness of a query $\\x$ using Eqn.  \\eqref{eqn:rearrangement}.  The entropy of the binary output variable $y$ given a fixed $f$ can be expressed in terms of the binary entropy function $h$: \n  ...\n  181  \\end{align}\n  182  \n  183: In the context of GP models, hyperparameters typically control the smoothness or spatial length-scale of functions. If we maintain a posterior distribution over these hyperparameters, which we can do e.\\,g.\\ via Hamiltonian Monte Carlo, we can choose either to treat them as nuisance parameters $\\theta^-$ and use Eq.\\ \\ref{eqn:BALD_bipartite}, or to include them in $\\theta^+$ and perform active learning over them as well. In certain cases, such as automatic relevance determination\\cite{rasmussen2005}, it may even make sense to treat hyperparameters as variables of primary interest, and the function $f$ itself as nuisance parameter $\\theta^-$.\n  184  \n  185  \\subsection{Preference Learning}\n\n/Users/fhuszar/Dropbox/thesis/raw_materials/lossBayes/lossBayes.tex:\n  246  \\section{SUPERVISED LEARNING} \\label{s:examples}\n  247  \n  248: In this section, we make our framework more concrete by investigating it in the predictive setting presented in \\secref{ss:predictive}. We recall that in order to apply our framework, we need to specify the loss, the action space, the Bayesian observation model and a tractable family $\\mathcal{Q}$ of approximate distributions over the latent variable $\\theta$. In the predictive setting, an action is a prediction function $h:\\mathcal{X}\\rightarrow\\mathcal{Y}$. We let the action space $\\mathcal{H}$ be the set of all possible such functions here -- we are thus in the non-parametric prediction regime where we are free to make arbitrary pointwise decision on $\\mathcal{X}$. This gives us rich predictive possibilities as well as actually enables us to analytically compute $h_q$, as we will see in the next paragraph. For the observation model, we consider Bayesian non-parametric probabilistic models based on Gaussian processes (GPs), which have been successfully applied to model a wide variety of phenomena in the past~\\citep{rasmussen06GP}. %%OPTIONAL\\footnote{Considering the non-parametric setup reduces the issue of model misspecification. Moreover, GPs are often applied using approximate inference and thus provide a useful illustration for our framework.}.\n  249  In \\secref{ss:GPR}, we first look at Gaussian process regression. In this case, we can obtain an analytic form for $p_\\dataset$ and $\\mathcal{R}_{p_\\dataset}(h_q)$ which gives us some insights about the approximation framework as well as when minimizing the KL divergence can be suboptimal. Because the quadratic cost function is not bounded (and so $M = \\infty$), we cannot directly apply our loss-EM algorithm for regression, but we can nevertheless get useful insights which suggest future research directions for regression with sparse GPs. In section~\\ref{ss:GPC}, we consider Gaussian process classification (GPC) which will provide a test bed for the loss-EM algorithm. In both cases, we use a GP as our prior over parameters and let $\\mathcal{Q}$ also be a family of GPs.\n  250  \n  ...\n  384  To investigate the effect of the test distribution $p(x)$ on our method, we generated three different transductive test sets of size 1000, with inputs sampled from $\\mathcal{U}(0,1)$, $\\mathcal{U}(0.2,1.2)$ and $\\mathcal{U}(0.5,1.5)$ respectively (columns of \\tableref{tab:results}), and repeated these experiments 10 times to get significance results. We used five different loss matrices: the loss for false negatives was constant at $c_{-}=1$, the loss for false positives $c_{+}$ was varied so that the decision threshold $p_{thresh}=\\frac{c_{+}}{c_{-} + c_{+}}$ changed linearly between 0.5 and 0.05 (rows of \\tableref{tab:results}).\n  385  \n  386: For each dataset, we compared three methods for approximate inference: Laplace approximation, expectation propagation (EP) and loss-EM (run separately for each loss and test set combination). Both Laplace and EP are standard approaches to GP classification~\\citep{rasmussen06GP}. To evaluate the performance of the methods, we used the following criterion based on the posterior risk:\n  387  \\addtolength{\\abovedisplayskip}{-1mm}\n  388  \\addtolength{\\belowdisplayskip}{-1mm}\n\n23 matches across 6 files\n\n\nSearching 86 files for \"asmussen\" (case sensitive)\n\n/Users/fhuszar/Dropbox/thesis/latex/bibliography/thesis.bib:\n  146  \n  147  @article{quinonero2005,\n  148: 	Author = {Qui{\\~n}onero-Candela, Joaquin and Rasmussen, Carl Edward},\n  149  	Date-Added = {2013-05-12 16:04:56 +0000},\n  150  	Date-Modified = {2013-05-12 16:05:03 +0000},\n  ...\n  773  \n  774  @article{nickisch2008gpc,\n  775: 	Author = {Nickisch, H. and C. E. Rasmussen},\n  776  	Date-Added = {2013-04-16 17:25:49 +0000},\n  777  	Date-Modified = {2013-04-16 17:25:49 +0000},\n  ...\n  793  \n  794  @article{candela05sparseGP,\n  795: 	Author = {J. Qui{\\`O}onero-Candela and C. E. Rasmussen},\n  796  	Date-Added = {2013-04-16 17:25:49 +0000},\n  797  	Date-Modified = {2013-04-16 17:25:49 +0000},\n  ...\n  958  	Year = {2002}}\n  959  \n  960: @book{Rasmussen2006,\n  961  	Address = {Cambridge, MA, USA},\n  962: 	Author = {Carl Edward Rasmussen and Christopher K. I. Williams},\n  963  	Date-Added = {2013-04-16 17:25:49 +0000},\n  964  	Date-Modified = {2013-04-16 17:25:49 +0000},\n  ...\n 1158  	Year = {2006}}\n 1159  \n 1160: @book{rasmussen2006,\n 1161: 	Author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},\n 1162  	Date-Added = {2013-04-16 17:12:02 +0000},\n 1163  	Date-Modified = {2013-04-16 17:12:02 +0000},\n ....\n 1192  \n 1193  @article{Nickisch2008,\n 1194: 	Author = {Nickisch, Hannes and Rasmussen, Carl Edward},\n 1195  	Date-Added = {2013-04-16 17:12:02 +0000},\n 1196  	Date-Modified = {2013-04-16 17:14:05 +0000},\n ....\n 1623  @incollection{BZMonteCarlo,\n 1624  	Address = {Cambridge, MA},\n 1625: 	Author = {C. E. Rasmussen and Z. Ghahramani},\n 1626  	Booktitle = {Advances in Neural Information Processing Systems},\n 1627  	Date-Added = {2013-04-16 17:05:02 +0000},\n ....\n 1697  	Year = {2012}}\n 1698  \n 1699: @article{rasmussen38gaussian,\n 1700: 	Author = {Rasmussen, C.E. and Williams, CKI},\n 1701  	Date-Added = {2013-04-16 17:05:02 +0000},\n 1702  	Date-Modified = {2013-04-16 17:05:02 +0000},\n ....\n 1741  	Year = {2005}}\n 1742  \n 1743: @article{Rasmussen2005,\n 1744: 	Author = {Rasmussen, C.E. and Williams, C.K.I.},\n 1745  	Date-Added = {2013-04-16 16:35:16 +0000},\n 1746  	Date-Modified = {2013-04-16 16:35:31 +0000},\n ....\n 1881  \n 1882  @inproceedings{Kuss2005,\n 1883: 	Author = {M. Kuss and C. E. Rasmussen},\n 1884  	Booktitle = {NIPS},\n 1885  	Date-Added = {2013-04-16 16:35:16 +0000},\n\n/Users/fhuszar/Dropbox/thesis/latex/part2/01_approximate_inference.tex:\n   22  The likelihood $p(\\data\\vert\\param)$ describes how data is related to the parameters $\\param$, and $p(\\param)$ is a prior distribution which capture's one a priori expectations about what the value of $\\param$ may be. The posterior distribution $p_\\data = p(\\param\\vert\\data)$ captures all statistically relevant information that the data $\\data$ provides about $\\param$, and it is therefore of central importance.\n   23  \n   24: The marginal likelihood, also called the model evidence $Z = \\int p(\\data\\vert\\param) p(\\param) d\\param$ is also of interest. It is often used to quantify how well a Bayesian model -- the combination of likelihood and prior -- fit the data. When the model involves additinal parameters or hyper-parameters that are not modelled probabilistically via prior distributions, it is common practice to learn their values by maximising model evidence \\citep[see \\eg][Chapter 5]{Rasmussen2006}.\n   25  \n   26  In practically interesting Bayesian models, the posterior distribution and model evidence are often computationally intractable to obtain and therefore one has to resort to approximations. The most popular methods for Bayesian approximate inference are variational inference and Markov chain Monte Carlo.\n\n/Users/fhuszar/Dropbox/thesis/latex/part2/02_herding.tex:\n  362  This means that the estimate converges for any bounded measurable function $f$. The speed of convergence, however, may not be as fast.\n  363  \n  364: Therefore it is crucial that the kernel we choose is representative of the function or functions $f$ we will integrate.  For example, in our experiments, the convergence of herding was sensitive to the width of the Gaussian kernel.  One of the major weaknesses of kernel methods in general is the difficulty of setting kernel parameters.  A key benefit of the Bayesian interpretation of herding and MMD presented in this paper is that it provides a recipe for adapting the Hilbert space to the observations $f(x_n)$.  To be precise, we can fit the kernel parameters by maximizing the marginal likelihood of Gaussian process conditioned on the observations.  Details can be found in \\citep{rasmussen38gaussian}.\n  365  \n  366  \\subsection{Computational Complexity}\n\n/Users/fhuszar/Dropbox/thesis/latex/part3/02_GP_BALD.tex:\n   81  BALD exploits the fact that in many active learning applications the output space $\\Ye$ is often simpler than the parameter space $\\Theta$. Here I consider the problem of active learning for binary classification, when the output takes one of two possible values $y \\in \\{-1,1\\}$. Given the simplicity of the outputs, binary classification is a highly relevant use-case for BALD.\n   82  \n   83: I will use a non-parametric Bayesian classification model, Gaussian process classification \\citep[GPC,][]{Rasmussen2006} to demonstrate the usefulness of BALD: GPC appears to be an especially challenging problem for information-theoretic active learning because its parameter space is infinite. Therefore, computing entropy of the posterior involves nontrivial quantities. However, by using the BALD approach and Eqn.\\ \\eqref{eqn:rearrangement} we are able to fully calculate the relevant information quantities without having to work out entropies of infinite dimensional objects. \n   84  \n   85  \n   ..\n   94  \\TODO{I need an appendix on kernels and Gaussian processes}\n   95  \n   96: We consider the probit case where, given the value of $f$, the binary label $y$ takes a Bernoulli distribution with probability $\\Phi(f(\\x))$, and $\\Phi$ is the cumulative distribution function of the normal distribution. For further details on GPC see \\citep{rasmussen2005}.\n   97  \n   98  Posterior inference in the GPC model is intractable; given some observations $\\data$, the posterior over $f$ becomes non-Gaussian and complicated. This is addressed by using approximate inference methods. The most commonly used approximate inference methods for Gaussian process classification are expectation propagation \\citep[EP,][]{Minka2002}, Laplace's approximation \\citep{williams1998}, assumed density filtering \\citep[ADF,][]{csato2000} and sparse methods \\citep{candela05sparseGP}. These all approximate the non-Gaussian posterior by a Gaussian \\citep{Nickisch2008}, but differ in the optimisation criterion and other restrictions. Throughout this chapter I will assume that we are provided with some Gaussian approximation to the GPC posterior resulting from one of these methods, though the active learning method is agnostic as to which method produced this estimate. Given the sequential nature of active learning, fast on-line methods \\citep{Csato2002} are particularly well suited for the task. In our derivation we will use {\\scriptsize$\\stackrel{1}{\\approx}$} to indicate where approximate inference is exploited.\n\n/Users/fhuszar/Dropbox/thesis/raw_materials/BALD-GPC/AL_NIPS2011.tex:\n  124  	f \\sim \\mathrm{GP}(\\mu(\\cdot),k(\\cdot,\\cdot)) \\qquad \\y\\vert\\x,f \\sim\\mathrm{Bernoulli}(\\Phi(f(\\x))) \n  125  \\end{align}\n  126: The latent parameter, now called $f$ (previously denoted as $\\param$), is a function $\\mathcal{X}\\rightarrow\\mathbb{R}$, and is assigned a Gaussian process prior with mean $\\mu(\\cdot)$ and covariance function $k(\\cdot,\\cdot)$. We consider the probit case where given the value of $f$, $y$ takes a Bernoulli distribution with probability $\\Phi(f(\\x))$, and $\\Phi$ is the cumulative distribution function of the normal distribution. For further details on GPC see \\cite{rasmussen2005}.\n  127  \n  128: Inference in the GPC model is intractable; given some observations $\\data$, the posterior over $f$ becomes non-Gaussian and complicated. The most commonly used approximate inference methods -- EP,  Laplace approximation, Assumed Density Filtering and sparse methods -- all approximate the posterior by a Gaussian \\cite{rasmussen2005}. Throughout this section we will assume that we are provided with such a Gaussian approximation from one of these methods, though the active learning algorithm does not care which. In our derivation we will use {\\scriptsize$\\stackrel{1}{\\approx}$} to indicate where such an approximation is exploited.\n  129  \n  130  Now, we will compute the informativeness of a query $\\x$ using Eqn.  \\eqref{eqn:rearrangement}.  The entropy of the binary output variable $y$ given a fixed $f$ can be expressed in terms of the binary entropy function $h$: \n  ...\n  181  \\end{align}\n  182  \n  183: In the context of GP models, hyperparameters typically control the smoothness or spatial length-scale of functions. If we maintain a posterior distribution over these hyperparameters, which we can do e.\\,g.\\ via Hamiltonian Monte Carlo, we can choose either to treat them as nuisance parameters $\\theta^-$ and use Eq.\\ \\ref{eqn:BALD_bipartite}, or to include them in $\\theta^+$ and perform active learning over them as well. In certain cases, such as automatic relevance determination\\cite{rasmussen2005}, it may even make sense to treat hyperparameters as variables of primary interest, and the function $f$ itself as nuisance parameter $\\theta^-$.\n  184  \n  185  \\subsection{Preference Learning}\n\n/Users/fhuszar/Dropbox/thesis/raw_materials/lossBayes/lossBayes.tex:\n  246  \\section{SUPERVISED LEARNING} \\label{s:examples}\n  247  \n  248: In this section, we make our framework more concrete by investigating it in the predictive setting presented in \\secref{ss:predictive}. We recall that in order to apply our framework, we need to specify the loss, the action space, the Bayesian observation model and a tractable family $\\mathcal{Q}$ of approximate distributions over the latent variable $\\theta$. In the predictive setting, an action is a prediction function $h:\\mathcal{X}\\rightarrow\\mathcal{Y}$. We let the action space $\\mathcal{H}$ be the set of all possible such functions here -- we are thus in the non-parametric prediction regime where we are free to make arbitrary pointwise decision on $\\mathcal{X}$. This gives us rich predictive possibilities as well as actually enables us to analytically compute $h_q$, as we will see in the next paragraph. For the observation model, we consider Bayesian non-parametric probabilistic models based on Gaussian processes (GPs), which have been successfully applied to model a wide variety of phenomena in the past~\\citep{Rasmussen2006}. %%OPTIONAL\\footnote{Considering the non-parametric setup reduces the issue of model misspecification. Moreover, GPs are often applied using approximate inference and thus provide a useful illustration for our framework.}.\n  249  In \\secref{ss:GPR}, we first look at Gaussian process regression. In this case, we can obtain an analytic form for $p_\\dataset$ and $\\mathcal{R}_{p_\\dataset}(h_q)$ which gives us some insights about the approximation framework as well as when minimizing the KL divergence can be suboptimal. Because the quadratic cost function is not bounded (and so $M = \\infty$), we cannot directly apply our loss-EM algorithm for regression, but we can nevertheless get useful insights which suggest future research directions for regression with sparse GPs. In section~\\ref{ss:GPC}, we consider Gaussian process classification (GPC) which will provide a test bed for the loss-EM algorithm. In both cases, we use a GP as our prior over parameters and let $\\mathcal{Q}$ also be a family of GPs.\n  250  \n  ...\n  384  To investigate the effect of the test distribution $p(x)$ on our method, we generated three different transductive test sets of size 1000, with inputs sampled from $\\mathcal{U}(0,1)$, $\\mathcal{U}(0.2,1.2)$ and $\\mathcal{U}(0.5,1.5)$ respectively (columns of \\tableref{tab:results}), and repeated these experiments 10 times to get significance results. We used five different loss matrices: the loss for false negatives was constant at $c_{-}=1$, the loss for false positives $c_{+}$ was varied so that the decision threshold $p_{thresh}=\\frac{c_{+}}{c_{-} + c_{+}}$ changed linearly between 0.5 and 0.05 (rows of \\tableref{tab:results}).\n  385  \n  386: For each dataset, we compared three methods for approximate inference: Laplace approximation, expectation propagation (EP) and loss-EM (run separately for each loss and test set combination). Both Laplace and EP are standard approaches to GP classification~\\citep{Rasmussen2006}. To evaluate the performance of the methods, we used the following criterion based on the posterior risk:\n  387  \\addtolength{\\abovedisplayskip}{-1mm}\n  388  \\addtolength{\\belowdisplayskip}{-1mm}\n\n23 matches across 6 files\n\n\nSearching 86 files for \"asmussen\" (case sensitive)\n\n/Users/fhuszar/Dropbox/thesis/latex/bibliography/thesis.bib:\n  146  \n  147  @article{quinonero2005,\n  148: 	Author = {Qui{\\~n}onero-Candela, Joaquin and Rasmussen, Carl Edward},\n  149  	Date-Added = {2013-05-12 16:04:56 +0000},\n  150  	Date-Modified = {2013-05-12 16:05:03 +0000},\n  ...\n  773  \n  774  @article{nickisch2008gpc,\n  775: 	Author = {Nickisch, H. and C. E. Rasmussen},\n  776  	Date-Added = {2013-04-16 17:25:49 +0000},\n  777  	Date-Modified = {2013-04-16 17:25:49 +0000},\n  ...\n  793  \n  794  @article{candela05sparseGP,\n  795: 	Author = {J. Qui{\\`O}onero-Candela and C. E. Rasmussen},\n  796  	Date-Added = {2013-04-16 17:25:49 +0000},\n  797  	Date-Modified = {2013-04-16 17:25:49 +0000},\n  ...\n  958  	Year = {2002}}\n  959  \n  960: @book{Rasmussen2006,\n  961  	Address = {Cambridge, MA, USA},\n  962: 	Author = {Carl Edward Rasmussen and Christopher K. I. Williams},\n  963  	Date-Added = {2013-04-16 17:25:49 +0000},\n  964  	Date-Modified = {2013-04-16 17:25:49 +0000},\n  ...\n 1158  	Year = {2006}}\n 1159  \n 1160: @book{rasmussen2006,\n 1161: 	Author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},\n 1162  	Date-Added = {2013-04-16 17:12:02 +0000},\n 1163  	Date-Modified = {2013-04-16 17:12:02 +0000},\n ....\n 1192  \n 1193  @article{Nickisch2008,\n 1194: 	Author = {Nickisch, Hannes and Rasmussen, Carl Edward},\n 1195  	Date-Added = {2013-04-16 17:12:02 +0000},\n 1196  	Date-Modified = {2013-04-16 17:14:05 +0000},\n ....\n 1623  @incollection{BZMonteCarlo,\n 1624  	Address = {Cambridge, MA},\n 1625: 	Author = {C. E. Rasmussen and Z. Ghahramani},\n 1626  	Booktitle = {Advances in Neural Information Processing Systems},\n 1627  	Date-Added = {2013-04-16 17:05:02 +0000},\n ....\n 1697  	Year = {2012}}\n 1698  \n 1699: @article{Rasmussen2006,\n 1700: 	Author = {Rasmussen, C.E. and Williams, CKI},\n 1701  	Date-Added = {2013-04-16 17:05:02 +0000},\n 1702  	Date-Modified = {2013-04-16 17:05:02 +0000},\n ....\n 1741  	Year = {2005}}\n 1742  \n 1743: @article{Rasmussen2005,\n 1744: 	Author = {Rasmussen, C.E. and Williams, C.K.I.},\n 1745  	Date-Added = {2013-04-16 16:35:16 +0000},\n 1746  	Date-Modified = {2013-04-16 16:35:31 +0000},\n ....\n 1881  \n 1882  @inproceedings{Kuss2005,\n 1883: 	Author = {M. Kuss and C. E. Rasmussen},\n 1884  	Booktitle = {NIPS},\n 1885  	Date-Added = {2013-04-16 16:35:16 +0000},\n\n/Users/fhuszar/Dropbox/thesis/latex/part2/01_approximate_inference.tex:\n   22  The likelihood $p(\\data\\vert\\param)$ describes how data is related to the parameters $\\param$, and $p(\\param)$ is a prior distribution which capture's one a priori expectations about what the value of $\\param$ may be. The posterior distribution $p_\\data = p(\\param\\vert\\data)$ captures all statistically relevant information that the data $\\data$ provides about $\\param$, and it is therefore of central importance.\n   23  \n   24: The marginal likelihood, also called the model evidence $Z = \\int p(\\data\\vert\\param) p(\\param) d\\param$ is also of interest. It is often used to quantify how well a Bayesian model -- the combination of likelihood and prior -- fit the data. When the model involves additinal parameters or hyper-parameters that are not modelled probabilistically via prior distributions, it is common practice to learn their values by maximising model evidence \\citep[see \\eg][Chapter 5]{Rasmussen2006}.\n   25  \n   26  In practically interesting Bayesian models, the posterior distribution and model evidence are often computationally intractable to obtain and therefore one has to resort to approximations. The most popular methods for Bayesian approximate inference are variational inference and Markov chain Monte Carlo.\n\n/Users/fhuszar/Dropbox/thesis/latex/part2/02_herding.tex:\n  362  This means that the estimate converges for any bounded measurable function $f$. The speed of convergence, however, may not be as fast.\n  363  \n  364: Therefore it is crucial that the kernel we choose is representative of the function or functions $f$ we will integrate.  For example, in our experiments, the convergence of herding was sensitive to the width of the Gaussian kernel.  One of the major weaknesses of kernel methods in general is the difficulty of setting kernel parameters.  A key benefit of the Bayesian interpretation of herding and MMD presented in this paper is that it provides a recipe for adapting the Hilbert space to the observations $f(x_n)$.  To be precise, we can fit the kernel parameters by maximizing the marginal likelihood of Gaussian process conditioned on the observations.  Details can be found in \\citep{Rasmussen2006}.\n  365  \n  366  \\subsection{Computational Complexity}\n\n/Users/fhuszar/Dropbox/thesis/latex/part3/02_GP_BALD.tex:\n   81  BALD exploits the fact that in many active learning applications the output space $\\Ye$ is often simpler than the parameter space $\\Theta$. Here I consider the problem of active learning for binary classification, when the output takes one of two possible values $y \\in \\{-1,1\\}$. Given the simplicity of the outputs, binary classification is a highly relevant use-case for BALD.\n   82  \n   83: I will use a non-parametric Bayesian classification model, Gaussian process classification \\citep[GPC,][]{Rasmussen2006} to demonstrate the usefulness of BALD: GPC appears to be an especially challenging problem for information-theoretic active learning because its parameter space is infinite. Therefore, computing entropy of the posterior involves nontrivial quantities. However, by using the BALD approach and Eqn.\\ \\eqref{eqn:rearrangement} we are able to fully calculate the relevant information quantities without having to work out entropies of infinite dimensional objects. \n   84  \n   85  \n   ..\n   94  \\TODO{I need an appendix on kernels and Gaussian processes}\n   95  \n   96: We consider the probit case where, given the value of $f$, the binary label $y$ takes a Bernoulli distribution with probability $\\Phi(f(\\x))$, and $\\Phi$ is the cumulative distribution function of the normal distribution. For further details on GPC see \\citep{rasmussen2005}.\n   97  \n   98  Posterior inference in the GPC model is intractable; given some observations $\\data$, the posterior over $f$ becomes non-Gaussian and complicated. This is addressed by using approximate inference methods. The most commonly used approximate inference methods for Gaussian process classification are expectation propagation \\citep[EP,][]{Minka2002}, Laplace's approximation \\citep{williams1998}, assumed density filtering \\citep[ADF,][]{csato2000} and sparse methods \\citep{candela05sparseGP}. These all approximate the non-Gaussian posterior by a Gaussian \\citep{Nickisch2008}, but differ in the optimisation criterion and other restrictions. Throughout this chapter I will assume that we are provided with some Gaussian approximation to the GPC posterior resulting from one of these methods, though the active learning method is agnostic as to which method produced this estimate. Given the sequential nature of active learning, fast on-line methods \\citep{Csato2002} are particularly well suited for the task. In our derivation we will use {\\scriptsize$\\stackrel{1}{\\approx}$} to indicate where approximate inference is exploited.\n\n/Users/fhuszar/Dropbox/thesis/raw_materials/BALD-GPC/AL_NIPS2011.tex:\n  124  	f \\sim \\mathrm{GP}(\\mu(\\cdot),k(\\cdot,\\cdot)) \\qquad \\y\\vert\\x,f \\sim\\mathrm{Bernoulli}(\\Phi(f(\\x))) \n  125  \\end{align}\n  126: The latent parameter, now called $f$ (previously denoted as $\\param$), is a function $\\mathcal{X}\\rightarrow\\mathbb{R}$, and is assigned a Gaussian process prior with mean $\\mu(\\cdot)$ and covariance function $k(\\cdot,\\cdot)$. We consider the probit case where given the value of $f$, $y$ takes a Bernoulli distribution with probability $\\Phi(f(\\x))$, and $\\Phi$ is the cumulative distribution function of the normal distribution. For further details on GPC see \\cite{rasmussen2005}.\n  127  \n  128: Inference in the GPC model is intractable; given some observations $\\data$, the posterior over $f$ becomes non-Gaussian and complicated. The most commonly used approximate inference methods -- EP,  Laplace approximation, Assumed Density Filtering and sparse methods -- all approximate the posterior by a Gaussian \\cite{rasmussen2005}. Throughout this section we will assume that we are provided with such a Gaussian approximation from one of these methods, though the active learning algorithm does not care which. In our derivation we will use {\\scriptsize$\\stackrel{1}{\\approx}$} to indicate where such an approximation is exploited.\n  129  \n  130  Now, we will compute the informativeness of a query $\\x$ using Eqn.  \\eqref{eqn:rearrangement}.  The entropy of the binary output variable $y$ given a fixed $f$ can be expressed in terms of the binary entropy function $h$: \n  ...\n  181  \\end{align}\n  182  \n  183: In the context of GP models, hyperparameters typically control the smoothness or spatial length-scale of functions. If we maintain a posterior distribution over these hyperparameters, which we can do e.\\,g.\\ via Hamiltonian Monte Carlo, we can choose either to treat them as nuisance parameters $\\theta^-$ and use Eq.\\ \\ref{eqn:BALD_bipartite}, or to include them in $\\theta^+$ and perform active learning over them as well. In certain cases, such as automatic relevance determination\\cite{rasmussen2005}, it may even make sense to treat hyperparameters as variables of primary interest, and the function $f$ itself as nuisance parameter $\\theta^-$.\n  184  \n  185  \\subsection{Preference Learning}\n\n/Users/fhuszar/Dropbox/thesis/raw_materials/lossBayes/lossBayes.tex:\n  246  \\section{SUPERVISED LEARNING} \\label{s:examples}\n  247  \n  248: In this section, we make our framework more concrete by investigating it in the predictive setting presented in \\secref{ss:predictive}. We recall that in order to apply our framework, we need to specify the loss, the action space, the Bayesian observation model and a tractable family $\\mathcal{Q}$ of approximate distributions over the latent variable $\\theta$. In the predictive setting, an action is a prediction function $h:\\mathcal{X}\\rightarrow\\mathcal{Y}$. We let the action space $\\mathcal{H}$ be the set of all possible such functions here -- we are thus in the non-parametric prediction regime where we are free to make arbitrary pointwise decision on $\\mathcal{X}$. This gives us rich predictive possibilities as well as actually enables us to analytically compute $h_q$, as we will see in the next paragraph. For the observation model, we consider Bayesian non-parametric probabilistic models based on Gaussian processes (GPs), which have been successfully applied to model a wide variety of phenomena in the past~\\citep{Rasmussen2006}. %%OPTIONAL\\footnote{Considering the non-parametric setup reduces the issue of model misspecification. Moreover, GPs are often applied using approximate inference and thus provide a useful illustration for our framework.}.\n  249  In \\secref{ss:GPR}, we first look at Gaussian process regression. In this case, we can obtain an analytic form for $p_\\dataset$ and $\\mathcal{R}_{p_\\dataset}(h_q)$ which gives us some insights about the approximation framework as well as when minimizing the KL divergence can be suboptimal. Because the quadratic cost function is not bounded (and so $M = \\infty$), we cannot directly apply our loss-EM algorithm for regression, but we can nevertheless get useful insights which suggest future research directions for regression with sparse GPs. In section~\\ref{ss:GPC}, we consider Gaussian process classification (GPC) which will provide a test bed for the loss-EM algorithm. In both cases, we use a GP as our prior over parameters and let $\\mathcal{Q}$ also be a family of GPs.\n  250  \n  ...\n  384  To investigate the effect of the test distribution $p(x)$ on our method, we generated three different transductive test sets of size 1000, with inputs sampled from $\\mathcal{U}(0,1)$, $\\mathcal{U}(0.2,1.2)$ and $\\mathcal{U}(0.5,1.5)$ respectively (columns of \\tableref{tab:results}), and repeated these experiments 10 times to get significance results. We used five different loss matrices: the loss for false negatives was constant at $c_{-}=1$, the loss for false positives $c_{+}$ was varied so that the decision threshold $p_{thresh}=\\frac{c_{+}}{c_{-} + c_{+}}$ changed linearly between 0.5 and 0.05 (rows of \\tableref{tab:results}).\n  385  \n  386: For each dataset, we compared three methods for approximate inference: Laplace approximation, expectation propagation (EP) and loss-EM (run separately for each loss and test set combination). Both Laplace and EP are standard approaches to GP classification~\\citep{Rasmussen2006}. To evaluate the performance of the methods, we used the following criterion based on the posterior risk:\n  387  \\addtolength{\\abovedisplayskip}{-1mm}\n  388  \\addtolength{\\belowdisplayskip}{-1mm}\n\n23 matches across 6 files\n",
			"settings":
			{
				"buffer_size": 45241,
				"line_ending": "Unix",
				"name": "Find Results",
				"scratch": true
			}
		},
		{
			"file": "latex/bibliography/thesis.bib",
			"settings":
			{
				"buffer_size": 90805,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/conclusions.tex",
			"settings":
			{
				"buffer_size": 4650,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/introduction.tex",
			"settings":
			{
				"buffer_size": 36,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/notation.tex",
			"settings":
			{
				"buffer_size": 4122,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/part1_main.tex",
			"settings":
			{
				"buffer_size": 366,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/part2_main.tex",
			"settings":
			{
				"buffer_size": 279,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/part3_main.tex",
			"settings":
			{
				"buffer_size": 357,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/preamble.tex",
			"settings":
			{
				"buffer_size": 1869,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/frontmatter/abstract.tex",
			"settings":
			{
				"buffer_size": 159,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/frontmatter/acknowledgement.tex",
			"settings":
			{
				"buffer_size": 90,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/frontmatter/dedication.tex",
			"settings":
			{
				"buffer_size": 165,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/part1/02_embeddings.tex",
			"settings":
			{
				"buffer_size": 38366,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/part2/01_approximate_inference.tex",
			"settings":
			{
				"buffer_size": 23611,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/part2/02_herding.tex",
			"settings":
			{
				"buffer_size": 41251,
				"line_ending": "Unix"
			}
		},
		{
			"file": "raw_materials/lossBayes/lossBayes.tex",
			"settings":
			{
				"buffer_size": 64505,
				"line_ending": "Windows"
			}
		},
		{
			"file": "latex/part3/01_introBALD.tex",
			"settings":
			{
				"buffer_size": 21682,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/part1/02_embeddings.tex.bak",
			"settings":
			{
				"buffer_size": 38116,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/part1/03_scoring_processes.tex",
			"settings":
			{
				"buffer_size": 21176,
				"line_ending": "Unix"
			}
		},
		{
			"settings":
			{
				"buffer_size": 0,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/part3/02_GP_BALD.tex",
			"settings":
			{
				"buffer_size": 47083,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/part3/03_quantum.tex",
			"settings":
			{
				"buffer_size": 45302,
				"line_ending": "Unix"
			}
		},
		{
			"file": "raw_materials/BALD-GPC/AL_NIPS2011.tex",
			"settings":
			{
				"buffer_size": 35262,
				"line_ending": "Windows"
			}
		},
		{
			"settings":
			{
				"buffer_size": 0,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/thesis.tex",
			"settings":
			{
				"buffer_size": 1433,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/figs/BALD/quantum/case1.tikz",
			"settings":
			{
				"buffer_size": 54634,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/figs/BALD/quantum/case2.tikz",
			"settings":
			{
				"buffer_size": 54765,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/figs/BALD/quantum/case3.tikz",
			"settings":
			{
				"buffer_size": 54921,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/figs/BALD/quantum/case4.tikz",
			"settings":
			{
				"buffer_size": 54431,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/figs/BALD/quantum/case5.tikz",
			"settings":
			{
				"buffer_size": 53115,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/figs/BALD/quantum/case5b.tikz",
			"settings":
			{
				"buffer_size": 2283,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/figs/BALD/quantum/case6.tikz",
			"settings":
			{
				"buffer_size": 53228,
				"line_ending": "Unix"
			}
		},
		{
			"file": "latex/figs/BALD/quantum/one_qubit_combined.tikz.tex",
			"settings":
			{
				"buffer_size": 3882,
				"line_ending": "Unix"
			}
		}
	],
	"build_system": "",
	"command_palette":
	{
		"height": 107.0,
		"selected_items":
		[
			[
				"figle",
				"Figlet: Add Comment"
			],
			[
				"fig",
				"Figlet: Add Comment"
			],
			[
				"figlet",
				"Figlet: Add Comment"
			],
			[
				"sy bash",
				"Set Syntax: Shell Script (Bash)"
			],
			[
				"indespa",
				"Indentation: Convert to Spaces"
			],
			[
				"markdow",
				"Set Syntax: Markdown"
			],
			[
				"markdo",
				"Set Syntax: Markdown"
			],
			[
				"inde",
				"Indentation: Reindent Lines"
			],
			[
				"inde spa",
				"Indentation: Convert to Spaces"
			],
			[
				"figlet se",
				"Figlet: Select Font"
			],
			[
				"fglcmm",
				"Figlet: Add Comment"
			],
			[
				"sybash",
				"Set Syntax: Shell Script (Bash)"
			],
			[
				"figlet sele",
				"Figlet: Select Font"
			],
			[
				"sy py",
				"Set Syntax: Python"
			],
			[
				"upper",
				"Convert Case: Upper Case"
			],
			[
				"figle fo",
				"Figlet: Select Font"
			],
			[
				"inspac",
				"Indentation: Convert to Spaces"
			],
			[
				"synsql",
				"Set Syntax: SQL"
			],
			[
				"indesa",
				"Indentation: Convert to Spaces"
			],
			[
				"sy sql",
				"Set Syntax: SQL"
			],
			[
				"sys",
				"Set Syntax: SQL"
			],
			[
				"LOWER",
				"Convert Case: Lower Case"
			],
			[
				"INDE SPA",
				"Indentation: Convert to Spaces"
			],
			[
				"sysql",
				"Set Syntax: SQL"
			],
			[
				"inde space",
				"Indentation: Convert to Spaces"
			],
			[
				"sy s",
				"Set Syntax: SQL"
			],
			[
				"in spac",
				"Indentation: Convert to Spaces"
			],
			[
				"sy json",
				"Set Syntax: JSON"
			],
			[
				"inspa",
				"Indentation: Convert to Spaces"
			],
			[
				"IND ",
				"Indentation: Convert to Tabs"
			],
			[
				"ind tab",
				"Indentation: Convert to Tabs"
			],
			[
				"ind sp",
				"Indentation: Convert to Spaces"
			],
			[
				"sy sq",
				"Set Syntax: SQL"
			],
			[
				"syn sql",
				"Set Syntax: SQL"
			],
			[
				"inde spac",
				"Indentation: Convert to Spaces"
			],
			[
				"SYN SQL",
				"Set Syntax: SQL"
			],
			[
				"sy spa",
				"Set Syntax: Java Server Page (JSP)"
			],
			[
				"inden space",
				"Indentation: Convert to Spaces"
			],
			[
				"SY SQ",
				"Set Syntax: SQL"
			],
			[
				"inde sp",
				"Indentation: Convert to Spaces"
			],
			[
				"synta sql",
				"Set Syntax: SQL"
			],
			[
				"packa",
				"Package Control: Install Package"
			],
			[
				"maven ",
				"Maven: Run ..."
			],
			[
				"maven t",
				"Maven: Test"
			],
			[
				"mvnt",
				"Maven: Test"
			],
			[
				"mvn t",
				"Maven: Test"
			],
			[
				"save all",
				"File: Save All"
			],
			[
				"mvnts",
				"Maven: Test"
			],
			[
				"mvntst",
				"Maven: Test"
			],
			[
				"mav test",
				"Maven: Test"
			],
			[
				"mav",
				"Maven: Test"
			],
			[
				"mave",
				"Maven: Test"
			],
			[
				"mvn",
				"Maven: Test"
			],
			[
				"maven",
				"Maven: Test"
			],
			[
				"ma",
				"Maven: Run clean install"
			],
			[
				"git",
				"Package Control: Install Package"
			],
			[
				"package",
				"Package Control: List Packages"
			],
			[
				"indeta",
				"Indentation: Convert to Tabs"
			],
			[
				"sypy",
				"Set Syntax: Python"
			],
			[
				"syn javas",
				"Set Syntax: JavaScript"
			],
			[
				"ind ta",
				"Indentation: Convert to Tabs"
			],
			[
				"sy jsr",
				"Set Syntax: JavaScript (Rails)"
			],
			[
				"sy java",
				"Set Syntax: Java"
			],
			[
				"SY BASH",
				"Set Syntax: Shell Script (Bash)"
			],
			[
				"in ta",
				"Indentation: Convert to Tabs"
			],
			[
				"indent ta",
				"Indentation: Convert to Tabs"
			],
			[
				"synpy",
				"Set Syntax: Python"
			],
			[
				"convert tab",
				"Indentation: Convert to Tabs"
			],
			[
				"convert spa",
				"Indentation: Convert to Spaces"
			],
			[
				"pack ins",
				"Package Control: Install Package"
			],
			[
				"syn py",
				"Set Syntax: Python"
			],
			[
				"spaces",
				"Indentation: Convert to Spaces"
			],
			[
				"mvn ",
				"Maven: Run ..."
			],
			[
				"pain",
				"Package Control: Install Package"
			],
			[
				"pa ins",
				"Package Control: Install Package"
			],
			[
				"packa instal",
				"Package Control: Install Package"
			],
			[
				"PACK IN ",
				"Package Control: Install Package"
			],
			[
				"pac ins",
				"Package Control: Install Package"
			],
			[
				"inden",
				"Indentation: Convert to Spaces"
			],
			[
				"indent",
				"Indentation: Convert to Tabs"
			]
		],
		"width": 449.0
	},
	"console":
	{
		"height": 125.0
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"file_history":
	[
		"/Users/fhuszar/svn/papers/nips-11-BALD/AL_NIPS2011.tex",
		"/Users/fhuszar/Dropbox/thesis/raw_materials/BALD-GPC/AL_NIPS2011_supp.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/figs/BALD/GPC/austra2.tikz",
		"/Users/fhuszar/Dropbox/thesis/latex/figs/BALD/GPC/isolet2.tikz",
		"/Users/fhuszar/Dropbox/thesis/latex/figs/BALD/GPC/vehicle2.tikz",
		"/Users/fhuszar/Dropbox/thesis/latex/figs/BALD/GPC/wdbc2.tikz",
		"/Users/fhuszar/Dropbox/thesis/latex/figs/BALD/GPC/wine2.tikz",
		"/Users/fhuszar/Dropbox/thesis/latex/figs/BALD/GPC/unused/grid2.tikz",
		"/Users/fhuszar/Dropbox/thesis/latex/part3/02_GP_BALD.tex.bak",
		"/Users/fhuszar/Dropbox/thesis/latex/conclusions.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/conclusions.tex.bak",
		"/Users/fhuszar/Dropbox/thesis/latex/quantum.tex",
		"/Users/fhuszar/peerindex/datascience/dbpedia/descendants.py",
		"/Users/fhuszar/Dropbox/thesis/latex/part3/01_introBALD.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/part3/01_introBALD.tex.bak",
		"/Users/fhuszar/Dropbox/thesis/latex/part3_main.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/main.tex",
		"/Users/fhuszar/Dropbox/thesis/check_spelling.tex",
		"/Users/fhuszar/Dropbox/thesis/check_spelling.sh",
		"/Users/fhuszar/Dropbox/thesis/latex/frontmatter/acknowledgement.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/frontmatter/abstract.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/frontmatter/dedication.tex",
		"/Users/fhuszar/Dropbox/thesis/README.md",
		"/Users/fhuszar/Downloads/INSERT-oneshot-user_info_update-to-user_info.hql",
		"/Users/fhuszar/peerindex/datapipeline/batch-pipeline/src/main/resources/hive/ddl/destinations/CREATE-TABLE-actor_feature.hql",
		"/Users/fhuszar/Dropbox/thesis/latex/preamble.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/CUEDthesisPSnPDF/thesis.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/part2/02_herding.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/part2/01_approximate_inference.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/part2/01_approximate_inference.tex.bak",
		"/Users/fhuszar/Dropbox/thesis/latex/part1_main.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/part3/03_quantum.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/part3/03_quantum.tex.bak",
		"/Users/fhuszar/Dropbox/thesis/title_variations.txt",
		"/Users/fhuszar/Dropbox/thesis/latex/part3/quantum_foobar.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/part3/quantum_PRA.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/part3/tmp",
		"/Users/fhuszar/Dropbox/thesis/latex/part2_main.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/figs/BALD/GPC/checkerboard2.tikz",
		"/Users/fhuszar/Dropbox/thesis/latex/figs/BALD/GPC/blockinmiddle2.tikz",
		"/Users/fhuszar/Dropbox/thesis/latex/figs/BALD/GPC/blockincorner2.tikz",
		"/Users/fhuszar/Dropbox/thesis/latex/figs/BALD/GPC/cancerB2.tikz",
		"/Users/fhuszar/Dropbox/thesis/latex/figs/BALD/GPC/prefcart2.tikz",
		"/Users/fhuszar/Dropbox/thesis/latex/figs/BALD/GPC/prefcpu2.tikz",
		"/Users/fhuszar/Dropbox/thesis/latex/figs/BALD/GPC/prefkinem2.tikz",
		"/Users/fhuszar/Dropbox/thesis/latex/thesis.bib",
		"/Users/fhuszar/Dropbox/thesis/latex/CUEDthesisPSnPDF.cls",
		"/Users/fhuszar/peerindex/datapipeline/batch-pipeline/src/main/resources/hive/ddl/sources/CREATE-TABLE-url_topics.hql",
		"/Users/fhuszar/Dropbox/thesis/raw_materials/collaborative_preference_GP/model/model.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/notation.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/part3/02_GP_BALD.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/part1/01_scoring_rules.tex",
		"/Users/fhuszar/peerindex/datascience/wefollow/twittersugg",
		"/Users/fhuszar/peerindex/datascience/wefollow/suggestions.json",
		"/Users/fhuszar/peerindex/datascience/wefollow/suggestions_Mischa.json",
		"/Users/fhuszar/peerindex/datascience/wefollow/nonnegreg.py",
		"/Users/fhuszar/peerindex/datascience/wefollow/nonnegative_classification.py",
		"/Users/fhuszar/peerindex/datascience/wefollow/test.csv",
		"/Users/fhuszar/peerindex/datascience/wefollow/wefollow_hashtags.hql",
		"/Users/fhuszar/02-INSERT-user_info.hql",
		"/Users/fhuszar/peerindex/datapipeline/batch-pipeline/src/main/resources/hive/ddl/destinations/CREATE-TABLE-actor_json.hql",
		"/Users/fhuszar/peerindex/datascience/getfollowers/CREATE TABLE comedycentral_channels_specific_topic",
		"/Users/fhuszar/peerindex/datapipeline/batch-pipeline/src/main/resources/hive/ddl/destinations/CREATE-TABLE-influence_graph.hql",
		"/Users/fhuszar/svn/papers/quantum/PRA/quantum_bald.tex",
		"/Users/fhuszar/svn/papers/quantum/quantum_bald.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/quantum2.tex",
		"/Users/fhuszar/Dropbox/thesis/latex/snippets.tex",
		"/Users/fhuszar/peerindex/datapipeline/hivescripts/community.hive.hql",
		"/Users/fhuszar/peerindex/datascience/hotornot_py/django_version/peerindex/peerindex/settings.py",
		"/Users/fhuszar/svn/simon/papers/thesis/thesis.tex",
		"/Users/fhuszar/svn/papers/quantum/quantum_bald",
		"/Users/fhuszar/svn/papers/nips-11-BALD/AL_NIPS2011_supp.tex",
		"/Users/fhuszar/peerindex/datascience/scoretraining/data/azeem_target.tsv",
		"/Users/fhuszar/peerindex/datascience/jite/train_model.py",
		"/Users/fhuszar/peerindex/datascience/getfollowers/hivescripts.sql",
		"/Users/fhuszar/peerindex/datascience/getfollowers/countries.tsv",
		"/Users/fhuszar/peerindex/datascience/getfollowers/specific_topic",
		"/Users/fhuszar/peerindex/datapipeline/ls",
		"/Users/fhuszar/peerindex/datapipeline/batch-pipeline/src/main/resources/hive/hql-daily/02-INSERT-action-to-influence_graph.hql",
		"/Users/fhuszar/peerindex/datapipeline/batch-pipeline/src/main/resources/hive/hql-daily/02-INSERT-user_info_update-to-user_info.hql",
		"/Users/fhuszar/foo/foo.txt",
		"/Users/fhuszar/Downloads/50e39dd0-7faf-446f-86a8-5da1a8e1cd63.json",
		"/Users/fhuszar/peerindex/datapipeline/batch-pipeline/src/test/resources/url_topics/dt=20130214/part-000.json",
		"/Users/fhuszar/peerindex/datapipeline/mockdata/url_topics/dt=20130214/part-000.json",
		"/Users/fhuszar/peerindex/datapipeline/batch-pipeline/src/test/resources/action/dt=20130214/part-000.json",
		"/Users/fhuszar/peerindex/datapipeline/hivescripts/action.hql",
		"/Users/fhuszar/peerindex/stream/hivescripts/create_tables.hql.sql",
		"/Users/fhuszar/peerindex/stream/datamodel/and",
		"/Users/fhuszar/peerindex/stream/datamodel/i",
		"/Users/fhuszar/peerindex/stream/datamodel/love",
		"/Users/fhuszar/peerindex/stream/datamodel/him",
		"/Users/fhuszar/peerindex/stream/datamodel/more",
		"/Users/fhuszar/peerindex/stream/datamodel/than",
		"/Users/fhuszar/peerindex/stream/datamodel/ever.\",\"verified\":false,\"contributors_enabled\":false,\"profile_sidebar_border_color\":\"AB4ED9\",\"name\":\"Sarah\",\"profile_background_color\"",
		"/Users/fhuszar/peerindex/stream/datamodel/Mar",
		"/Users/fhuszar/peerindex/stream/datamodel/06",
		"/Users/fhuszar/peerindex/stream/datamodel/04",
		"/Users/fhuszar/peerindex/stream/datamodel/milk",
		"/Users/fhuszar/peerindex/stream/datamodel/with",
		"/Users/fhuszar/peerindex/stream/datamodel/@TheRealHartmann",
		"/Users/fhuszar/peerindex/stream/datamodel/👌❤\",\"geo\":null,\"retweeted\":false,\"in_reply_to_screen_name\":null,\"truncated\":false,\"lang\":\"en\",\"entities\":{\"urls\":[],\"hashtags\":[],\"user_mentions\":[{\"id\"",
		"/Users/fhuszar/peerindex/stream/datamodel/Hartman\",\"indices\":[23,39],\"screen_name\":\"TheRealHartmann\",\"id_str\":\"795056388\"}]},\"in_reply_to_status_id_str\":null,\"id\"",
		"/Users/fhuszar/peerindex/stream/datamodel/href=\\\"http",
		"/Users/fhuszar/peerindex/stream/datamodel/rel=\\\"nofollow\\\">Twitter",
		"/Users/fhuszar/peerindex/stream/datamodel/for",
		"/Users/fhuszar/peerindex/stream/datamodel/iPhone</a>\",\"in_reply_to_user_id_str\":null,\"favorited\":false,\"in_reply_to_status_id\":null,\"retweet_count\"",
		"/Users/fhuszar/peerindex/stream/datamodel/Jan",
		"/Users/fhuszar/peerindex/stream/datamodel/11",
		"/Users/fhuszar/peerindex/stream/datamodel/16",
		"/Users/fhuszar/peerindex/stream/datamodel/+0000",
		"/Users/fhuszar/peerindex/stream/datamodel/2011\",\"default_profile_image\":false,\"followers_count\":277,\"profile_image_url_https\":\"https:/si0.twimg.com/profile_images/3038748500/a86b7994310266a4fb14d19a6e202d6c_normal.jpeg\",\"geo_enabled\":false,\"profile_background_image_url\":\"http:/a0.twimg.com/profile_background_images/489054432/marylin.jpg\",\"profile_background_image_url_https\":\"https:/si0.twimg.com/profile_background_images/489054432/marylin.jpg\",\"follow_request_sent\":null,\"url\":null,\"utc_offset\":null,\"time_zone\":null,\"notifications\":null,\"profile_use_background_image\":true,\"friends_count\":277,\"profile_sidebar_fill_color\":\"0A0108\",\"screen_name\":\"S_gazerro720\",\"id_str\":\"261537894\",\"profile_image_url\":\"http:/a0.twimg.com/profile_images/3038748500/a86b7994310266a4fb14d19a6e202d6c_normal.jpeg\",\"listed_count\":0,\"is_translator\":false},\"coordinates\":null,\"_peerindex\":{\"topic\":[{\"id\":0,\"score\":0.0},{\"id\":1,\"score\":1.0},{\"id\":2,\"score\":2.0},{\"id\":3,\"score\":0.0},{\"id\":4,\"score\":1.0},{\"id\":5,\"score\":2.0},{\"id\":6,\"score\":0.0},{\"id\":7,\"score\":1.0},{\"id\":8,\"score\":2.0},{\"id\":9,\"score\":0.0},{\"id\":10,\"score\":1.0},{\"id\":11,\"score\":2.0},{\"id\":12,\"score\":0.0},{\"id\":13,\"score\":1.0},{\"id\":14,\"score\":2.0},{\"id\":15,\"score\":0.0},{\"id\":16,\"score\":1.0},{\"id\":17,\"score\":2.0},{\"id\":18,\"score\":0.0},{\"id\":19,\"score\":1.0},{\"id\":20,\"score\":2.0},{\"id\":21,\"score\":0.0},{\"id\":22,\"score\":1.0},{\"id\":23,\"score\":2.0},{\"id\":24,\"score\":0.0},{\"id\":25,\"score\":1.0},{\"id\":26,\"score\":2.0},{\"id\":27,\"score\":0.0},{\"id\":28,\"score\":1.0},{\"id\":29,\"score\":2.0},{\"id\":30,\"score\":0.0},{\"id\":31,\"score\":1.0},{\"id\":32,\"score\":2.0},{\"id\":33,\"score\":0.0},{\"id\":34,\"score\":1.0},{\"id\":35,\"score\":2.0},{\"id\":36,\"score\":0.0},{\"id\":37,\"score\":1.0},{\"id\":38,\"score\":2.0},{\"id\":39,\"score\":0.0},{\"id\":40,\"score\":1.0},{\"id\":41,\"score\":2.0},{\"id\":42,\"score\":0.0},{\"id\":43,\"score\":1.0},{\"id\":44,\"score\":2.0},{\"id\":45,\"score\":0.0},{\"id\":46,\"score\":1.0},{\"id\":47,\"score\":2.0},{\"id\":48,\"score\":0.0},{\"id\":49,\"score\":1.0},{\"id\":50,\"score\":2.0},{\"id\":51,\"score\":0.0},{\"id\":52,\"score\":1.0},{\"id\":53,\"score\":2.0},{\"id\":54,\"score\":0.0},{\"id\":55,\"score\":1.0},{\"id\":56,\"score\":2.0},{\"id\":57,\"score\":0.0},{\"id\":58,\"score\":1.0},{\"id\":59,\"score\":2.0},{\"id\":60,\"score\":0.0},{\"id\":61,\"score\":1.0},{\"id\":62,\"score\":2.0},{\"id\":63,\"score\":0.0},{\"id\":64,\"score\":1.0},{\"id\":65,\"score\":2.0},{\"id\":66,\"score\":0.0},{\"id\":67,\"score\":1.0},{\"id\":68,\"score\":2.0},{\"id\":69,\"score\":0.0},{\"id\":70,\"score\":1.0},{\"id\":71,\"score\":2.0},{\"id\":72,\"score\":0.0},{\"id\":73,\"score\":1.0},{\"id\":74,\"score\":2.0},{\"id\":75,\"score\":0.0},{\"id\":76,\"score\":1.0},{\"id\":77,\"score\":2.0},{\"id\":78,\"score\":0.0},{\"id\":79,\"score\":1.0},{\"id\":80,\"score\":2.0},{\"id\":81,\"score\":0.0},{\"id\":82,\"score\":1.0},{\"id\":83,\"score\":2.0},{\"id\":84,\"score\":0.0},{\"id\":85,\"score\":1.0},{\"id\":86,\"score\":2.0},{\"id\":87,\"score\":0.0},{\"id\":88,\"score\":1.0},{\"id\":89,\"score\":2.0},{\"id\":90,\"score\":0.0},{\"id\":91,\"score\":1.0},{\"id\":92,\"score\":2.0},{\"id\":93,\"score\":0.0},{\"id\":94,\"score\":1.0},{\"id\":95,\"score\":2.0},{\"id\":96,\"score\":0.0},{\"id\":97,\"score\":1.0},{\"id\":98,\"score\":2.0},{\"id\":99,\"score\":0.0},{\"id\":100,\"score\":1.0},{\"id\":101,\"score\":2.0},{\"id\":102,\"score\":0.0},{\"id\":103,\"score\":1.0},{\"id\":104,\"score\":2.0},{\"id\":105,\"score\":0.0},{\"id\":106,\"score\":1.0},{\"id\":107,\"score\":2.0},{\"id\":108,\"score\":0.0},{\"id\":109,\"score\":1.0},{\"id\":110,\"score\":2.0},{\"id\":111,\"score\":0.0},{\"id\":112,\"score\":1.0},{\"id\":113,\"score\":2.0},{\"id\":114,\"score\":0.0},{\"id\":115,\"score\":1.0},{\"id\":116,\"score\":2.0},{\"id\":117,\"score\":0.0},{\"id\":118,\"score\":1.0},{\"id\":119,\"score\":2.0},{\"id\":120,\"score\":0.0},{\"id\":121,\"score\":1.0},{\"id\":122,\"score\":2.0},{\"id\":123,\"score\":0.0},{\"id\":124,\"score\":1.0},{\"id\":125,\"score\":2.0},{\"id\":126,\"score\":0.0},{\"id\":127,\"score\":1.0},{\"id\":128,\"score\":2.0},{\"id\":129,\"score\":0.0},{\"id\":130,\"score\":1.0},{\"id\":131,\"score\":2.0},{\"id\":132,\"score\":0.0},{\"id\":133,\"score\":1.0},{\"id\":134,\"score\":2.0},{\"id\":135,\"score\":0.0},{\"id\":136,\"score\":1.0},{\"id\":137,\"score\":2.0},{\"id\":138,\"score\":0.0},{\"id\":139,\"score\":1.0},{\"id\":140,\"score\":2.0},{\"id\":141,\"score\":0.0},{\"id\":142,\"score\":1.0},{\"id\":143,\"score\":2.0},{\"id\":144,\"score\":0.0},{\"id\":145,\"score\":1.0},{\"id\":146,\"score\":2.0},{\"id\":147,\"score\":0.0},{\"id\":148,\"score\":1.0},{\"id\":149,\"score\":2.0},{\"id\":150,\"score\":0.0},{\"id\":151,\"score\":1.0},{\"id\":152,\"score\":2.0},{\"id\":153,\"score\":0.0},{\"id\":154,\"score\":1.0},{\"id\":155,\"score\":2.0},{\"id\":156,\"score\":0.0},{\"id\":157,\"score\":1.0},{\"id\":158,\"score\":2.0},{\"id\":159,\"score\":0.0},{\"id\":160,\"score\":1.0},{\"id\":161,\"score\":2.0},{\"id\":162,\"score\":0.0},{\"id\":163,\"score\":1.0},{\"id\":164,\"score\":2.0},{\"id\":165,\"score\":0.0},{\"id\":166,\"score\":1.0},{\"id\":167,\"score\":2.0},{\"id\":168,\"score\":0.0},{\"id\":169,\"score\":1.0},{\"id\":170,\"score\":2.0},{\"id\":171,\"score\":0.0},{\"id\":172,\"score\":1.0},{\"id\":173,\"score\":2.0},{\"id\":174,\"score\":0.0},{\"id\":175,\"score\":1.0},{\"id\":176,\"score\":2.0},{\"id\":177,\"score\":0.0},{\"id\":178,\"score\":1.0},{\"id\":179,\"score\":2.0},{\"id\":180,\"score\":0.0},{\"id\":181,\"score\":1.0},{\"id\":182,\"score\":2.0},{\"id\":183,\"score\":0.0},{\"id\":184,\"score\":1.0},{\"id\":185,\"score\":2.0},{\"id\":186,\"score\":0.0},{\"id\":187,\"score\":1.0},{\"id\":188,\"score\":2.0},{\"id\":189,\"score\":0.0},{\"id\":190,\"score\":1.0},{\"id\":191,\"score\":2.0},{\"id\":192,\"score\":0.0},{\"id\":193,\"score\":1.0},{\"id\":194,\"score\":2.0},{\"id\":195,\"score\":0.0},{\"id\":196,\"score\":1.0},{\"id\":197,\"score\":2.0},{\"id\":198,\"score\":0.0},{\"id\":199,\"score\":1.0},{\"id\":200,\"score\":2.0},{\"id\":201,\"score\":0.0},{\"id\":202,\"score\":1.0},{\"id\":203,\"score\":2.0},{\"id\":204,\"score\":0.0},{\"id\":205,\"score\":1.0},{\"id\":206,\"score\":2.0},{\"id\":207,\"score\":0.0},{\"id\":208,\"score\":1.0},{\"id\":209,\"score\":2.0},{\"id\":210,\"score\":0.0},{\"id\":211,\"score\":1.0},{\"id\":212,\"score\":2.0},{\"id\":213,\"score\":0.0},{\"id\":214,\"score\":1.0},{\"id\":215,\"score\":2.0},{\"id\":216,\"score\":0.0},{\"id\":217,\"score\":1.0},{\"id\":218,\"score\":2.0},{\"id\":219,\"score\":0.0},{\"id\":220,\"score\":1.0},{\"id\":221,\"score\":2.0},{\"id\":222,\"score\":0.0},{\"id\":223,\"score\":1.0},{\"id\":224,\"score\":2.0},{\"id\":225,\"score\":0.0},{\"id\":226,\"score\":1.0},{\"id\":227,\"score\":2.0},{\"id\":228,\"score\":0.0},{\"id\":229,\"score\":1.0},{\"id\":230,\"score\":2.0},{\"id\":231,\"score\":0.0},{\"id\":232,\"score\":1.0},{\"id\":233,\"score\":2.0},{\"id\":234,\"score\":0.0},{\"id\":235,\"score\":1.0},{\"id\":236,\"score\":2.0},{\"id\":237,\"score\":0.0},{\"id\":238,\"score\":1.0},{\"id\":239,\"score\":2.0},{\"id\":240,\"score\":0.0},{\"id\":241,\"score\":1.0},{\"id\":242,\"score\":2.0},{\"id\":243,\"score\":0.0},{\"id\":244,\"score\":1.0},{\"id\":245,\"score\":2.0},{\"id\":246,\"score\":0.0},{\"id\":247,\"score\":1.0},{\"id\":248,\"score\":2.0},{\"id\":249,\"score\":0.0},{\"id\":250,\"score\":1.0},{\"id\":251,\"score\":2.0},{\"id\":252,\"score\":0.0},{\"id\":253,\"score\":1.0},{\"id\":254,\"score\":2.0},{\"id\":255,\"score\":0.0},{\"id\":256,\"score\":1.0},{\"id\":257,\"score\":2.0},{\"id\":258,\"score\":0.0},{\"id\":259,\"score\":1.0},{\"id\":260,\"score\":2.0},{\"id\":261,\"score\":0.0},{\"id\":262,\"score\":1.0},{\"id\":263,\"score\":2.0},{\"id\":264,\"score\":0.0},{\"id\":265,\"score\":1.0},{\"id\":266,\"score\":2.0},{\"id\":267,\"score\":0.0},{\"id\":268,\"score\":1.0},{\"id\":269,\"score\":2.0},{\"id\":270,\"score\":0.0},{\"id\":271,\"score\":1.0},{\"id\":272,\"score\":2.0},{\"id\":273,\"score\":0.0},{\"id\":274,\"score\":1.0},{\"id\":275,\"score\":2.0},{\"id\":276,\"score\":0.0},{\"id\":277,\"score\":1.0},{\"id\":278,\"score\":2.0},{\"id\":279,\"score\":0.0},{\"id\":280,\"score\":1.0},{\"id\":281,\"score\":2.0},{\"id\":282,\"score\":0.0},{\"id\":283,\"score\":1.0},{\"id\":284,\"score\":2.0},{\"id\":285,\"score\":0.0},{\"id\":286,\"score\":1.0},{\"id\":287,\"score\":2.0},{\"id\":288,\"score\":0.0},{\"id\":289,\"score\":1.0},{\"id\":290,\"score\":2.0},{\"id\":291,\"score\":0.0},{\"id\":292,\"score\":1.0},{\"id\":293,\"score\":2.0},{\"id\":294,\"score\":0.0},{\"id\":295,\"score\":1.0},{\"id\":296,\"score\":2.0},{\"id\":297,\"score\":0.0},{\"id\":298,\"score\":1.0},{\"id\":299,\"score\":2.0},{\"id\":300,\"score\":0.0},{\"id\":301,\"score\":1.0},{\"id\":302,\"score\":2.0},{\"id\":303,\"score\":0.0},{\"id\":304,\"score\":1.0},{\"id\":305,\"score\":2.0},{\"id\":306,\"score\":0.0},{\"id\":307,\"score\":1.0},{\"id\":308,\"score\":2.0},{\"id\":309,\"score\":0.0},{\"id\":310,\"score\":1.0},{\"id\":311,\"score\":2.0},{\"id\":312,\"score\":0.0},{\"id\":313,\"score\":1.0},{\"id\":314,\"score\":2.0},{\"id\":315,\"score\":0.0},{\"id\":316,\"score\":1.0},{\"id\":317,\"score\":2.0},{\"id\":318,\"score\":0.0},{\"id\":319,\"score\":1.0},{\"id\":320,\"score\":2.0},{\"id\":321,\"score\":0.0},{\"id\":322,\"score\":1.0},{\"id\":323,\"score\":2.0},{\"id\":324,\"score\":0.0},{\"id\":325,\"score\":1.0},{\"id\":326,\"score\":2.0},{\"id\":327,\"score\":0.0},{\"id\":328,\"score\":1.0},{\"id\":329,\"score\":2.0},{\"id\":330,\"score\":0.0},{\"id\":331,\"score\":1.0},{\"id\":332,\"score\":2.0},{\"id\":333,\"score\":0.0},{\"id\":334,\"score\":1.0},{\"id\":335,\"score\":2.0},{\"id\":336,\"score\":0.0},{\"id\":337,\"score\":1.0},{\"id\":338,\"score\":2.0},{\"id\":339,\"score\":0.0},{\"id\":340,\"score\":1.0},{\"id\":341,\"score\":2.0},{\"id\":342,\"score\":0.0},{\"id\":343,\"score\":1.0},{\"id\":344,\"score\":2.0},{\"id\":345,\"score\":0.0},{\"id\":346,\"score\":1.0},{\"id\":347,\"score\":2.0},{\"id\":348,\"score\":0.0},{\"id\":349,\"score\":1.0},{\"id\":350,\"score\":2.0},{\"id\":351,\"score\":0.0},{\"id\":352,\"score\":1.0},{\"id\":353,\"score\":2.0},{\"id\":354,\"score\":0.0},{\"id\":355,\"score\":1.0},{\"id\":356,\"score\":2.0},{\"id\":357,\"score\":0.0},{\"id\":358,\"score\":1.0},{\"id\":359,\"score\":2.0},{\"id\":360,\"score\":0.0},{\"id\":361,\"score\":1.0},{\"id\":362,\"score\":2.0},{\"id\":363,\"score\":0.0},{\"id\":364,\"score\":1.0},{\"id\":365,\"score\":2.0},{\"id\":366,\"score\":0.0},{\"id\":367,\"score\":1.0},{\"id\":368,\"score\":2.0},{\"id\":369,\"score\":0.0},{\"id\":370,\"score\":1.0},{\"id\":371,\"score\":2.0},{\"id\":372,\"score\":0.0},{\"id\":373,\"score\":1.0},{\"id\":374,\"score\":2.0},{\"id\":375,\"score\":0.0},{\"id\":376,\"score\":1.0},{\"id\":377,\"score\":2.0},{\"id\":378,\"score\":0.0},{\"id\":379,\"score\":1.0},{\"id\":380,\"score\":2.0},{\"id\":381,\"score\":0.0},{\"id\":382,\"score\":1.0},{\"id\":383,\"score\":2.0},{\"id\":384,\"score\":0.0},{\"id\":385,\"score\":1.0},{\"id\":386,\"score\":2.0},{\"id\":387,\"score\":0.0},{\"id\":388,\"score\":1.0},{\"id\":389,\"score\":2.0},{\"id\":390,\"score\":0.0},{\"id\":391,\"score\":1.0},{\"id\":392,\"score\":2.0},{\"id\":393,\"score\":0.0},{\"id\":394,\"score\":1.0},{\"id\":395,\"score\":2.0},{\"id\":396,\"score\":0.0},{\"id\":397,\"score\":1.0},{\"id\":398,\"score\":2.0},{\"id\":399,\"score\":0.0},{\"id\":400,\"score\":1.0},{\"id\":401,\"score\":2.0},{\"id\":402,\"score\":0.0},{\"id\":403,\"score\":1.0},{\"id\":404,\"score\":2.0},{\"id\":405,\"score\":0.0},{\"id\":406,\"score\":1.0},{\"id\":407,\"score\":2.0},{\"id\":408,\"score\":0.0},{\"id\":409,\"score\":1.0},{\"id\":410,\"score\":2.0},{\"id\":411,\"score\":0.0},{\"id\":412,\"score\":1.0},{\"id\":413,\"score\":2.0},{\"id\":414,\"score\":0.0},{\"id\":415,\"score\":1.0},{\"id\":416,\"score\":2.0},{\"id\":417,\"score\":0.0},{\"id\":418,\"score\":1.0},{\"id\":419,\"score\":2.0},{\"id\":420,\"score\":0.0},{\"id\":421,\"score\":1.0},{\"id\":422,\"score\":2.0},{\"id\":423,\"score\":0.0},{\"id\":424,\"score\":1.0},{\"id\":425,\"score\":2.0},{\"id\":426,\"score\":0.0},{\"id\":427,\"score\":1.0},{\"id\":428,\"score\":2.0},{\"id\":429,\"score\":0.0},{\"id\":430,\"score\":1.0},{\"id\":431,\"score\":2.0},{\"id\":432,\"score\":0.0},{\"id\":433,\"score\":1.0},{\"id\":434,\"score\":2.0},{\"id\":435,\"score\":0.0},{\"id\":436,\"score\":1.0},{\"id\":437,\"score\":2.0},{\"id\":438,\"score\":0.0},{\"id\":439,\"score\":1.0},{\"id\":440,\"score\":2.0},{\"id\":441,\"score\":0.0},{\"id\":442,\"score\":1.0},{\"id\":443,\"score\":2.0},{\"id\":444,\"score\":0.0},{\"id\":445,\"score\":1.0},{\"id\":446,\"score\":2.0},{\"id\":447,\"score\":0.0},{\"id\":448,\"score\":1.0},{\"id\":449,\"score\":2.0},{\"id\":450,\"score\":0.0},{\"id\":451,\"score\":1.0},{\"id\":452,\"score\":2.0},{\"id\":453,\"score\":0.0},{\"id\":454,\"score\":1.0},{\"id\":455,\"score\":2.0},{\"id\":456,\"score\":0.0},{\"id\":457,\"score\":1.0},{\"id\":458,\"score\":2.0},{\"id\":459,\"score\":0.0},{\"id\":460,\"score\":1.0},{\"id\":461,\"score\":2.0},{\"id\":462,\"score\":0.0},{\"id\":463,\"score\":1.0},{\"id\":464,\"score\":2.0},{\"id\":465,\"score\":0.0},{\"id\":466,\"score\":1.0},{\"id\":467,\"score\":2.0},{\"id\":468,\"score\":0.0},{\"id\":469,\"score\":1.0},{\"id\":470,\"score\":2.0},{\"id\":471,\"score\":0.0},{\"id\":472,\"score\":1.0},{\"id\":473,\"score\":2.0},{\"id\":474,\"score\":0.0},{\"id\":475,\"score\":1.0},{\"id\":476,\"score\":2.0},{\"id\":477,\"score\":0.0},{\"id\":478,\"score\":1.0},{\"id\":479,\"score\":2.0},{\"id\":480,\"score\":0.0},{\"id\":481,\"score\":1.0},{\"id\":482,\"score\":2.0},{\"id\":483,\"score\":0.0},{\"id\":484,\"score\":1.0},{\"id\":485,\"score\":2.0},{\"id\":486,\"score\":0.0},{\"id\":487,\"score\":1.0},{\"id\":488,\"score\":2.0},{\"id\":489,\"score\":0.0},{\"id\":490,\"score\":1.0},{\"id\":491,\"score\":2.0},{\"id\":492,\"score\":0.0},{\"id\":493,\"score\":1.0},{\"id\":494,\"score\":2.0},{\"id\":495,\"score\":0.0},{\"id\":496,\"score\":1.0},{\"id\":497,\"score\":2.0},{\"id\":498,\"score\":0.0},{\"id\":499,\"score\":1.0},{\"id\":500,\"score\":2.0},{\"id\":501,\"score\":0.0},{\"id\":502,\"score\":1.0},{\"id\":503,\"score\":2.0},{\"id\":504,\"score\":0.0},{\"id\":505,\"score\":1.0},{\"id\":506,\"score\":2.0},{\"id\":507,\"score\":0.0},{\"id\":508,\"score\":1.0},{\"id\":509,\"score\":2.0},{\"id\":510,\"score\":0.0},{\"id\":511,\"score\":1.0},{\"id\":512,\"score\":2.0},{\"id\":513,\"score\":0.0},{\"id\":514,\"score\":1.0},{\"id\":515,\"score\":2.0},{\"id\":516,\"score\":0.0},{\"id\":517,\"score\":1.0},{\"id\":518,\"score\":2.0},{\"id\":519,\"score\":0.0},{\"id\":520,\"score\":1.0},{\"id\":521,\"score\":2.0},{\"id\":522,\"score\":0.0},{\"id\":523,\"score\":1.0},{\"id\":524,\"score\":2.0},{\"id\":525,\"score\":0.0},{\"id\":526,\"score\":1.0},{\"id\":527,\"score\":2.0},{\"id\":528,\"score\":0.0},{\"id\":529,\"score\":1.0},{\"id\":530,\"score\":2.0},{\"id\":531,\"score\":0.0},{\"id\":532,\"score\":1.0},{\"id\":533,\"score\":2.0},{\"id\":534,\"score\":0.0},{\"id\":535,\"score\":1.0},{\"id\":536,\"score\":2.0},{\"id\":537,\"score\":0.0},{\"id\":538,\"score\":1.0},{\"id\":539,\"score\":2.0},{\"id\":540,\"score\":0.0},{\"id\":541,\"score\":1.0},{\"id\":542,\"score\":2.0},{\"id\":543,\"score\":0.0},{\"id\":544,\"score\":1.0},{\"id\":545,\"score\":2.0},{\"id\":546,\"score\":0.0},{\"id\":547,\"score\":1.0},{\"id\":548,\"score\":2.0},{\"id\":549,\"score\":0.0},{\"id\":550,\"score\":1.0},{\"id\":551,\"score\":2.0},{\"id\":552,\"score\":0.0},{\"id\":553,\"score\":1.0},{\"id\":554,\"score\":2.0},{\"id\":555,\"score\":0.0},{\"id\":556,\"score\":1.0},{\"id\":557,\"score\":2.0},{\"id\":558,\"score\":0.0},{\"id\":559,\"score\":1.0},{\"id\":560,\"score\":2.0},{\"id\":561,\"score\":0.0},{\"id\":562,\"score\":1.0},{\"id\":563,\"score\":2.0},{\"id\":564,\"score\":0.0},{\"id\":565,\"score\":1.0},{\"id\":566,\"score\":2.0},{\"id\":567,\"score\":0.0},{\"id\":568,\"score\":1.0},{\"id\":569,\"score\":2.0},{\"id\":570,\"score\":0.0},{\"id\":571,\"score\":1.0},{\"id\":572,\"score\":2.0},{\"id\":573,\"score\":0.0},{\"id\":574,\"score\":1.0},{\"id\":575,\"score\":2.0},{\"id\":576,\"score\":0.0},{\"id\":577,\"score\":1.0},{\"id\":578,\"score\":2.0},{\"id\":579,\"score\":0.0},{\"id\":580,\"score\":1.0},{\"id\":581,\"score\":2.0},{\"id\":582,\"score\":0.0},{\"id\":583,\"score\":1.0},{\"id\":584,\"score\":2.0},{\"id\":585,\"score\":0.0},{\"id\":586,\"score\":1.0},{\"id\":587,\"score\":2.0},{\"id\":588,\"score\":0.0},{\"id\":589,\"score\":1.0},{\"id\":590,\"score\":2.0},{\"id\":591,\"score\":0.0},{\"id\":592,\"score\":1.0},{\"id\":593,\"score\":2.0},{\"id\":594,\"score\":0.0},{\"id\":595,\"score\":1.0},{\"id\":596,\"score\":2.0},{\"id\":597,\"score\":0.0},{\"id\":598,\"score\":1.0},{\"id\"",
		"/dev/fd/63",
		"/Users/fhuszar/peerindex/stream/datamodel/2013\",\"in_reply_to_user_id\":null,\"id_str\":\"289764586591232002\",\"place\":null,\"user\":{\"location\":\"\",\"default_profile\":false,\"statuses_count\":6670,\"profile_background_tile\":true,\"lang\":\"en\",\"profile_link_color\":\"D62B80\",\"profile_banner_url\":\"https:/si0.twimg.com/profile_banners/261537894/1348088989\",\"id\":261537894,\"following\":null,\"favourites_count\":1257,\"protected\":false,\"profile_text_color\"",
		"/Users/fhuszar/peerindex/stream/datamodel/1",
		"/Users/fhuszar/peerindex/stream/datamodel/@frank_ocean",
		"/Users/fhuszar/peerindex/stream/datamodel/fan",
		"/Users/fhuszar/peerindex/stream/datamodel/he",
		"/Users/fhuszar/peerindex/stream/datamodel/is",
		"/Users/fhuszar/peerindex/stream/datamodel/my",
		"/Users/fhuszar/peerindex/stream/datamodel/life",
		"/Users/fhuszar/peerindex/stream/datamodel/{\"contributors\"",
		"/Users/fhuszar/peerindex/emrstreaming/ferenc.hive",
		"/Applications/eclipse/Eclipse.app/Contents/Info.plist",
		"/Users/fhuszar/peerindex/datascience/gender/name_age.py",
		"/Users/fhuszar/peerindex/datascience/ratemymates/tmp",
		"/Users/fhuszar/hacked.json"
	],
	"find":
	{
		"height": 35.0
	},
	"find_in_files":
	{
		"height": 0.0,
		"where_history":
		[
			"*",
			"03_qu*",
			"01_approximate*",
			"*",
			"*tikz",
			"*",
			"01_intro*,02_GP*",
			"03_quantum.tex",
			"*",
			"01_approx*.*",
			"*.*",
			"<current file>",
			"*.*",
			"countries.tsv",
			""
		]
	},
	"find_state":
	{
		"case_sensitive": true,
		"find_history":
		[
			"x",
			"param",
			"\\mathcal{D}",
			"doteq",
			"d_L",
			"h_p",
			"h_q",
			"kullback",
			"rasmussen2005",
			"asmussen",
			"rasmussen38gaussian",
			"asmussen",
			"rasmussen06GP",
			"asmussen",
			"herding_herding_",
			"sec:",
			"sec:experiments",
			"subsubsection",
			"\\subsection{",
			"Bayesian herding",
			"% defining custom colors\n\\definecolor{mycolor1}{rgb}{1,0,1}",
			"width= 1.39in,\nheight= 1.2in,",
			"Eqn:",
			"\\Gamma",
			"\\gamma",
			"\\Gamma",
			"\\gamma",
			"\\Gamma",
			"\\gamma",
			"tomography",
			"tomograp",
			"figure",
			"because it is",
			"fig:toy",
			"black",
			"blue",
			"The kernel scoring",
			"kernel score",
			"figure",
			"},\\ref{",
			"label",
			"\\label{fig:",
			"\\label:{fig:",
			"fig:",
			"fig:Bloch disk",
			"fig:",
			"fig",
			"single qubit",
			"usetikzlibrary",
			"%!TEX root = ../main.tex",
			"%!TEX root = main.tex",
			"\n%!TEX root = main.tex\n%!TEX root = main.tex\n%!TEX root = main.tex",
			"\n%!TEX root = main.tex\n%!TEX root = main.tex\nTEX root = ../main.tex",
			"\n%!TEX root = main.tex\n%!TEX root = main.tex\n%!TEX root = ../main.tex",
			"}\n",
			"title",
			"Figure",
			"}f(\\x_",
			"f(\\x_i)",
			"fig:all",
			"Fig. ",
			"e.g.",
			"% defining custom colors\n\\definecolor{mycolor1}{rgb}{0.8,0.8,0}\n\\definecolor{mycolor2}{rgb}{0,1,1}\n\\definecolor{mycolor3}{rgb}{1,0,1}\n\\definecolor{mycolor4}{rgb}{1,0.8,0.5}\n\\definecolor{mycolor5}{rgb}{0.7,0.4,0.01}",
			"in addition to",
			"mycolor",
			"\\mathcal{P}",
			"\\ourmethod",
			"Results",
			"RESULTS",
			"x",
			"f_{\\x}",
			"]",
			"[",
			"f",
			"ln",
			"Eqn. ",
			"{Eqn:",
			"\\label{Eqn:rearrangement} ",
			"Eqn:rearrangement",
			"eqns",
			"eqn",
			"s the experiment",
			"adaptively",
			"adaptivelys",
			"adaptivly",
			"theta",
			"matching",
			"herding",
			"subsubsection",
			"figure",
			"out-of",
			"figure",
			", ",
			"figure",
			"empirical distri",
			"empirical distrin",
			"within-mo",
			"figure",
			"acknowledge",
			"Acknowledgements",
			"first 8 samples",
			"figure",
			"\\E_",
			"\\E",
			"symmetric",
			"symmetry",
			"\\E_",
			"\\expect{y \\sim \\mu_{Y}}",
			"user_hashtag",
			"num_words",
			"n",
			"KEY",
			"\n ",
			"equation",
			"score matching",
			"characteristics",
			"\\theta",
			"theta",
			"param",
			"\\theta",
			"\\Theta",
			"\\theta",
			"\\Theta",
			"\\theta",
			"\\Theta",
			"\\theta",
			"\\Theta",
			"\\theta"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
			"Rasmussen2006",
			"",
			"width= 1.8in,\nheight= 1.5in,",
			"eqn:",
			"D",
			"\\outcome",
			"%!TEX root = ../thesis.tex",
			"%!TEX root = thesis.tex",
			"fig:BALD_GPC_results",
			"",
			"Eqn.\\ ",
			"{Eqn:quantum_",
			"Eqns",
			"Eqn",
			"\\param",
			"\\citep{",
			"Sriperumbudur2008",
			""
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 16,
			"sheets":
			[
				{
					"buffer": 0,
					"file": "latex/part1/01_scoring_rules.tex",
					"settings":
					{
						"buffer_size": 61296,
						"regions":
						{
						},
						"selection":
						[
							[
								34123,
								34146
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 8257.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "latex/part1/01_scoring_rules.tex.bak",
					"settings":
					{
						"buffer_size": 60573,
						"regions":
						{
						},
						"selection":
						[
							[
								26,
								26
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 2,
					"settings":
					{
						"buffer_size": 45241,
						"regions":
						{
							"match":
							{
								"flags": 112,
								"regions":
								[
									[
										472,
										493
									],
									[
										714,
										735
									],
									[
										814,
										835
									],
									[
										952,
										973
									],
									[
										1099,
										1120
									],
									[
										1234,
										1255
									],
									[
										1362,
										1383
									],
									[
										1667,
										1681
									],
									[
										2402,
										2413
									],
									[
										2947,
										2958
									],
									[
										3485,
										3496
									],
									[
										4030,
										4041
									],
									[
										4536,
										4547
									],
									[
										5060,
										5071
									],
									[
										5624,
										5635
									],
									[
										6201,
										6212
									],
									[
										6650,
										6658
									],
									[
										6866,
										6874
									],
									[
										7083,
										7091
									],
									[
										7248,
										7256
									],
									[
										7333,
										7341
									],
									[
										7529,
										7537
									],
									[
										7562,
										7570
									],
									[
										7805,
										7813
									],
									[
										8033,
										8041
									],
									[
										8239,
										8247
									],
									[
										8278,
										8286
									],
									[
										8470,
										8478
									],
									[
										8503,
										8511
									],
									[
										8727,
										8735
									],
									[
										9798,
										9806
									],
									[
										11039,
										11047
									],
									[
										11681,
										11689
									],
									[
										12522,
										12530
									],
									[
										14360,
										14368
									],
									[
										14710,
										14718
									],
									[
										15796,
										15804
									],
									[
										17178,
										17186
									],
									[
										19142,
										19150
									],
									[
										19587,
										19595
									],
									[
										19803,
										19811
									],
									[
										20020,
										20028
									],
									[
										20185,
										20193
									],
									[
										20270,
										20278
									],
									[
										20466,
										20474
									],
									[
										20499,
										20507
									],
									[
										20742,
										20750
									],
									[
										20970,
										20978
									],
									[
										21176,
										21184
									],
									[
										21215,
										21223
									],
									[
										21407,
										21415
									],
									[
										21440,
										21448
									],
									[
										21664,
										21672
									],
									[
										22735,
										22743
									],
									[
										23976,
										23984
									],
									[
										24618,
										24626
									],
									[
										25459,
										25467
									],
									[
										27297,
										27305
									],
									[
										27647,
										27655
									],
									[
										28733,
										28741
									],
									[
										30115,
										30123
									],
									[
										32079,
										32087
									],
									[
										32524,
										32532
									],
									[
										32740,
										32748
									],
									[
										32957,
										32965
									],
									[
										33122,
										33130
									],
									[
										33207,
										33215
									],
									[
										33403,
										33411
									],
									[
										33436,
										33444
									],
									[
										33679,
										33687
									],
									[
										33907,
										33915
									],
									[
										34113,
										34121
									],
									[
										34146,
										34154
									],
									[
										34338,
										34346
									],
									[
										34371,
										34379
									],
									[
										34595,
										34603
									],
									[
										35666,
										35674
									],
									[
										36907,
										36915
									],
									[
										37543,
										37551
									],
									[
										38384,
										38392
									],
									[
										40222,
										40230
									],
									[
										40572,
										40580
									],
									[
										41658,
										41666
									],
									[
										43040,
										43048
									],
									[
										45004,
										45012
									]
								],
								"scope": ""
							}
						},
						"selection":
						[
							[
								40234,
								40221
							]
						],
						"settings":
						{
							"detect_indentation": false,
							"output_tag": 10,
							"result_base_dir": "",
							"result_file_regex": "^([A-Za-z\\\\/<].*):$",
							"result_line_regex": "^ +([0-9]+):",
							"scroll_past_end": true,
							"syntax": "Packages/Default/Find Results.hidden-tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 10027.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "latex/bibliography/thesis.bib",
					"settings":
					{
						"buffer_size": 90805,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/Bibtex.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 4,
					"file": "latex/conclusions.tex",
					"settings":
					{
						"buffer_size": 4650,
						"regions":
						{
						},
						"selection":
						[
							[
								23,
								23
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 5,
					"file": "latex/introduction.tex",
					"settings":
					{
						"buffer_size": 36,
						"regions":
						{
						},
						"selection":
						[
							[
								23,
								23
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 6,
					"file": "latex/notation.tex",
					"settings":
					{
						"buffer_size": 4122,
						"regions":
						{
						},
						"selection":
						[
							[
								1552,
								1564
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 400.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 7,
					"file": "latex/part1_main.tex",
					"settings":
					{
						"buffer_size": 366,
						"regions":
						{
						},
						"selection":
						[
							[
								366,
								366
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 8,
					"file": "latex/part2_main.tex",
					"settings":
					{
						"buffer_size": 279,
						"regions":
						{
						},
						"selection":
						[
							[
								77,
								77
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 9,
					"file": "latex/part3_main.tex",
					"settings":
					{
						"buffer_size": 357,
						"regions":
						{
						},
						"selection":
						[
							[
								23,
								23
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 10,
					"file": "latex/preamble.tex",
					"settings":
					{
						"buffer_size": 1869,
						"regions":
						{
						},
						"selection":
						[
							[
								412,
								412
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 11,
					"file": "latex/frontmatter/abstract.tex",
					"settings":
					{
						"buffer_size": 159,
						"regions":
						{
						},
						"selection":
						[
							[
								26,
								26
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 12,
					"file": "latex/frontmatter/acknowledgement.tex",
					"settings":
					{
						"buffer_size": 90,
						"regions":
						{
						},
						"selection":
						[
							[
								26,
								26
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 13,
					"file": "latex/frontmatter/dedication.tex",
					"settings":
					{
						"buffer_size": 165,
						"regions":
						{
						},
						"selection":
						[
							[
								26,
								26
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 14,
					"file": "latex/part1/02_embeddings.tex",
					"settings":
					{
						"buffer_size": 38366,
						"regions":
						{
						},
						"selection":
						[
							[
								4120,
								4120
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 15,
					"file": "latex/part2/01_approximate_inference.tex",
					"settings":
					{
						"buffer_size": 23611,
						"regions":
						{
						},
						"selection":
						[
							[
								19759,
								19759
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 4270.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 16,
					"file": "latex/part2/02_herding.tex",
					"settings":
					{
						"buffer_size": 41251,
						"regions":
						{
						},
						"selection":
						[
							[
								7732,
								7732
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 1307.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 17,
					"file": "raw_materials/lossBayes/lossBayes.tex",
					"settings":
					{
						"buffer_size": 64505,
						"regions":
						{
						},
						"selection":
						[
							[
								559,
								559
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 18,
					"file": "latex/part3/01_introBALD.tex",
					"settings":
					{
						"buffer_size": 21682,
						"regions":
						{
						},
						"selection":
						[
							[
								250,
								250
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 19,
					"file": "latex/part1/02_embeddings.tex.bak",
					"settings":
					{
						"buffer_size": 38116,
						"regions":
						{
						},
						"selection":
						[
							[
								26,
								26
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 20,
					"file": "latex/part1/03_scoring_processes.tex",
					"settings":
					{
						"buffer_size": 21176,
						"regions":
						{
						},
						"selection":
						[
							[
								970,
								970
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 21,
					"settings":
					{
						"buffer_size": 0,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 22,
					"file": "latex/part3/02_GP_BALD.tex",
					"settings":
					{
						"buffer_size": 47083,
						"regions":
						{
						},
						"selection":
						[
							[
								5020,
								5020
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 6011.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 23,
					"file": "latex/part3/03_quantum.tex",
					"settings":
					{
						"buffer_size": 45302,
						"regions":
						{
						},
						"selection":
						[
							[
								39924,
								39924
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 8065.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 24,
					"file": "raw_materials/BALD-GPC/AL_NIPS2011.tex",
					"settings":
					{
						"buffer_size": 35262,
						"regions":
						{
						},
						"selection":
						[
							[
								639,
								639
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 25,
					"settings":
					{
						"buffer_size": 0,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"default_dir": "/Users/fhuszar/Dropbox/thesis/latex",
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 26,
					"file": "latex/thesis.tex",
					"settings":
					{
						"buffer_size": 1433,
						"regions":
						{
						},
						"selection":
						[
							[
								738,
								738
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 59.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 27,
					"file": "latex/figs/BALD/quantum/case1.tikz",
					"settings":
					{
						"buffer_size": 54634,
						"regions":
						{
						},
						"selection":
						[
							[
								344,
								344
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/TeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 28,
					"file": "latex/figs/BALD/quantum/case2.tikz",
					"settings":
					{
						"buffer_size": 54765,
						"regions":
						{
						},
						"selection":
						[
							[
								514,
								514
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/TeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 29,
					"file": "latex/figs/BALD/quantum/case3.tikz",
					"settings":
					{
						"buffer_size": 54921,
						"regions":
						{
						},
						"selection":
						[
							[
								412,
								412
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/TeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 30,
					"file": "latex/figs/BALD/quantum/case4.tikz",
					"settings":
					{
						"buffer_size": 54431,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/TeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 31,
					"file": "latex/figs/BALD/quantum/case5.tikz",
					"settings":
					{
						"buffer_size": 53115,
						"regions":
						{
						},
						"selection":
						[
							[
								412,
								412
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/TeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 32,
					"file": "latex/figs/BALD/quantum/case5b.tikz",
					"settings":
					{
						"buffer_size": 2283,
						"regions":
						{
						},
						"selection":
						[
							[
								412,
								412
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/TeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 33,
					"file": "latex/figs/BALD/quantum/case6.tikz",
					"settings":
					{
						"buffer_size": 53228,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/TeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 34,
					"file": "latex/figs/BALD/quantum/one_qubit_combined.tikz.tex",
					"settings":
					{
						"buffer_size": 3882,
						"regions":
						{
						},
						"selection":
						[
							[
								2600,
								2600
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 803.0,
						"zoom_level": 1.0
					},
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 34.0
	},
	"input":
	{
		"height": 31.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			]
		],
		"cols":
		[
			0.0,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": true,
	"output.exec":
	{
		"height": 100.0
	},
	"replace":
	{
		"height": 0.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"selected_items":
		[
			[
				"thesis",
				"thesis.bib"
			],
			[
				"facebookscorer",
				"src/main/java/net/peerindex/businesslogic/facebook/FacebookScorer.java"
			],
			[
				"scoringut",
				"src/main/java/net/peerindex/businesslogic/scoring/ScoringUtils.java"
			],
			[
				"scoringutil",
				"src/main/java/net/peerindex/businesslogic/scoring/ScoringUtils.java"
			],
			[
				"topicinfo",
				"src/main/java/net/peerindex/model/TopicInfo.java"
			],
			[
				"topicnames",
				"src/main/java/net/peerindex/model/TopicNames.java"
			],
			[
				"testrestco",
				"src/test/java/net/peerindex/api/TestRESTController.java"
			]
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 500.0,
		"selected_items":
		[
		],
		"width": 380.0
	},
	"show_minimap": true,
	"show_open_files": false,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 253.0,
	"status_bar_visible": true
}
