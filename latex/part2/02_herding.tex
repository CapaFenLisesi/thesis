%!TEX root = ../thesis.tex
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}

\begin{summarycontributions}
The loss-calibrated quasi-Monte Carlo framework and the interpretation of kernel herding in this framework is original contribution by FH. The equivalence of optimally weighted kernel herding and Bayesian quadrature is joint work with David Duvenaud and has been published \citep{Huszar2012herding}. FH proposed the research. FH and DD contributed equally to developing the idea and interpreting results. The theoretical analysis of approximate submodularity is original contribution by FH. The method was implemented and experiments were carried out by DD. Figures \ref{fig:fig1} -- \ref{fig:error_curve_outmodel} are taken from \citep{Huszar2012herding} as published or with slight modification, with the consent of the co-author.
\end{summarycontributions}

% ##     ## ######## ########  ########  #### ##    ##  ######   
% ##     ## ##       ##     ## ##     ##  ##  ###   ## ##    ##  
% ##     ## ##       ##     ## ##     ##  ##  ####  ## ##        
% ######### ######   ########  ##     ##  ##  ## ## ## ##   #### 
% ##     ## ##       ##   ##   ##     ##  ##  ##  #### ##    ##  
% ##     ## ##       ##    ##  ##     ##  ##  ##   ### ##    ##  
% ##     ## ######## ##     ## ########  #### ##    ##  ######   

\section{Introduction}

In this chapter I explore two quasi-Monte Carlo methods: Bayesian quadrature (\bq{}) \citep{BZHermiteQuadrature,BZMonteCarlo} (also known as Bayesian Monte Carlo) and kernel herding \citep{Chen2010}. Both methods can be used for approximate inference, and they are examples of loss-calibrated quasi-Monte Carlo introduced in the previous chapter.

Despite the focus on approximate inference, I am going to present the algorithms more generally context of numerical integration. Bayesian quadrature and kernel herding seek a solution to a central problem in statistical machine learning: to compute expectations of functions over probability distributions:
\begin{equation}
	Z_{f,p} = \int f(\x) p(\x) d\x \label{eqn:integral}
\end{equation}

In addition to approximate inference, examples of related problems include computing marginal distributions, making predictions by marginalizing over parameters, or computing the Bayes risk in a decision problem. In this chapter I will assume that the distribution $p(x)$ is known in analytic form, and $f(x)$ can be evaluated at arbitrary locations\footnote{As \citet{BZMonteCarlo} note, when $p(x)$ is complicated, it can often be replaced by a simpler, tractable distribution, with complexity adsorbed into $f$.}.

Bayesian quadrature (\bq{}) estimates integral \eqref{eqn:integral} by modelling the function $f$ probabilistically, and inferring a posterior distribution conditioned on the observed function values $y_n = f_{x_n}$. Based on the posterior over $f$, the posterior expectation of $Z_{f,p}$ can be used as an estimate to the integral. The points where the function should be evaluated can be found via Bayesian experimental design, providing a deterministic procedure for selecting sample locations. The sequence of points where the function is querried can be interpreted as pseudo-samples in quasi-Monte Carlo.

Herding, proposed recently by \cite{Chen2010}, produces pseudosamples by minimising the discrepancy of moments between the sample set and the target distribution. Similarly to traditional Monte Carlo, an estimate is formed by taking the empirical mean over samples $\hat{Z} = \frac{1}{N}\sum_{n=1}^{N}f_{x_n}$. Under certain assumptions, herding has provably fast, $\mathcal{O}(\nicefrac{1}{N})$, and has demonstrated strong empirical performance in a variety of tasks. Because herding minimises maximum mean discrepancy from the target distribution $p(x)$, it is a special case of loss-calibrated quasi-Monte Carlo.

In this chapter, I show that the Maximum Mean Discrepancy (MMD) criterion used to choose samples in kernel herding is identical to the expected error in the estimate of the integral $Z_{f,p}$ under a Gaussian process prior for $f$. This expected error is the criterion being minimized when choosing samples for Bayesian quadrature. Because Bayesian quadrature assigns different weights to each of the observed function values $f(\vx)$, we can view Bayesian quadrature as a weighted version of kernel herding. We show that these weights are optimal in a minimax sense over all functions in the Hilbert space defined by our kernel. This implies that Bayesian quadrature is equivalent to a weighted version kernel herding, and as such it also implements loss-calibrated approximate inference.

Second, we show that minimising MMD, when using \bq{} weights is closely related to the sparse dictionary selection problem studied in \citep{KrauseCevher10}, and therefore is approximately submodular with respect to the samples chosen. This hints that greedy algorithms may be used to effectively minimise the objective function. We call this greedy method Sequential Bayesian Quadrature (\sbq{}).

I then demonstrate empirically the relative performance of herding, i.i.d random sampling, and \sbq{}, and demonstrate that \sbq{} attains an empirical rate of convergence faster than $\mathcal{O}(1/N)$.

% ##     ## ######## ########  ########  #### ##    ##  ######   
% ##     ## ##       ##     ## ##     ##  ##  ###   ## ##    ##  
% ##     ## ##       ##     ## ##     ##  ##  ####  ## ##        
% ######### ######   ########  ##     ##  ##  ## ## ## ##   #### 
% ##     ## ##       ##   ##   ##     ##  ##  ##  #### ##    ##  
% ##     ## ##       ##    ##  ##     ##  ##  ##   ### ##    ##  
% ##     ## ######## ##     ## ########  #### ##    ##  ######   

\section{Herding} 

Herding was introduced by \citep{welling2009herding} as a method for generating pseudo-samples from a distribution in such a way that certain nonlinear moments of the sample set closely match those of the target distribution. The target distribution can be a Bayesian posterior, but this is not a requirement, the method was introduced with general probability distributions in mind. The empirical mean  $\frac{1}{N}\sum_{n=1}^{N}f_{x_n}$.over the pseudosamples is used to estimate expectations over the distribution.

For selecting pseudosamples, herding optimises the maximum mean discrepancy \citep[MMD;\ ][]{Sriperumbudur2008} in a myopic, greedy fashion. Recall from Chapter \ref{sec:scoring_rules} that MMD measures the divergence between two distributions, $p$ and $q$ with respect to a reproducing kernel Hilbert space (RKHS) $\He$ as follows:

\begin{align}
MMD^2(p,q) &= \sup_{\substack{f\in\He\\\Hnorm{f}=1}}\left\vert\int f_x p(x) dx - \int f_x q(x) dx\right\vert^2\\
	&= \Hnorm{\mu_{p} - \mu_{q}}^2\\
	&=  \expect{x,x'\sim p} k(x,x')	- 2 \expect{x\sim p}\expect{x'\sim q} k(x,x') + \expect{x,x'\sim q} k(x,x'),
\end{align}
%
where $\mu_{p}\in\He$ denotes the mean element associated with distribution $p$. For characteristic kernels, such as the Gaussian kernel, the mapping between a distribution and its mean element is bijective and the associated kernel scoring rule is strictly proper (Section \ref{sec:kernel_score}). As a consequence $\mmd_(p,q)=0$ if and only if $p=q$, making MMD a powerful measure of divergence.

Herding uses maximum mean discrepancy to evaluate of how well the set of pseudo-samples $\{\vx_1,\ldots,\vx_{N}\}$ represents the target distribution $p$:

\begin{align}
	\epsilon_{herding}&\left(\{\vx_1,\ldots,\vx_{N}\}\right) = \mmd\left(p,\frac{1}{N}\sum_{n=1}^{N}\delta_{x_n}\right)\\
	&=\iint \expect{x,x'\sim p} k(x,x') - 2 \frac{1}{N}\sum_{n=1}^{N} \expect{x\sim p}k(x,x_n)
		+ \frac{1}{N^2}\sum_{n,m=1}^{N} k(x_n,x_m)
\label{eq:mmd_assumption}
\end{align}

The herding procedure greedily minimizes its objective $\epsilon_{herding}\left(\{\vx_1,\ldots,\vx_{N}\}\right)$ , adding pseudosamples $\vx_n$ one at a time. When selecting the $n+1$-st pseudosample:

\begin{align}
\vx_{n+1} &\leftarrow \argmin_{\vx \in \mathcal{X}} \label{eqn:herding_criterion} \epsilon_{herding}\left(\{\vx_1,\ldots,\vx_{n},\vx\}\right)\\
	&= \argmax_{\vx \in \mathcal{X}} 2 \expect{\vx' \sim p}{k(\vx, \vx')} - \frac{1}{n+1}\sum_{m=1}^{n} k(\vx,\vx_m)\mbox{,}
\end{align}

assuming $k(\vx,\vx) = \mbox{const}$.
The formula \eqref{eqn:herding_criterion} admits an intuitive interpretation: the first term encourages sampling in areas with high mass under the target distribution $p(\vx)$. The second term discourages sampling at points close to existing samples. 

Evaluating \eqref{eqn:herding_criterion} requires us to compute $\expect{\vx' \sim p}{k(\vx, \vx')} $, that is to integrate the kernel against the target distribution. Throughout the paper we will assume that these integrals can be computed in closed form. Whilst the integration can indeed be carried out analytically in several cases \citep{Song2008,Chen2010}, this requirement is the most pertinent limitation on applications of kernel herding, Bayesian quadrature and related algorithms.

\subsection{Complexity and convergence Rates}

Criterion \eqref{eqn:herding_criterion} for selecting the $n+1$-th sample can be evaluated in only $\mathcal{O}(n)$ time. Adding these up for all subsequent samples, and assuming that optimisation in each step has $\mathcal{O}(1)$ complexity, producing $N$ pseudosamples via kernel herding costs $\mathcal{O}(N^2)$ operations in total.

In finite dimensional Hilbert spaces, the herding algorithm has been shown to reduce $\mmd$ at a rate $\mathcal{O}(\nicefrac{1}{N})$, which compares favourably with the $\mathcal{O}(\frac{1}{\sqrt{N}})$ rate obtained by non-deterministic Monte Carlo samplers \citep{Chen2010,Bach2012}. However, as pointed out by \citep{Bach2012}, this fast convergence is not guaranteed in infinite dimensional Hilbert spaces, such as the RKHS corresponding to the Gaussian kernel. These results assume that in each step the optimisation in Eqn.\ \ref{eqn:herding_criterion} is solved exactly. In addition, herding is shown to converge even if this maximisation is imperfect \citep{Gelfand2010}.

% ########   #######  
% ##     ## ##     ## 
% ##     ## ##     ## 
% ########  ##     ## 
% ##     ## ##  ## ## 
% ##     ## ##    ##  
% ########   ##### ## 

\section{Bayesian quadrature} 

\begin{figure}
\centering
\includegraphics[width=.8\columnwidth]{figs/herding/bq_intro4}
\caption[An illustration of Bayesian quadrature]{An illustration of Bayesian Quadrature. The function $f(x)$ is sampled at a set of input locations. This induces a Gaussian process posterior distribution on $f$, which is integrated in closed form against the target density, $p(\vx)$. Since the amount of volume under $f$ is uncertain, this gives rise to a (Gaussian) posterior distribution over $Z_{f,p}$.}
\label{fig:bq_intro}
\end{figure}

In herding, the integral \eqref{eqn:integral} is approximated by the empirical mean of the function evaluated at some set of pseudo-samples $\frac{1}{N}\sum_{n=1}^{N}f_{x_n}$. Equivalently, we can say that Monte Carlo and herding both assign an equal $\nicefrac{1}{N}$ weight to each of the samples.

In \citep{BZMonteCarlo}, an alternate method is proposed: Bayesian Monte Carlo, or Bayesian quadrature (\bq). \bq{} puts a prior distribution on $f$, then estimates integral \eqref{eqn:integral} by inferring a posterior distribution over the function $f$, conditioned on the observations $f(\vx_n)$ at some query points $\vx_n$. This method allows us to choose sample locations $\vx_n$ in any desired manner, and as we will see assigns non-uniform weight to the function values $f_{x_n}$. See Figure \ref{fig:bq_intro} for an illustration of Bayesian Quadrature.

Here I derive the \bq{} estimate of the intergral in Eqn.\ \eqref{eqn:integral}. Let us assume we have evaluated the function at certain test locations $\x_1 \ldots \x_N$, collectovely denoted as $X$. The function values $f(\vx_1) \ldots f(\vx_N)$ are collectively denoted as the vector $\vf(\vX)$. The Bayesian solution implies a predictive distribution over $Z$. I am now goig to calculate the mean of this distribution, $\expect{}Z_{f,p}$. This is the optimal Bayesian estimator for a squared loss.

For simplicity, $f$ is assigned a Gaussian process prior with kernel function $k$ and mean $0$ \citep{Rasmussen2006}. Note that the GP assumption is the Bayesian analogoue to the assumption that integrand functions belong to an RKHS made by kernel herding in Eqn.\ \eqref{eq:mmd_assumption}. Conveniently, the \gp{} posterior allows us to compute the expectation of $Z_{f,p}$ in closed form: 

\begin{align}
	\expect{} \left[Z_{f,p} \vert \vX, \vf(\vX) \right] &= \expect{} \left[ {\int f(\vx)p(\vx)d\vx} \middle\vert \vX, \vf(\vX) \right]\\
	 & = \int \expect{} [f(\vx) \vert \vX, \vf(\vX) ] p(\vx) d\vx\\
	 & = \int \vk(\vx, \vX) K^{-1} \vf(\vX) p(\vx) d\vx \\
	 & = \vz^T K^{-1} \vf(\vX),
	\label{eq:marg_mean_symbolic}
\end{align} 
where $\vk(\vx, \vX)$ is the vector formed by evaluating the kernel between $\vx$ and each of the sample locations $\x_n$; $K$ is the kernel Gram matrix formed by evaluating the kernel between each pair of test locations $\vx_n$ and $\vx_m$; and finally
\begin{align}
z_n & = \int k(\vx, \vx_n) p(\vx) d\vx = \expect{\vx \sim p}{k(\vx_n, \vx)}.
\end{align}

Conveniently, just like in kernel herding or Monte Carlo, the estimate of $Z_{f,p}$ is still a linear combination of observed function values $f_{\x_n}$, however now the weights are not uniform:

\begin{align}
	&\hat{Z}(\vX,\vf(\vX)) \defeq \\
	&\defeq \expect{} \left[Z_{f,p} \vert \vX, \vf(\vX) \right]\\
	&= \vz^T K^{-1} \vf(\vX) \\
	&= \sum_n w_{\bq}^{(n)} f_{\x_n},
\end{align}  
where 
\begin{align}  
	w_{\bq}^{(n)} & = \sum_m \vz_m^T [K^{-1}]_{nm}
	\label{eq:bq_weights}
\end{align}

Thus, one can view the \bq{} estimate as a weighted version of the herding estimate. The weights assigned to function values $f_{x_n}$ depend only on the location of samples $\vX$. Interestingly, the weights do not generally sum to 1, and they are not even necessarily positive.

\subsection{Weights in Bayesian quadrature}

\begin{figure}[h]
	\centering
	\includegraphics[width=.8\columnwidth]{figs/herding/weights_v1_n100}
	\caption[Empirical distribution of weights in sequential Bayesian quadrature]{A set of optimal weights given by \bq{}, after 100 \sbq{} samples were selected on the distribution shown in Figure \ref{fig:fig1}. Note that the optimal weights are spread away from the uniform weight ($\nicefrac{1}{N}$), and that some weights are even negative. The sum of these weights is 0.93.}
	\label{fig:weights100}
\end{figure}

When weighting samples, it is often assumed, or enforced \citep[as in][]{Bach2012,Song2008}, that the weights $\vw$ form a probability distribution. However, there is no technical reason for this requirement, and in fact, the optimal weights in \bq{} do not have this property. Figure \ref{fig:weights100} shows a representative set of 100 \bq{} weights chosen on samples representing the distribution in figure \ref{fig:fig1}. There are several negative weights, and the sum of all weights is 0.93.

Figure \ref{fig:weights_shrinkage} demonstrates that, in general, the sum of the Bayesian weights exhibits shrinkage when the number of samples is small. The sum or weights is less then 1 and increases gradually as more and more observations are made. This is a desirable property, due to the smoothness assumptions inherent in Gaussian process prior, which implies a $0$ prior mean over the value of the integral. Without any observations whatsoever, our best guess for the value of the integral is $0$. Now assume we evaluated the function somewhere once, and observed the value $f_{\x_1}$. Kernel herding -- or indeed any algorithm where the weights are restricted to sum to 1 -- would give the overly confident estimate $Z_{p,f} \approx f_{\x_1}$. Instead, \bq{} is more conservative and estimates the integral as $Z_{p,f} \approx w\cdot f_{\x_1}$ where $w \leq 1$.

\begin{figure}
	\centering
	\includegraphics[width=.8\columnwidth]{figs/herding/weights_shrinkage}
	
		\caption[The concept of shrinkage in Bayesian quadrature]{An example of Bayesian shrinkage in the sample weights. In this example, the kernel width is approximately $\nicefrac{1}{20}$ the width of the distribution being considered. Because the prior over functions is zero mean, in the small sample case the weights are shrunk towards zero. The weights given by simple Monte Carlo and herding do not exhibit shrinkage. }
	\label{fig:weights_shrinkage}
\end{figure}

\subsection{Sequential sampling for BQ}

Bayesian quadrature provides not only a point estimate of $\hat{Z}(\vX,\vf(\vX))$, but a full Gaussian posterior distribution $Z_{f,p}$. We can quantify the uncertainty or risk of this estimate this distribution over $\var{} \left[ Z_{f,p} \vert \vX, \vf(\vX) \right]$ quantifies our uncertainty in the estimate. When selecting locations to evaluate the function $f$, minimising the posterior variance is a sensible strategy. Below, I give a closed form formula for the posterior variance of $Z_{f,p}$, conditioned on the observations $f_{x_1} \dots f_{x_N}$, which we will denote by $\epsilon^2_{\bq{}}$. For a detaled derivation see \citep{BZMonteCarlo}.
\begin{align}
	\epsilon^{2}_{\bq{}}(\{ \vx_1,\ldots,\vx_N \}) &= \var{} \left[ Z_{f,p} \vert \vX, \vf(\vX) \right]\\
	&= \expect{f\vert X,\vf(X)} \left( \hat{Z}(\vX,\vf(\vX)) - Z_{f,p} \right)^2\\
	&= \expect{\vx, \vx' \sim p}{k(\vx, \vx')} - \vz^T K^{-1} \vz\mbox{,}
		\label{eqn:marg_var_symbolic}
\end{align}
%
where $z_n = \expect{\vx' \sim p}{k(\vx_n, \vx')}$ as before.

Perhaps surprisingly, the posterior variance of $Z_{f,p}$ does not depend on the observed function values, only on the location $x_n$ of samples, therefore

\begin{align}
	\var{} \left[ Z_{f,p} \vert \vX, \vf(\vX) \right] &= \var{} \left[ Z_{f,p} \vert \vX \right]\\
\expect{f\vert X,\vf(X)} \left( Z_{f,p} - \hat{Z}(\vX,\vf(\vX))  \right)^2 &= \expect{f\sim\gp} \left( Z_{f,p} - \hat{Z}(\vX,\vf(\vX)) \right)^2,\label{eqn:variance_independent}
\end{align}
%
where $\gp$ denotes the gaussian process prior. A similar independence on function values is observed in other optimal experimental design problems involving Gaussian processes \citep{Krause2006}. This allows the optimal sample locations to be computed ahead of time, without observing any values of $f$ at all \citep{minka2000dqr}.

We can contrast the \bq{} objective $\epsilon^{2}_{\bq{}}$ in \eqref{eqn:marg_var_symbolic} to the objective being minimized in herding, $\epsilon^{2}_{herding}$ of Eqn.\ \eqref{eq:mmd_assumption}. Just like $\epsilon^{2}_{herding}$, $\epsilon^{2}_{\bq{}}$ expresses a trade-off between accuracy and diversity of samples. On the one hand, as samples get close to high density regions under $p$, the values in $\vz$ increase, which results in decreasing variance. On the other hand, as samples get closer to each other, eigenvalues of $K$ increase, resulting in an increase in variance. 

In a similar fashion to herding, we may use a greedy method to minimise $\epsilon^{2}_{\bq{}}$, adding one sample at a time. We will call this algorithm \emph{Sequential Bayesian Quadrature} (\sbq{}):

\begin{align}
\vx_{n+1} &\leftarrow \argmin_{\vx \in \mathcal{X}} \epsilon_{\bq{}}\left(\{\vx_1,\ldots,\vx_{n},\vx\}\right) \label{eqn:sequential_BQ}
\end{align}

Using incremental updates to the Cholesky factor, the criterion can be evaluated in $\mathcal{O}(n^2)$ time. Iteratively selecting $N$ samples thus takes $\mathcal{O}(N^3)$ time, assuming optimisation can be done on $\mathcal{O}(1)$ time.

\section{Relating Bayesian quadrature to MMD}

The similarity in the behaviour of $\epsilon^{2}_{herding}$ and $\epsilon^{2}_{\bq{}}$ is not a coincidence, the two quantities are closely related to each other, and to \mmd.

\begin{proposition}
The expected variance in the Bayesian quadrature $\epsilon^{2}_{\bq{}}$  is the maximum mean discrepancy between the target distribution $p$ and $q_{\bq{}}(x) = \sum_{n=1}^{N}w^{(n)}_{\bq{}}\delta(x - x_n)$
%
\begin{proof}
First, recall from Eqn.\ \ref{eqn:variance_independent} that the variance of $Z_{f,p}$ does not depend on the observed function values $f(\x_n)$, only on the sample locations $x_n$:
%
\begin{align}
\var{} \left[ Z_{f,p} \vert \vX, \vf(\vX) \right] &= \var{} \left[ Z_{f,p} \vert \vX \right]\\
	&= \expect{f\sim\gp} \left(  Z_{f,p} - \hat{Z}(\vX,\vf(\vX)) \right)^2
\end{align}

Thus, expanding the variance and using the representer theorem we get
%
\begin{align}
	\var{} \left[ Z_{f,p} \vert \vX, \vf(\vX) \right] 
	&= \expect{f\sim\gp} \left( Z_{f,p} - \hat{Z}(\vX,\vf(\vX)) \right)^2\\
	&= \expect{f\sim GP} \left( \expect{x\sim p} f(x) - \sum_{n=1}^{N}w^{(n)}_{\bq{}} f(x_n)\right)^2\\	
	&= \expect{f\sim GP} \left( \expect{x\sim p} \scalar{f}{k(\cdot,x)} - \sum_{n=1}^{N}w^{(n)}_{\bq{}} \scalar{f}{k(\cdot,x_n)}\right)^2\\
	&= \expect{f\sim GP} \scalar{f}{\expect{x\sim p} k(\cdot,x) - \sum_{n=1}^{N}w^{(n)}_{\bq{}}k(\cdot,x_n)}^2\\
	&= \expect{f\sim GP} \scalar{f}{\mu_p - \mu_{q_{\bq{}}}}^2,
\end{align}
%
where $\mu_{q_{\bq{}}}$ is the mean embedding of the measure $q_{\bq{}} = \sum_{n=1}^{N} w^{(n)}_{\bq{}} \delta(\x - \x_n)$. Note that $q_{\bq{}}$ is not a probability measure, and not even a positive measure, as the weigths $\vw_{\bq{}}$ can be negative. Nevertheless, the mean embedding of non-positive measures is still well defined.

Using the fact that $f$ is a standard Gaussian process in $\He$, therefore
%
\begin{equation}
	\forall g\in\He: \scalar{f}{g} \sim \mathcal{N}(0,\Hnorm{g})
\end{equation}
%
we can write
%
\begin{align}
	\var{f\vert X,\vf(X)} Z_{f,p} &= \expect{f\sim GP} \scalar{f}{\mu_p - \mu_{q_{\bq{}}}}^2\\
	&= \var{f\sim GP} \scalar{f}{\mu_p - \mu_{q_{\bq{}}}} + \left(\expect{f\sim GP} \scalar{f}{\mu_p - \mu_{q_{\bq{}}}} \right)^2\\
	&= \Hnorm{\mu_p - \mu_{q_{\bq{}}}}^2\\
	&= \mmd^2(p,q_{\bq{}})
\end{align}
\end{proof}
\end{proposition}

So sequential Bayesian quadrature and kernel herding in fact minimise the same Bregman divergence, maximum mean discrepancy. The only difference between the two methods is that \sbq{} allows for non-uniform weights, whereas in kernel herding the weights are constrained. In this sense, \sbq{} is closely related to the weighted variant of kernel herding proposed by \citet{Bach2012} in independent work.

Furthermore, we can establish that the weights used by \sbq{} are optimal for any weighted kernel herding procedure. The the posterior mean $\hat{Z}(\vX,\vf(\vX)) = \expect{}[{Z_{f,p}\vert f_1,\ldots,f_N}]$ is a Bayes estimator under the squared loss, therefore it has minimal expected squared error amongst all estimators. This observation, combined with our previous findings allows us to further rewrite $\epsilon^{2}_{\bq{}}$ into the following minimax forms:

\begin{align}
\epsilon^{2}_{\bq{}} &= \sup_{\substack{f\in\He\\\Hnorm{f}{\He}=1}} \left| \int f(x) p(x) dx - \sum_{n=1}^{N}w^{(n)}_{\bq{}} f_{x_n}\right|^2\\
	&= \inf_{\hat{Z}:\mathcal{X}^N\mapsto\mathbb{R}} \sup_{\substack{f\in\He\\\Hnorm{f}=1}} \left| Z_{f,p} - \hat{Z}\left(f_{x_1},\ldots,f_{x_N}\right)\right|^2\\
	&= \inf_{\bm{w}\in\mathbb{R}^N} \sup_{\substack{f\in\He\\\Hnorm{f}=1}} \left| Z_{f,p} - \sum_{n=1}^{N}w_n  f_{x_n}\right|^2
\end{align}

In summary, sequential Bayesian quadrature minimises the same objective as kernel herding, but with the uniform $\frac{1}{N}$ weights replaced by the optimal weights. As a corollary, \sbq{} estimate converges at least as fast as any other herding estimate using the same kernel, including the weighted kernel herding by \citet{Bach2012}.

\section{Approximate submodularity}

Herding and \sbq{} are examples of greedy algorithms optimising set functions: they add each subsequent pseudosample in such a way as to minimize the instantaneous reduction in $\mmd$. This myopic, greedy strategy is potentially suboptimal compared to optimising all sample locations simultaneously. In this section, I use the concept of submodularity \citep[see \eg][]{KrauseCevher10} in an attempt to understand the error introduced by the greedy sequential optimisation strategy.

A set function $s:2^\mathcal{X} \mapsto \mathbb{R}$ is \textit{submodular} if, for all $A\subseteq B\subseteq \mathcal{X}$ and $\forall x \in \mathcal{X}$
%
\begin{align}
	s(A\cup\{x\})-s(A)\geq s(B\cup\{x\})-s(B)
\end{align}
%
Intuitively, submodularity is a diminishing returns property: adding an element to a smaller set has larger relative effect than adding it to a larger set. A key result \cite[see e.\,g.\ ][and references therein]{KrauseCevher10} is that greedily maximising a submodular function is guaranteed not to differ from the optimal strategy by more than a constant factor of $(1-\frac{1}{e})$.

Unfortunately, neither kernel herding or Bayesian quadrature are submodular optimisation problems. However, noting that \sbq{} is identical to the sparse dictionary selection problem studied in detail by \citet{KrauseCevher10}, we can conclude that \sbq{} at least satisfies a weaker condition called \emph{approximate submodularity}:

A set function $s:2^\mathcal{X} \mapsto \mathbb{R}$ is \textit{approximately submodular} with constant $\epsilon>0$, if for all $A\subseteq B\subseteq \mathcal{X}$ and $\forall x \in \mathcal{X}$
%
\begin{align}
s(A\cup\{x\})-s(A)\geq s(B\cup\{x\})-s(B) - \epsilon
\end{align}

\begin{proposition}\label{prop:submodularity_SBQ}
$\epsilon^{2}_{\bq{}}(\emptyset)-\epsilon^{2}_{\bq{}}(\cdot)$ is a weakly submodular set function with constant $\epsilon<4r$, where $r$ is the incoherency
%
\begin{equation}
	r = \max_{x,x'\in\mathcal{P}\subseteq\mathcal{X}} \frac{k(x,x')}{\sqrt{k(x,x)k(x',x')}}
\end{equation}

\begin{proof} By the definition of $\mmd$ we can see that
$\epsilon^{2}_{\bq{}} = \inf_{w\in\mathbb{R}^N}\Hnorm{\mu_p - \sum_{n=1}^N w^{(n)}k(\cdot,\vx_n)}^2$ is the squared distance between the mean element $\mu_p$ and its projection onto the subspace spanned by the elements $k(\cdot,\vx_n)$. Thus, the objective is analogous to the objective of sparse dictionary selection. Substituting $k=1$ into Theorem 1 of \citet{KrauseCevher10} concludes the proof.
\end{proof}
\end{proposition}

Unfortunately, weak submodularity does not provide the strong near-optimality guarantees as submodularity does . If $s:2^\mathcal{X} \mapsto \mathbb{R}$ is a weakly submodular function with constant $\epsilon$, and $\vert\mathcal{A}_n\vert=n$ is the result of greedy optimisation of $s$, then
\begin{equation}
	s(\mathcal{A}_n) \geq \left(1-\frac{1}{e}\right)\max_{\vert\mathcal{A}\vert\leq n}s(\mathcal{A}) - n\epsilon
\end{equation}

As pointed out by \citet{KrauseCevher10}, this guarantee is very weak, as in our case the objective function $\epsilon^{2}_{\bq{}}(\emptyset)-\epsilon^{2}_{\bq{}}(\cdot)$ is bounded above by a constant. However, establishing a connection between \sbq{} and sparse dictionary selection problem opens up interesting directions for future research, and it may be possible to apply algorithms and theory developed for sparse dictionary selection to kernel-based quasi-Monte Carlo methods.

% ######## ##     ## ########  ######## ########  #### ##     ## ######## ##    ## ########  ######  
% ##        ##   ##  ##     ## ##       ##     ##  ##  ###   ### ##       ###   ##    ##    ##    ## 
% ##         ## ##   ##     ## ##       ##     ##  ##  #### #### ##       ####  ##    ##    ##       
% ######      ###    ########  ######   ########   ##  ## ### ## ######   ## ## ##    ##     ######  
% ##         ## ##   ##        ##       ##   ##    ##  ##     ## ##       ##  ####    ##          ## 
% ##        ##   ##  ##        ##       ##    ##   ##  ##     ## ##       ##   ###    ##    ##    ## 
% ######## ##     ## ##        ######## ##     ## #### ##     ## ######## ##    ##    ##     ######  

\section{Experiments and results}\label{sec:herding_experiments}

In this section, we examine empirically the rates of convergence of sequential Bayesian quadrature and herding. We examine both the expected error rates, and the empirical error rates.

In all experiments, the target distribution $p$ is chosen a two dimensional mixture of 20 Gaussians, whose equiprobability contours are shown in Figure \ref{fig:fig1}. To ensure a comparison fair to herding, the target distribution, and the kernel used by both methods, correspond exactly to the one used in \citep[Fig.\ 1]{Chen2010}.

For experimental simplicity, each of the sequential sampling algorithms minimizes the next sample location from a pool of 10000 locations randomly drawn from the base distribution. In practice, one would run a gradint-based local optimizer from each of these candidate locations, however in our experiments we found that this did not make a significant difference in the sample locations chosen. 

\subsection{Matching a distribution}

\begin{figure}[h]
\centering
\includegraphics[width=.8\columnwidth]{figs/herding/fig1_v2}
\caption[Sequential Bayesian quadrature versus kernel herding]{The first 8 samples from sequential Bayesian quadrature, versus the first 20 samples from herding. Only 8 weighted \sbq{} samples are needed to give an estimator with the same maximum mean discrepancy as using 20 herding samples with uniform weights. Relative sizes of samples indicate their relative weights.\label{fig:fig1}}
\end{figure} 

We first extend an experiment from \citep{Chen2010} designed to illustrate the mode-seeking behavior of herding in comparison to random samples. In the first experiment of \citep{Chen2010}, it is shown that a small number of i.\,i.\,d.\ samples drawn from a multimodal distribution will tend to, by chance, assign too many samples to some modes, and too few to some other modes. In contrast, herding places pseudo-samples (called super-samples in that paper) in such a way as to avoid regions already well-represented, and seeks modes that are under-represented.

We demonstrate that although herding improves upon i.\,i.\,d.\ sampling, the uniform weighting of super-samples leads to sub-optimal performance. Figure \ref{fig:fig1} shows the first 20 samples chosen by kernel herding, in comparison with the first 8 samples chosen by \sbq{}. By weighting the 8 \sbq{} samples by the quadrature weights in \eqref{eq:bq_weights}, we can obtain the same MMD value as by using the 20 uniformly-weighted herding samples. 

\begin{figure}[t]
\centering
\includegraphics[width=.8\columnwidth]{figs/herding/expected_variance_v7_400}
\caption[Discrepancy of Bayesian quadrature, herding and random sampling]{The maximum mean discrepancy, or expected error of several different quadrature methods. Herding appears to approach a rate close to $\mathcal{O}(1/N)$. \sbq{} appears to attain a faster, but unknown rate.}
\label{fig:mmd_curve}
\end{figure}

Figure \ref{fig:mmd_curve} shows MMD versus the number of samples added, on the distribution shown in Figure \ref{fig:fig1}. We can see that in all cases, \sbq{} dominates herding. It appears that \sbq{} converges at a rate faster than $\mathcal{O}(1/N)$, possibly exponentially fast, although the theoretical rate is unknown.

There are two differences between herding and \sbq{}:  \sbq{} chooses samples according to a different criterion, and also weights those samples differently. We may ask whether the sample locations or the weights are contributing more to the faster convergence of \sbq{}. Indeed, in Figure \ref{fig:fig1} we observe that the samples selected by \sbq{} are quite similar to the samples selected by kernel herding. To answer this question, we also plot in Figure \ref{fig:mmd_curve} the performance of a fourth method, which selects samples using kernel herding, but later re-weights the herding samples with \bq{} weights. Initially, this method attains similar performance to \sbq{}, but as the number of samples increases, \sbq{} attains a better rate of convergence. This result indicates that the different sample locations chosen by \sbq{}, and not only the optimal weights, are responsible for the superior convergence rate of \sbq{}.

\subsection{Estimating Integrals}

We then examined the empirical performance of the different algorithms at estimating integrals of real-valued functions. To begin with, we looked at performance on 100 randomly drawn functions, of the form:
%
\begin{align}
f(\vx) & = \sum_{i=1}^{10} \alpha_i k(\vx, \vc_i),
\end{align}
%
where
%
\begin{align}
\Hnorm{f}^2 = \sum_{i=1}^{10} \sum_{j=1}^{10} \alpha_i \alpha_j k(\vc_i, \vc_j) = 1
\end{align}

That is, these functions belonged exactly to the unit ball of the RKHS defined by the kernel $k(\vx, \vx')$ used to model them.

\begin{figure}[t]
\centering
\includegraphics[width=.8\columnwidth]{figs/herding/error_curve_rkhs_400_v4}
\caption[Empirical error of Bayesian quadrature, herding and random sampling]{Within-model error: The empirical error rate in estimating $Z_{f,p}$, for several different sampling methods, averaged over 250 functions randomly drawn from the RKHS corresponding to the kernel used.}
\label{fig:error_curve}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=.8\columnwidth]{figs/herding/bound_curve_rkhs}
\caption[Illustrating MMD as an upper bound on empirical error rate]{The empirical error rate in estimating $Z_{f,p}$,  for the \sbq{} estimator, on 10 random functions drawn from the RKHS corresponding to the kernel used. Also shown is the upper bound on the error rate implied by the $\mmd$.}
\label{fig:bound_curve}
\end{figure}

Figure \ref{fig:error_curve} shows the empirical error versus the number of samples, on the distribution shown in Figure \ref{fig:fig1}. The empirical rates attained by the method appear to be similar to the MMD rates in Figure \ref{fig:mmd_curve}.

By definition, MMD provides a upper bound on the estimation error in the integral of any function in the unit ball of the RKHS (Eqn.\ \eqref{eqn:rkhs-mmd}), including the Bayesian estimator, \sbq{}. Figure \ref{fig:bound_curve} demonstrates this quickly decreasing bound on the \sbq{} empirical error.

\subsection{Out-of-model performance}

A central assumption underlying both kernel herding and\sbq{} is that the integrand function belongs to the RKHS specified by the kernel. To see how performance is effected if this assumption is violated, we performed empirical tests with functions chosen from outside the RKHS. We drew 100 functions of the form:

\begin{align}
f(\vx) & = \sum_{i=1}^{10} \alpha_i \exp(-\frac{1}{2} (\vx -\vc_i)^T \Sigma_i^{-1} (\vx -\vc_i)
\end{align}

where each $\alpha_i$ $\vc_i$ $\Sigma_i$ were drawn from broad distributions. This ensured that the drawn functions had features such as narrow bumps and ridges which would not be well modelled by functions belonging to the isotropic kernel defined by $k$.

\begin{figure}[h]
\centering
\includegraphics[width=.8\columnwidth]{figs/herding/error_curve_outmodel_400_v3}
\caption[Out-of-model error of Bayeisan quadrature, herding and random sampling]{Out-of-model error: The empirical error rates in estimating $Z_{f,p}$, for several different sampling methods, averaged over 250 functions drawn from outside the RKHS corresponding to the kernels used.}
\label{fig:error_curve_outmodel}
\end{figure}

Figure \ref{fig:error_curve_outmodel} shows that, on functions drawn from outside the assumed RKHS, relative performance of all methods remains similar.

% ########  ####  ######   ######  ##     ##  ######   ######  ####  #######  ##    ## 
% ##     ##  ##  ##    ## ##    ## ##     ## ##    ## ##    ##  ##  ##     ## ###   ## 
% ##     ##  ##  ##       ##       ##     ## ##       ##        ##  ##     ## ####  ## 
% ##     ##  ##   ######  ##       ##     ##  ######   ######   ##  ##     ## ## ## ## 
% ##     ##  ##        ## ##       ##     ##       ##       ##  ##  ##     ## ##  #### 
% ##     ##  ##  ##    ## ##    ## ##     ## ##    ## ##    ##  ##  ##     ## ##   ### 
% ########  ####  ######   ######   #######   ######   ######  ####  #######  ##    ## 

\section{Summary and conclusions}

In this chapter, I presented two quasi-Monte Carlo methods, kernel herding and sequential Bayesian quadrature. I showed that the discrepancy minimized by both methods when selecting pseudosample locations is the maximum mean discrepancy in a reproducing kernel Hilbert space. MMD is an example of Bregman divergences and is related to the kernel score introduced in section \ref{sec:kernel_score}. Therefore, kernel herding and Bayesian quadrature are examples of loss-calibrated approximate inference.

The results also imply that sequential Bayesian quadrature can viewed as an optimally-weighted version of kernel herding, and therefore achieves superior convergence rates. I empirically demonstrated this superior rate of convergence, and demonstrated how MMD provides an upper bound on the empirical error of the Bayesian quadrature estimate.

Despite their attractive performance and properties, these kernel-based QMC methods have a serious drawback: they rely on expectations of the form $\expect{x\sim p} k(\cdot,x)$ to be calculated. This limits their potential application to approximate inference, where these expectations are usually not available in closed form.

\citet{Chen2010} apply kernel herding to super-sample empirical distributions resulting from extensive MCMC simulations. MCMC is prone to introducing positive correlation between subsequent samples, which can be removed by applying \sbq{} or kernel herding. This procedure has limited practical applications, as the main computational bottleneck is Bayesian analysis is obtaining MCMC samples rather than using them for predictions. \citet{Osborne2012} propose an algorithm similar to \sbq{} in the context of approximate inference to calculate the marginal likelihood or model evidence. However, because the likelihood function is non-negative, it cannot be directly modelled by kernels. Hence, alternative modeling techniques are required which result in a departure from the theoretically attractive MMD interpretation.

The reconciliation of herding and \bq{} in the scoring rule framework naturally points to possible extensions. Indeed the kernel-based MMD criterion could be replaced by almost any Bregman divergence, such as the spherical kernel divergence from Eqn.\ \ref{eqn:spherical_kernel_divergence}. It is unclear if such extensions would have any advantage compared to existing methods. As discussed, the KL divergence is inappropriate for this purpose because the KL divergence between continuous and atomic distributions is not well defined.

Using herding techniques, we are able to achieve fast convergence on a Hilbert space of \emph{well-behaved} functions, but this fast convergence is at the expense of the estimate not necessarily converging for functions outside this space.
If we use a characteristic kernel \citep{Sriperumbudur2008}, such as the exponentiated-quadratic or Laplacian kernels, then convergence in MMD implies weak convergence of $q_N$ to the target distribution. 
%$p$\footnote{this statement is analogous to Levy's continuity theorem [cite], and we plan to include a more precise theorem and proof of this in the final version of the paper and supplementary material}. 
This means that the estimate converges for any bounded measurable function $f$. The speed of convergence, however, may not be as fast.

Therefore it is crucial that the kernel we choose is representative of the function or functions $f$ we will integrate. For example, in our experiments, the convergence of herding was sensitive to the width of the Gaussian kernel. One of the major weaknesses of kernel methods in general is the difficulty of setting kernel parameters. A key benefit of the Bayesian interpretation of herding and MMD presented in this paper is that it provides a recipe for adapting the Hilbert space to the observations $f(x_n)$. To be precise, we can fit the kernel parameters by maximizing the marginal likelihood of Gaussian process conditioned on the observations. Details can be found in \citep{Rasmussen2006,Osborne2012}.

\begin{table}[t]
\begin{center}
	\begin{tabular}{c|ccc}
		%\hline
		method & complexity & rate & guarantee\\
		%\hline
		\midrule
		MCMC & $\mathcal{O}(N)$ & variable & ergodic theorem\\
		i.i.d. MC & $\mathcal{O}(N)$ & $\frac{1}{\sqrt{N}}$ & law of large numbers\\
		herding & $\mathcal{O}(N^2)$ & $\frac{1}{\sqrt{N}} \geq \cdot \geq \frac{1}{N}$ & \citep{Chen2010,Bach2012} \\
		SBQ & $\mathcal{O}(N^3)$ & unknown & approximate submodularity\\
		%\hline
	\end{tabular}
\end{center}
\caption{A comparison of the rates of convergence and computational complexity of several integration methods.}
\label{tbl:rates}
\end{table}

While we have shown that Bayesian Quadrature provides the optimal re-weighting of samples, computing the optimal weights comes at an increased computational cost relative to herding. 

The computational complexity of computing Bayesian quadrature weights for $N$ samples is $\mathcal{O}(N^3)$, due to the necessity of inverting the Gram matrix $K(\vx, \vx)$. Using the Woodbury identity, the cost of adding a new sample to an existing set is $\mathcal{O}(N^2)$. For herding, the computational complexity of evaluating a new sample is only $\mathcal{O}(N)$, making the cost of choosing $N$ herding samples $\mathcal{O}(N^2)$. For Monte Carlo, the cost of adding an i.i.d. sample from the target distribution is only $\mathcal{O}(1)$.

The relative computational cost of computing samples and weights using \bq{}, herding, and sampling must be weighed against the cost of evaluating $f$ at the sample locations. Depending on this trade-off, the three sampling methods form a Pareto frontier over computational speed and estimator accuracy. When computing $f$ is cheap, we may wish to use Monte Carlo methods. In cases where $f$ is computationally costly, we would expect to choose the \sbq{} method. When $f$ is relatively expensive, but a very large number of samples are required, we may choose to use kernel herding instead. However, because the rate of convergence of \sbq{} is faster, there may be situations in which the $\mathcal{O}(N^3)$ cost is relatively inexpensive, due to the smaller $N$ required by \sbq{} to achieve the same accuracy as compared to using other methods. There also exists the possibility to switch to a less costly sampling algorithm as the number of samples increases. Table \ref{tbl:rates} summarizes the rates of convergence of all the methods considered here.