%!TEX root = thesis.tex

Probability distributions and random variables play a central role in Bayesian analysis and reasoning. In Bayesian machine learning, we model observed data by introducing latent parameters. The relationship between latent parameter and data is probabilistically modelled via a probability distribution, called the likelihood. The parameter itself is treated as a random variable and is assigned a prior probability distribution.
The most important distribution of all, the posterior is obtained by combining the likelihood and prior via Bayes' rule. In the Bayesian framework the posterior distribution captures all that there is to learn about the unobserved parameter from the data.

\paragraph{divergences are important} \lipsum[1]Several applications need measures of similarity between

\paragraph{foo}Measures of information \lipsum[1]

\paragraph{proper scoring} In this thesis I focus on proper measures of divergence and information, and show how they can be used to solve problems in Bayesian machine learning. \lipsum[1]

This thesis is divided into three main parts.

Part \ref{part:1} is a general discussion of proper scoring rules, measures of divergence and information. Chapter \ref{sec:scoring_rules} introduces key concepts: proper and strictly proper scoring rules, Bregman divergences and the value of information. I give a thorough review of the most common scoring rules used in statistics and machine learning. In addition, I show that maximum mean discrepancy, a kernel-based divergence measure is also related to scoring rules and introduce a novel scoring rule called the spherical kernel score.

In Chapter \ref{sec:information_geometry} I develop graphical visualisation technique that allows me to illustrate the behaviour of the various scoring rules reviewed in the previous chapter. To do this I introduce the concept of information geometry and the Riemannian manifold induced by a proper scoring rule. I illustrate the behaviour of the logarithmic, Brier and kernel scoring rules on various families of probability distributions.

Part \ref{part:2} discusses the application of proper divergences in approximate Bayesian inference. Approximate inference methods make it possible to infer probabilistic models from data when exact Bayesian inference are computationally intractable. Chapter \ref{sec:approximate_inference} outlines a general framework for approximate inference using scoring rule-based divergences. The framework is derived from Bayesian decision theory, and is called loss-calibrated approximate inference. I discuss opportunities, limitations and point out to connections to existing methodology. I also give a blueprint to a class of nonparametric approximate inference algoritmhs called loss-calibrated quasi-Monte Carlo.

In Chapter \ref{sec:herding} I study two practical examples of quasi-Monte Carlo algorithms, kernel herding and Bayesian quadrature. I show that both algorithms seek to minimise maximum mean discrepancy, hence they are special cases of loss-calibrated quasi-Monte Carlo. 

Part \ref{part:3} focuses on measures of information and their application to Bayesian active learning. Active learning is a form of machine learning in which a learning algorithm is able to interactivly querry the environment in order to build a statistical model more quickly. In Chapter \ref{sec:active_learning_framework} a general framework for Bayesian active learning is described, based on proper scoring rules and associated information quantities. The chapter is also a unifying review of several practical machine learning algorithms that are reinterpreted in this framework.

Chapter \ref{sec:BALD} 

Chapter \ref{sec:quantum}