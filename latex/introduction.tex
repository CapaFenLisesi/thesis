%!TEX root = thesis.tex

Probability distributions and random variables play a central role in Bayesian computation. In Bayesian analysis, one models observed data by introducing latent parameters, which are treated as random variables and assigned a \emph{prior probability distribution}. The relationship between latent parameter and data is modelled via a \emph{probability distribution}, called the likelihood. The most important object of Bayesian analysis, the  \emph{posterior distribution} is obtained by combining the likelihood and prior via Bayes' rule. In the Bayesian framework the posterior captures all that there is to learn about the unobserved parameter from the data.

In order to design effective Bayesian machine learning algorithms one needs rich frameworks for describing, characterising, comparing and manipulating probability distributions and random variables. This thesis is centred around one such framework: information geometry defined by strictly proper scoring rules.

The scoring rule framework allows us to define general notions of divergence. Divergence functionals quantify the difference or discrepancy between two probability distributions. The best known example, the Kullback-Leibler (KL)divergence, has been used for decades in a wide range of Bayesian applications. Divergences play a central role in approximate inference. 

Scoring rules also provide ways to quantify the value of information one random quantity provides about another one. The value of information functionals that one can define based on the framework generalise Shannon's mutual information which is used in a variety of applications in statistics and communications. Quantifying the value of information is of central importance in active machine learning and optimal experiment design, where one seeks to select the most informative measurements to perform. The generalisations considered in this thesis allows one to contextualise the value of information

Part \ref{part:1} is a general discussion of proper scoring rules, measures of divergence and information. Chapter \ref{sec:scoring_rules} introduces key concepts: proper and strictly proper scoring rules, Bregman divergences and the value of information. I give a thorough review of the most common scoring rules used in statistics and machine learning. In addition, I show that maximum mean discrepancy, a kernel-based divergence measure is also related to scoring rules and introduce a novel scoring rule called the spherical kernel score.

In Chapter \ref{sec:information_geometry} I develop graphical visualisation technique that allows me to illustrate the behaviour of the various scoring rules reviewed in the previous chapter. To do this I introduce the concept of information geometry and the Riemannian manifold induced by a proper scoring rule. I illustrate the behaviour of the logarithmic, Brier and kernel scoring rules on various families of probability distributions.

Part \ref{part:2} discusses the application of proper divergences in approximate Bayesian inference. Approximate inference methods make it possible to infer probabilistic models from data when exact Bayesian inference are computationally intractable. Chapter \ref{sec:approximate_inference} outlines a general framework for approximate inference using scoring rule-based divergences. The framework is derived from Bayesian decision theory, and is called loss-calibrated approximate inference. I discuss opportunities, limitations and point out to connections to existing methodology. I also give a blueprint to a class of nonparametric approximate inference algoritmhs called loss-calibrated quasi-Monte Carlo.

In Chapter \ref{sec:herding} I study two practical examples of quasi-Monte Carlo algorithms, kernel herding and Bayesian quadrature. I show that both algorithms seek to minimise maximum mean discrepancy, hence they are special cases of loss-calibrated quasi-Monte Carlo. 

Part \ref{part:3} focuses on measures of information and their application to Bayesian active learning. Active learning is a form of machine learning in which a learning algorithm is able to interactivly querry the environment in order to build a statistical model more quickly. In Chapter \ref{sec:active_learning_framework} a general framework for Bayesian active learning is described, based on proper scoring rules and associated information quantities. The chapter is also a unifying review of several practical machine learning algorithms that are reinterpreted in this framework.

Chapter \ref{sec:BALD} 

Chapter \ref{sec:quantum}