%!TEX root = thesis.tex

Machine learning is fragmented. Researchers in the field publish an endless stream of ideas, methods, optimisation problems and probabilistic models. Substantial progress is made when, from this system of thoughts, unifying frameworks and generalisations emerge. Unifying views help researchers establish connections between concepts and techniques they previously thought were unrelated, and develop new ones in a more systematic fashion.

A good example of this unification process is the emergence of probabilistic generative models. By the early '90s, several unsupervised learning algorithms existed. Principal components analysis, Gaussian mixture models, slow feature analysis, Kalman-filters, autoencoders, independent component analysis, Boltzmann machines, factor analysis, hidden Markov models and others co-existed for years or decades, but each of these methods were by large studied in isolation. The relationship between these methods became clear when a unifying framework, based on probabilistic generative models emerged \citep{Lauritzen1996,Roweis1999,Tipping1999,Turner2007}.

Over the past decades, graphical models \citep{Lauritzen1996} have become the standard language for describing complicated, hierarchical probabilistic models. Each new probabilistic model is explained in terms of conditional independence statements represented graphically. The movement introduced the useful distinction between models and algorithms: inference methods are usually developed for general classes of graphical models, rather than in the context of a particular model. The relationship between models is now clear, and it became relatively straightforward to adopt probabilistic models to new problems.

I hope this thesis contributes to the unification of machine learning by presenting a unifying framework for Bayesian machine learning problems based on scoring rules and information geometry. The unifying view allows one to uncover relationships between seemingly unrelated methods. There are four main connections pointed out in this thesis.

\paragraph{Score matching, approximate inference and active learning} Information geometry provides a common basis for score matching, approximate Bayesian inference and Bayesian active learning. Scoring-rule-based divergences play a central role in all three of these problems. In score matching and approximate inference one tries to minimise divergences, in Bayesian active learning the goal is to maximise them.

\paragraph{Maximum mean discrepancy and kernel scoring rule} The main original contribution in chapter \ref{sec:scoring_rules} is the connection between maximum mean discrepancy \citep{Gretton2012} and the kernel scoring rule \citep{Jose2008}. These related concepts have been developed and studied by two distinct communities, derived from different first principles. Establishing this connection allowed me to define a new concepts, such as the \emph{kernel spherical divergence} in Eqn.\ \eqref{eqn:spherical_kernel_divergence} and the \emph{kernel value of information} in Eqn.\ \eqref{eqn:kernel_information}.

\paragraph{Kernel herding, Bayesian quadrature and loss-calibrated quasi-Monte Carlo} Herding was originally introduced by \citet{welling2009herding} as a heuristic procedure obtained by ``taking the zero temperature limit of the corresponding maximum likelihood problem'' \citep[quoted from][]{welling2009herding}. Although authors have studied several properties of this method, it was unclear whether and how herding is related to existing methods in machine learning. In \citep{Huszar2012herding} we showed that herding is closely related to Bayesian quadrature. In chapter \ref{sec:approximate_inference} I also show that it is a special case of the general procedure I call loss-calibrated quasi-Monte Carlo.

\paragraph{Bayesian optimisation, Bayesian quadrature and active learning} In chaper \ref{sec:active_learning_framework} I introduce a unifying framework for Bayesian active learning. This unifying framework naturally accommodates Bayesian optimisation, Bayesian quadrature, information theoretic active learning and transductive learning. Scoring rules provide a unifying language for these models inasmuch as they can all be described via the scoring rule they use to quantify the usefulness of the Bayesian posterior.

\vspace{11pt}

The main limitation of the presented theoretical framework is that it is non-constructive. General scoring rule-based information quantities and divergences are intractable to calculate, and the framework provides no guidance as to how they can be viably approximated in real algorithms. Several approximation algorithms have been proposed to tackle individual problems, such as variational message passing \citep{Winn2006} and expectation-propagation \citep{Minka2001} in the context of approximate inference, or the informative vector machine \citep{Lawrence2004} and information-greedy quasi-Newton methods \citep{Hennig2012newton} in the context of active learning.

In Chapter \ref{sec:BALD} I presented BALD, Bayesian Active Learning by Disagreement, an algorithm for Bayesian active learning based on Shannon's information. In addition to theoretical contributions, I presented applications of this state-of-the-art algorithm to important real-life problems.

\paragraph{Binary classification} Binary classification is one of the most widely studied problems in machine learning and pattern recognition. In Chapter \ref{sec:BALD} I presented a computationally attractive algorithm to perform Bayesian active learning in the Gaussian process classification model. Active learning in this Bayesian non-parametric model is particularly hard to tackle with traditional approaches because the underlying parameter space has infinite dimensions. Experimental results show that the approach based on BALD often outperforms state-of-the-art methods of comparable complexity.

\paragraph{Preference elicitation} Learning about people's preferences has several commercial and research applications. In Chapter \ref{sec:BALD} I extended BALD to the problem of binary preference learning, and demonstrated that the approach achieves good performance in a variety of datasets. In follow-up work not presented in this thesis we extended the framework to collaborative setting, where differences in multiple people's preferences are also modelled \citep{Houlsby2012preference}.

\paragraph{Quantum tomography} Learning about the state of a quantum system such as a quantum computer is a difficult statistical inference task, known as quantum state tomography. The number of measurements required to pin down a quantum state grows faster than exponential in the size of the system. To speed this process up, in Chapter \ref{sec:quantum} I presented an application of BALD to Bayesian quantum tomography. In simulations I demonstrated substantial speedup compared to the fastest known methods based on mutually unbiased bases. The procedure has since been implemented experimentally in follow-up work by \citet{Kravtsov2013}.