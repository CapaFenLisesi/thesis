%!TEX root = ../thesis.tex

\begin{summarycontributions}
The material presented in this chapter is largely introductory, based on textbook material and references provided. The following otherwise unpublished material represent original contributions in this chapter:
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
	\item connection between maximum mean discrepancy and scoring rules (Section \ref{sec:kernel_score})
	\item the kernel value of information and diversity operator (Section \ref{sec:kernel_score})
	\item the spherical scoring rule (Section \ref{sec:spherical_kernel_score})
\end{itemize}
\end{summarycontributions}
\section{Introduction}

In this section I describe scoring rules, a framework for quantifying the accuracy of probabilistic forecasts. Scoring rules allow one to define useful generalisations of well-known information quantities, such as entropy, divergence and mutual information. Each scoring rule defines a unique geometry over probabilistic models, which can be exploited in a variety of statistical applications. They provide a unifying framework for problems such as parameter estimation, approximate Bayesian inference and Bayesian optimal experiment design.

Imagine we want to build a probabilistic forecaster that predicts the value of a random quantity $\X$. We can describe any such probabilistic forecaster as a probability distribution $P(x)$ over the space of possible outcomes $\Xe$. After observing the outcome $X=x$ we want to assess how good our predictions were. \emph{Scoring rule} is a general term to describe any functional that quantifies this: if the outcome is $X=x$, and our prediction was $P$ we incur a score $\score (x,P)$.  Mathematically, a scoring rule can be any measurable function that maps an outcome-probability distribution pair onto real numbers: $\score:\Xe\times\probmeasures{\Xe}\mapsto\Reals\cup\{\infty\}$. Throughout this thesis I use $\probmeasures{\Xe}$ to denote the set of Borel probability distributions or measures over a set $\Xe$. I follow a convention of \citet{Dawid1994} whereby scoring rules are interpreted as losses, so lower values are associated with better predictions.

A well known example of scoring rules is the logarithmic score, or simply the log score: $\score_{\mbox{log}}(x,P) = -\log P(\x)$, which is the central quantity of interest in maximum likelihood estimation. The logarithmic scoring rule is a fundamental example and has several unique characteristics (see section \ref{sec:log_score}), which made it ubiquitous in the probabilistic machine learning community. But it is not the only one, and there are situations in which it is more convenient or efficient to use alternative scoring rules instead of the logarithmic. This chapter will give further examples of scoring rules and describe where they have been applied in statistics or machine learning.

\section{Information quantities}

A scoring rule allows us to define useful information quantities, which can be exploited in a variety of applications \citep[see also][]{Gneiting2007}: these are generalised notions of entropy, divergence and value of information.
\begin{definition}[Generalised entropy]
Given a scoring rule $\score:\Xe\times\probmeasures{\Xe}\mapsto\Reals$, let us define the generalised entropy of a distribution $P\in\probmeasures{\Xe}$ as follows:
%
\begin{equation}
	\genentropy{S}{P}= \expect{x\sim P} \score(x,P).
\end{equation}
\end{definition}


This entropy measures how difficult it is to forecast the outcome on average, when the true distribution $P$ of outcomes is known and used as the forecasting model. One can often think of this quantity as a measure of uncertainty in the distribution, and as we will see this quantity is also closely related to the Bayes-risk of decision problems (section \ref{sec:loss_scoring_rule}).

A further quantity of interest is the divergence between two distributions $P$ and $Q$.

\begin{definition}[Generalised divergence]\label{def:generalised_divergence}
Given a scoring rule $\score:\Xe\times\probmeasures{\Xe}\mapsto\Reals$, let us define the divergence between two distributions $P,Q\in\probmeasures{\Xe}$ as follows:
%
	\begin{align}
		\divergence{\score}{P}{Q} &= \expect{x\sim P} \score(x,Q) - \expect{x\sim P} \score(x,P)\\\label{eqn:def_divergence}
		&=\expect{x\sim P} \left[ \score(x,Q) - \score(x,P) \right].
	\end{align}
\end{definition}

The divergence measures how much worse off one would be using some probability distribution $Q$, rather than $P$, to forecast a quantity $\X$, which is indeed sampled from $P$. It can be interpreted as a measure of dissimilarity between two distributions $P$ and $Q$. Divergences are normally non-symmetric, that is $\divergence{\score}{P}{Q}$ generally does not equal $\divergence{\score}{Q}{P}$, although later in this chapter we will see examples of symmetric divergences.

Since scoring rules measure how accurate a probabilistic forecast is, it is desirable that using the true probability model $P$ should never incur a higher average score than using an incorrect model $Q$. If that would be the case, the divergence would always be non-negative. However, this is not automatically true for all scoring rules. A scoring rule that has this desirable property is called \emph{proper}.

\begin{definition}[Proper scoring rule]\label{def:strictly_proper}
	$\score:\Xe\times\probmeasures{\Xe}\mapsto\Reals$ is a \emph{proper scoring rule} with respect to a class of distributions $\Qe$ if $\forall P,Q\in\Qe$ the following inequality holds:
	\begin{equation}
		\expect{x\sim P} \score(x,Q) \geq \expect{x\sim P} \score(x,P),
	\end{equation}
	or equivalently in terms of the divergence $\divergence{\score}{\cdot}{\cdot}$:
	\begin{equation}
		\divergence{\score}{P}{Q} \geq 0.
	\end{equation}
	
	The scoring rule $s$ is said to be \emph{strictly proper} w.\,r.\,t.\ $\Qe$ if equality holds only when $P=Q$.
\end{definition}

Strictly proper scoring rules can therefore detect, on average, whether a forecast $Q$ matches the true distribution of the unknown quantity $P$. This property is exploited in score matching, where a parametric probability model is fitted to \iid observations.

\begin{definition}[Score matching estimate]\label{def:score_matching}
Let $\{P_{X\vert\theta}, \theta\in\Theta\}$ be a parametric family of distributions and $\score$ a strictly proper scoring rule with respect to this class. The following estimator is called the score matching estimate:
\begin{equation}
	\hat{\theta}_N(x_1,\ldots,x_N) = \argmin_{\theta\in\Theta} \sum_{n=1}^{N}\score(x_n,P_{X\vert\theta}). \label{eqn:score_matching}
\end{equation}
For most scoring rules the above estimating equation can be formulated in terms of the divergence as follows.
\begin{equation}
	\hat{\theta}_N(x_1,\ldots,x_N) = \argmin_{\theta\in\Theta} \divergence{\score}{\frac{1}{N}\sum_{n=1}^{N}\delta(\x-\x_n)}{P_{X\vert\theta}},
\end{equation}
where $\delta$ denotes the Dirac measure. The above equation is an unbiased estimating equation \citep{dawid94scoring}, and under suitable regularity conditions $\hat{\theta}_N(x_1,\ldots,x_N)$ is a consistent estimator, that is when $x_1,\ldots\sim P_{X\vert\theta_0}$ \iid
\begin{equation}
	\lim_{N \rightarrow \infty} \hat{\theta}_N(x_1,\ldots,x_N) = \theta_0\hspace{1cm} P_{X\vert\theta_0}\mbox{-\,almost surely}.
\end{equation}
\end{definition}

The divergence defined in \eqref{eqn:def_divergence} is a special case of Bregman divergences. Bregman divergences are an important class of divergence functions on complex domains, and include well known measures of distance or dissimilarity such as the Euclidean distance or KL divergence.

\begin{definition}[Bregman divergence]
	Let $H$ be a differentiable, strictly concave function on a convex domain $\Theta$. For $P,Q\in\Theta$ 
	\begin{align}
		\divergence{Bregman,H}{P}{Q} = H(P) - H(Q) + \scalar{\nabla H(Q)}{Q-P},
	\end{align}
	where $\scalar{f}{g} = \int_{x\in\Xe} f(x)g(x) dx$ denotes scalar product. If P is a probability distribution, one can also write $\scalar{P}{f} = \expect{x\sim P} f(x)$. The symbol $\nabla$ denotes differentiation.
\end{definition}

\begin{statement}[Generalised divergences $d_{\score}$ for strictly proper $\score$ are Bregman divergences]
	Let $S$ be a strictly proper scoring rule, with generalised entropy $\genentropy{\score}{P}$. If $\genentropy{\score}{P}$ is differentiable with respect to $P$, then the generalised divergence $\divergence{\score}{P}{Q} = \expect{x\sim P}\score(x,Q) - \genentropy{\score}{P}$ is a Bregman divergence with $H(\cdot) = \genentropy{\score}{\cdot}$.
\begin{proof}[Proof (sketch)]
	Review the definition of the entropy $\genentropy{\score}{P}$, using linear algebra notation for the expectation $\scalar{P}{f} = \expect{x\sim P}f(x)$ as in \citep{Amari2010}:
		\begin{equation}
			\genentropy{S}{P} = \expect{x \sim P}S(x,P) = \scalar{P}{S(\cdot,P)}.
		\end{equation}
	Using this notation, noting the linearity of scalar products (expectation)
		\begin{align}
			\nabla\genentropy{S}{P} &=  \nabla\scalar{P}{S(\cdot,P)}\\
				&= S(\cdot,P) + \scalar{P}{\nabla S(\cdot,P)}.
		\end{align}
	The second term $\scalar{P}{\nabla S(\cdot,P)}=0$ because of strictly proper property of $S$. Thus
 		\begin{align}
 		 	\divergence{Bregman,\mathbb{H}_{S}}{P}{Q}&= \genentropy{S}{Q}  + \scalar{\nabla \genentropy{S}{Q}}{P-Q} -  \genentropy{S}{P} \\
 		 		&= \genentropy{S}{Q} + \scalar{S(\cdot,Q)}{P - Q} -  \genentropy{S}{P}\\
 		 		&= \scalar{S(\cdot,Q)}{P} - \genentropy{S}{P}\\
 		 		&= \divergence{S}{P}{Q}.
 		\end{align}
 		Concavity of $\genentropy{\score}{P}$ also follows from strictly proper property $\divergence{S}{P}{Q}>0,P\neq Q$.
\end{proof}
\end{statement}

\begin{figure}[t]
\begin{center}
	\includegraphics[width=0.8\columnwidth]{figs/embeddings/Bregman}
\end{center}
\caption[Pictorial illustration of Bregman divergences]{Pictorial illustration of Bregman divergences. Peter and Quentin are points who live on a convex hill, whose surface is described by the concave function $H(\cdot)$. Peter lives at $(p,H(p))$, Quentin at $(q,H(q))$. Because the hill is convex and they are both points, they cannot normally see each other, unless $p=q$. Anyone above the tangential line $H[q] + \dot{H}(q)(\cdot-q)$ can see Quentin, but Peter is normally below this line. If Peter wants to see Quentin, he has to jump up. The Bregman divergence $\divergence{H}{p}{q}$ measures how high Peter has to jump to see Quentin. In this example $H$ was chosen to be the Brier (quadratic) entropy, so here the divergence is symmetric, but this is not generally the case.\label{fig:Bregman}}
\end{figure}

For a more elaborate proof and discussion of Bregman divergences and scoring rules please refer to \citep{Amari2010,Dawid2007}. An intuitive explanation of Bregman divergences is given in Figure \ref{fig:Bregman}.

The information quantities introduced so far only dealt with single random variable $\X$, and comparing probability distributions over the same variable. In the following I will define information quantities that describe the relationship and dependence between more than one variable. A particularly useful quantity is the value of information, which quantifies how much useful information one random variable $Y$ holds about another one $\X$.

\begin{definition}[Generalised value of information]
	\label{def:value_of_information}
	Let $X,Y$ be random variables with joint distribution $P\in\probmeasures{\Xe\times\Ye}$. Let $\score:\Xe\times\probmeasures{\Xe}\mapsto\Reals$ be a scoring rule over the variable $X$. We define the value of information in variable $Y$ about variable $X$ with respect to the scoring rule $\score$ as
	\begin{equation}
		\information{S}{X}{Y} =  \expect{x\sim P_{X}}\score(x,P_{X})- \expect{y\sim P_{Y}} \expect{x\sim P_{X\vert Y=y}}\score(x,P_{X\vert Y=y}).
	\end{equation}
	Alternatively, we can write information in terms of the generalised entropy or divergence functions
		\begin{align}
			\information{\score}{X}{Y} &=  \genentropy{\score}{P_{X}} - \expect{y \sim P_{Y}} \genentropy{\score}{P_{X\vert Y=y}}\\
				&= \expect{y\sim P_{Y}}\divergence{S}{P_{X\vert Y=y}}{P_{X}}.
		\end{align}
\end{definition}

This quantity measures the extent to which observing the value of $Y$ is useful in forecasting variable $X$. Remarkably, this information quantity is non-symmetric. Indeed, the definition only requires a scoring rule over the variable $X$, but none over variable $Y$, so defining the value of information in $Y$ about $X$ does not even imply a definition of the value of information in $X$ about $Y$.

If the scoring rule is proper, the value of information is always non-negative. Furthermore, if the scoring rule is strictly proper, the information is zero, if and only if the two variables are independent.

\begin{theorem}
	Let $\score:\Xe\times\probmeasures{\Xe}\mapsto\Reals$ be a strictly proper scoring rule with respect to probability distributions $\probmeasures{\Xe}$, and $P\in\probmeasures{\Xe\times\Ye}$ the joint probability of variables $X$ and $Y$. Then the two statements are equivalent:
	\begin{enumerate}
		\item $\information{\score}{X}{Y} = 0$, and
		\item $\independent{X}{Y}$; the variables $X$ and $Y$ are independent.
	\end{enumerate}
	\begin{proof}
		If $X$ is independent of $Y$, then $\forall y: P_{X\vert Y=y} = P_{X}$, which implies $\forall y:  \divergence{S}{P_{X\vert Y=y}}{P_{X}}=0$, and hence $\information{\score}{X}{Y} = 0$.
		
		On the other hand, $\information{\score}{X}{Y} > 0$ implies $\exists y: \divergence{S}{P_{X\vert Y=y}}{P_{X}} > 0$, therefore by strict propriety of $\score$, $\exists y: P_X \neq P_{X\vert Y=y}$, hence $\X$ and $Y$ are dependent.
	\end{proof}
\end{theorem}

As a corollary, strictly proper scoring rules are equivalently strong in the sense that if one detects dependence between variables, than any of them will:

\begin{corollary}[Weak equivalence of strictly proper scoring rules]
	Let $\score_1,\score_2:\Xe\times\probmeasures{\Xe}\mapsto\Reals$ be two strictly proper scoring rules over $X$. $X$ and $Y$ are two random variables. Then $\information{\score_1}{X}{Y} > 0$ if and only if $\information{\score_2}{X}{Y} > 0$.
\end{corollary}

It also follows that the value of information defined by strictly proper scoring rules is weakly symmetric in the following sense:

\begin{corollary}[Weak symmetry of information]
	Let $\score_X:\Xe\times\probmeasures{\Xe}\mapsto\Reals$ be a strictly proper scoring rule over $X$ and $\score_Y:\Ye\times\probmeasures{\Ye}\mapsto\Reals$ be a strictly proper scoring rule over $Y$.  Then $\information{\score_X}{X}{Y} > 0$ if and only if $\information{\score_Y}{Y}{X} > 0$.
\end{corollary}

We can also define a conditional version of this quantity which measures how much additional information $Y$ provides about $X$ given the value of a third variable $Z$ which is also observed.

\begin{definition}[Conditional value of information]
	\label{def:conditional_value_of_information}
	Let $X,Y,Z$ be random variables with joint distribution $P\in\probmeasures{\Xe\times\Ye\times\Ze}$. Let $\score:\Xe\times\probmeasures{\Xe}\mapsto\Reals$ be a scoring rule over the variable $X$. We define the conditional value of information in variable $Y$ about variable $X$ with respect to the scoring rule $\score$ as
	\begin{equation}
		\conditionalinformation{S}{X}{Y}{Z=z} =  \expect{x\sim P_{X\vert Z=z}}\score(x,P_{X\vert Z=z})- \expect{y\sim P_{Y\vert Z=z}} \expect{x\sim P_{X\vert Y=y, Z=z}}\score(x,P_{X\vert Y=y, Z=z}).
	\end{equation}
	Alternatively, we can write information in terms of the generalised entropy or divergence functions
		\begin{align}
			\conditionalinformation{\score}{X}{Y}{Z=z} &=  \genentropy{\score}{P_{X\vert Z=z}} - \expect{y \sim P_{Y \vert Z=z}} \genentropy{\score}{P_{X\vert Y=y, Z=z}}\\
				&= \expect{y\sim P_{Y}}\divergence{S}{P_{X\vert Y=y, Z=z}}{P_{X\vert Z=z}}.
		\end{align}
\end{definition}

Just like in the case of non-conditional value of information, the definition only calls for a scoring rule over $X$, not over the other variables $X$ or $Z$. Just like the value of information was related to statistical independence, conditional value of information is related to conditional independence in the following sense.

\begin{statement}
	Let $\score:\Xe\times\probmeasures{\Xe}\mapsto\Reals$ be a strictly proper scoring rule with respect to Borel probability distributions $\probmeasures{\Xe}$, and $P\in\probmeasures{\Xe\times\Ye\times\Ze}$ the joint probability of variables $X$ and $Y$ and $Z$. Then the following two statements are equivalent:
	\begin{enumerate}
		\item $\conditionalinformation{\score}{X}{Y}{Z=z} = 0$, and
		\item $\conditionallyindependent{X}{Y}{Z=z}$; the variables $X$ and $Y$ are conditionally independent given $Z=z$.
	\end{enumerate}
\end{statement}


\section{Examples of scoring rules}

After having discussed general properties of scoring rules and information quantities based on them, let us look at particular examples of scoring rules, entropies and divergences they define. I will review three widely known scoring rules, the logarithmic, Brier (quadratic) and spherical scores. Then I present the kernel scoring rule, which is lesser known in the statistics literature. I establish the connections between the kernel scoring rule to the maximum mean discrepancy, a divergence measure that has gained popularity recently in the machine learning community over the past years \citep{Gretton2012,Sriperumbudur2008}.

Following the discussion of kernel scoring rules I define a novel scoring rule, called \emph{spherical kernel scoring rule}, examine its properties, and provide a proof that it is strictly proper. Finally, I show the connections between scoring rules and Bayesian decision theory, and explain how decision problems give rise to scoring rules and associated information quantities.

\subsection{The logarithmic score\label{sec:log_score}}

The most straightforward, and most widely used scoring rule is the logarithmic score which is of the form:
%
\begin{equation}
	\score_{log}(x,P) = - \log P(x).
\end{equation}

This score is widely used, most notably in maximum likelihood estimation of parametric models. Maximum likelihood estimation is a special case of score matching as defined in Definition \ref{def:score_matching}:
%
\begin{equation}
	\hat{\theta}_{ML} = \argmax_{\theta} \sum_{n=1}^{N} \log P(x_i \vert \theta).
\end{equation}

The associated entropy function is Shannon's entropy, also known as differential entropy for continuous distributions:
%
\begin{equation}
	\genentropy{Shannon}{P} = - \expect{x\sim P} \log P(x).
\end{equation}

The divergence function is the Kullback-Leibler (KL) divergence, which is very widely used in approximate Bayesian inference, model selection and active learning:
%
\begin{equation}
	\KL{P}{Q} = \expect{x\sim P} \log \frac{P(x)}{Q(x)}.\label{eqn:KL_divergence}
\end{equation}

The KL divergence is only well-defined when the distribution $Q$ is absolutely continuous with respect to $P$. This is a serious limitation of the KL divergence for our purposes in later chapters: If $P$ is a continuous density, then $Q$ has to be continuous as well for the KL divergence to be defined. Therefore we cannot express the KL divergence between, say, an empirical distribution of samples and a continuous distribution, as we did in Definition \ref{def:score_matching}.

A related problem is that Shannon's entropy of atomic distributions or mixed atomic and continuous distributions is either not well defined or depends only on the relative weight of the atoms but not on their locations. As we will see, information quantities based on other scoring rules remain well defined for wider classes of distributions including atomic ones.

These problems are related to a property of the logarithmic score, known as locality: The value of the scoring rule $\score(x,P)$ only depends on the value of the density function evaluated at the point $x$. This is a unique property of the logarithmic score: any strictly proper scoring rule that is local is analogous to the logarithmic score. Note, that there are weaker definitions of locality of scoring rules, which hold for scoring rules other than the logarithmic \citep{Parry2012, Dawid2012}.

\cbstart Maximum likelihood estimation, the logarithmic score and KL divergence play fundamental roles in statistics and in the field of information geometry \citep{Amari00}. KL divergence, and the associated Fisher information metric can be used to define the natural Riemannian geometry of finite-dimensional discrete probability distributions \cite{Amari00}.

The Fisher information matrix also plays a central role in the Crem\'{e}r-Rao bound \citep{Cremer99}, which expresses a lower bound for the variance of any estimator of the parameters of a statistical model. In the limit of increasing number of observations the maximum likelihood estimator achieves this lower bound, and is hence it is called assymptotically efficient. Most other scoring rules considered in this chapteres do not yield assymptotically efficient estimators \citep[see \eg][]{Varin11}. These deep connections to information geometry and the theory of statistical estimation make the logarithmic score such an important and ubiquitous example.
\cbend

The value of information becomes Shannon's mutual information, a crucial quantity in communication and channel coding \citep{Shannon1948, MacKay2002}. Shannon's mutual information has several equivalent definitions. Interestingly, it can be rewritten as the KL divergence between the joint distribution $P_{\X,Y}$ and the product of its marginals $P_{\X}P_{Y}$:
%
\begin{align}
	\information{Shannon}{X}{Y} &= \genentropy{Shannon}{X} - \expect{y \sim P_{Y}} \genentropy{Shannon}{P_{X\vert Y=y}}\\
		&= \expect{y \sim P_{Y}}\KL{P_{X\vert Y=y}}{P_X}\\
		&= \expect{y \sim P_{Y}}\left[\expect{x \sim P_{X\vert Y=y}} \log\frac{P_{X\vert Y=y}(x)}{P_X(x)} \right]\\
		&= \expect{(x,y) \sim P} \log \frac{P(x,y)}{P_{X}(x)P_{Y}(y)}\\
		&= \KL{P(x,y)}{P_{X}(x)P_{Y}(y)}.\label{eqn:mutualinfo_as_KLdivergence}
\end{align}

As a consequence, Shannon's information is symmetric. Recall, that the value of information is generally non-symmetric, 

The Shannon information in $Y$ about $X$ is the same as the Shannon information in $X$ about $Y$. This is a remarkable property of the log-score and, as we concluded in the previous section, is not generally true for value of information defined based on general scoring rules.

For completeness, I note here that some authors have generalised Shannon's mutual information along the lines of \eqref{eqn:mutualinfo_as_KLdivergence}, by replacing the KL divergence with a more general divergence $d$:
%
\begin{equation}
	\mathbb{J}_{d}(X,Y) = \mbox{d}\left[ P(x,y) \middle\| P_{X}(x)P_{Y}(y) \right].\label{eqn:mutualinfo_generalisations}
\end{equation}

Examples of information functionals defined this way are described in \citep{Poczos2011}.
On one hand, an information functional like $\mathbb{J}$ has several nice properties, most notably that it is always symmetric. On the other hand, in the general case we loose the intuitive meaning of information as ``the extent to which observing the value of one variable is useful for predicting the value of the other one''. Furthermore, if we wanted to use a divergence function corresponding to a scoring rule, the scoring rule should be defined over the joint space $\Xe\times\Ye$, which is often not desired.

\subsection{The pseudo-likelihood\label{sec:pseudolikelihood}}

The idea of maximum pseudo-likelihood estimation was introduced originally by \citet{Besag1977} to estimate parameters of Gaussian random fields. Later it was popularised in the context of parameter estimation in general Markov random fields \citep{Comets1992} and in Boltzmann machines \citep{Hyvarinen2006}. The pseudo-likelihood is particularly useful for estimating parameters of statistical models with intractable normalisation constants.

\begin{equation}
	\score_{\mbox{pseudo}}(x,P) = - \sum_{d=1}^{D} \log P(x_d\vert x_{\neg d}),
\end{equation}
where $x_d$ denotes the $d^{\mbox{th}}$ component of the vector $\x$ and$x_{\neg d}$ denotes the vector composed of all components of $x$ other than the $d^{\mbox{th}}$ component $x_d$.

In the pseudo-likelihood each of the terms is the conditional probability over one variable conditioned on all the remaining variables. Such quantities can be computed by marginalising a single variable at a time, therefore by computing a one dimensional integral or sum:
%
\begin{equation}
	p(x_d\vert x_{\neg d}) = \frac{P(x)}{\int P(X_d=y,x_{\neg d}) dy} = \frac{C \cdot P(x)}{\int C \cdot P(X_d=y,x_{\neg d}) dy}.
\end{equation}
%
This can be computed even if the joint probability of all variables $P$ is known only up to a multiplicative constant $C$, which is very often the case.

Take the Boltzmann distribution with parameters $W$ and $b$ as an example. 
%
\begin{equation}
	P(\x) = \frac{1}{Z}\exp(x^{T}Wx + b^{T}x), x\in\{0,1\}^D,
\end{equation}
%
where $Z = \sum_{x\in\{0,1\}^D}\exp(x^{T}Wx + b^{T}x)$ is the partition function or normalisation constant that is analytically intractable to compute in the general case. On the other hand, the conditional distribution of a single component of $x$ conditioned on the rest is easy to compute as follows:
%
\begin{align}
	P(x_d\vert x_{\neg d}, W, b) &= \frac{p(x)}{\int p(x_d=y,x_{\neg d}) dy}\\
		&= \frac{\frac{1}{Z}\exp(x^{T}Wx + b^{T}x)}{\sum_{x_d\in\{0,1\}}\frac{1}{Z}\exp(x^{T}Wx + b^{T}x)}\\
		&= \frac{\exp(x^{T}Wx + b^{T}x)}{\sum_{x_d\in\{0,1\}}\exp(x^{T}Wx + b^{T}x)}\\
		&= \frac{\exp\left( x_d \left( W_{d,d} + 2 W_{d,\neg d}^{T}x_{\neg d} + b_d \right)\right)}{\exp( W_{d,d} + 2 W_{d,\neg d}^{T}x_{\neg d} + b_{d}) + 1}.
\end{align}

The pseudo-likelihood thus becomes a sum of easy-to-compute sigmoidal terms. These sigmoidal terms, and their derivatives with respect to parameters $W$ and $b$ can be computed in polynomial time, allowing for fast estimation algorithms. \citet{Hyvarinen2006} showed that pseudo-likelihood estimation -- score matching with the pseudo-likelihood score -- is consistent for fully visible Boltzmann machines. \citet{Besag1977,Comets1992} showed similar results for Markov random fields.

The difference between the pseudo-likelihood score and the log score becomes more apparent when rewriting the log score by the chain rule of joint probabilities:
%
\begin{equation}
	\score_{\mbox{log}}(x,p) = - \log P(x) =  - \sum_{d=1}^{D} \log P(x_d\vert x_{1:d-1}).
\end{equation}

Here the $d^{\mbox{th}}$ term is a probability conditioned on $d-1$ variables. Computing the $d^{\mbox{th}}$ term therefore would require $D-d$ dimensional integral in the general case. The pseudo-likelihood makes computations more efficient by conditioning on more variables than needed by the chain rule, therefore requiring lower dimensional integrals.

\citet{Csiszar2004} showed that pseudo-likelihood score is strictly proper for strictly positive distributions. Moreover, for always positive distributions the following generalisation of the pseudo-likelihood is also a strictly proper scoring rule \citep*{Dawid2012}:
%
\begin{equation}
	\score_{\mbox{DLP12}}(x,P) = - \sum_{d=1}^{D} \score_d\left(x_d, P_{X_d \vert X_{\neg d}=x_{\neg d}}\right),
\end{equation}
%
where $\score_d$ are strictly proper scoring rules for each dimension.

\cbstart

\subsection{Composite likelihoods\label{sec:composite_likelihood}}

Composite likelihoods were introduced by \citet{Lindsay88}, and they can be thought of as a further generalisation of the pseudo-likelihood. The main motivation behind composite likelihoods is to construct estimators which do not require the evaluation of intractable marginalisation constants which makes logarithmic score infeasible in many practical applications. Instead, composite likelihood scores are constructed as sums of lower dimensional marginal or conditional logarithmic scores which are feasible to compute.

In their most general form, composite likelihood can be described as follows. Let $P\in\probmeasures{\Xe}$ a probability distribution over the variable $X\in\Xe$. Let $\mathcal{A}_1,\cdot,\mathcal{A}_K\subseteq\Xe$ denote measurable events, $w_1,\ldots,w_K$ positive weights. The composite likelihood is a scoring rule of the following form:
%
\begin{equation}
	\score_{CL}(x,P) = \sum_{k=1}^{K} w_k \cdot \log P(x\in\mathcal{A}_k).\label{eqn:composite_likelihood}
\end{equation}

Because each component is strictly proper, composite likelihoods are proper -- albeit not always strictly proper -- scoring rules. By specifying the sets of events $\mathcal{A}_k$ and corresponding weights, this class generalises the log score, the pseudo-likelihood, as well as other interesting cases. This construction has been referred to by different names in the literature, including composite likelihood \citep{Lindsay88,Varin11}, pseudo-likelihood \citep{Molenberghs05}, qausi-likelihood \citep{Hjort94} and approximate likelihood \citep{Stein04}. Here I adopt the terminology of \citet{Lindsay88}, and reserve the term pseudo-likelihood to refer to the special case covered in the previous section, originally introduced by \citet{Besag1977}.

A review by \citet{Varin11} divides composite likelihood methods into two main branches based on whether they are composed of marginal or conditional probabilities.

\emph{Composite conditional likelihood} methods construct proper scoring rules out of sets of conditional distributions. Besag's pseudolikelihood of Section \ref{sec:pseudolikelihood} is the best known example of this family. \citet{Molenberghs05} considered the following composite likelihood composed of pairwise conditional distributions:
%
\begin{equation}
	\score_{MV05}(x,P) = \sum_{d=1}^{D}\sum_{\substack{e=1\\e\neq d}}^{D} \log P(x_d\vert x_e).
\end{equation}
%
A further example of composite conditional likelihoods is the order-$m$ likelihood estimation technique introduced by \citet{Azzalini83}. This approach works by omitting long-range spatial dependencies in stationary stochastic processes.

In \emph{composite marginal likelihood} methods the scoring rule is constructed from marginal distributions. The simplest of such scoring rules is the \emph{independence likelihood} \citep{Chandler07} of the following form:
%
\begin{equation}
	\score_{independence}(x,P) = \sum_{d=1}^{D} \log P(x_d).
\end{equation}
%
It is clear that the independence likelihood is strictly proper only with respect to distributions that factorise over dimensions. Thus, it only permits unbiased estimation of marginal parameters. More powerful scoring rules attempt to better capture dependencies by considering higher order marginals, such as the pairwise likelihood of \citet{Cox04}:
%
\begin{equation}
	\score_{pairwise}(x,P) = \sum_{d=1}^{D-1} \sum_{e=d+1}^{D} \log P(x_d,x_e).
\end{equation}
%
Further special cases are reviewed in \citep{Varin11}.

A central object of interest in the topic of composite likelihoods is the \emph{sandwich information matrix}, a generalisation of the Fisher information matrix. This matrix is crucial in characterising the convergence properties of statistical estimators based on composite likelihoods. For example, a composite likelihood is strictly proper scoring rule, whenever the corresponding sandwich matrix is of full rank. For definitions, and further details the reader is referred to \citep{Lindsay88} or \citep{Varin11}.

Composite likelihood scores can be generalised further -- along the lines of \citep{Dawid2012} -- by replacing the component likelihoods by arbitrary strictly proper scoring rules. It is unclear wether such generlisations provide any practical benefit, especially as the established theoretical framework based on the sandwich information matrix would not extend to this case.

\cbend

\subsection{The quadratic Brier score\label{sec:Brier_score}}

Another widely used scoring rule is the so-called \emph{Brier score} or quadratic score, originally introduced in \citep{Brier1950}. It was first applied to evaluating probabilistic weather forecasts and it is still widely used in meteorology \citep{Ferro2007} as well as in medicine \citep{Spiegelhalter2006} and epidemiology \citep{Redelmeier1991}. It is also related to the root mean squared error (RMSE) of probabilistic binary classifiers, which is a commonly used loss function for training neural networks \citep{Rumelhart1988}.

We will define the Brier score in terms of the $L^2$ norm of probability distributions, which we define as
%
\begin{equation}
	\customnorm{P}{2} \defeq \sqrt{\expect{x \sim P} P(x)}.
\end{equation}

The above definition, albeit slightly informal, is well defined and finite for most classes of probability distributions we are concerned with. For continuous distributions, $P(x)$ denotes the probability density, for discrete distributions $P(x)$ denotes the probability of outcome $x$. Similarly, one can define the scalar product between two distributions as follows:
%
\begin{equation}
	\scalar{P}{Q} \defeq \sqrt{\expect{x \sim P} Q(x)}.
\end{equation}

Using these definitions we can define the Brier score as follows:
%
\begin{align}
	\score_{Brier}(x,P) &= \customnorm{P - \delta_{x}}{2}^2\\
		&= \customnorm{P}{2}^{2} - 2P(x)  + 1\\
		&= \expect{x' \sim P} P(x') - 2P(x) + 1,
\end{align}
%
where $\delta_{x}$ is the discrete or continuous Dirac measure concentrated at the observed point $\x$.

The score gives rise to the following entropy function:
%
\begin{align}
	\genentropy{Brier}{P} &= \expect{x \sim P}\left[\expect{x' \sim P} P(x') - 2P(x) + 1\right]\\
		&= 1 -\expect{x \sim P} P(x)\\
		&= 1 - \customnorm{P}{2}^2.
\end{align}

For discrete distributions when $\dim \Xe = D$, the quadratic entropy function is bounded. It's maximum value is attained when $P$ is the $D$ dimensional uniform distribution: then it equals $1 - \sum_{d=1}^{D}\frac{1}{D^2} = 1 - \frac{1}{D}$. The upper bound is $1$ if $\dim \Xe = \infty$. The entropy function is also non-negative for discrete distributions, with $\genentropy{Brier}{P}=0$ only for atomic distributions $P=\delta_{x_0}$.

In uncountable domains, just like Shannon's entropy, The entropy function becomes unbounded from below. For atomic distributions it takes value $-\infty$. Unlike Shannon's entropy, it is always bounded from above.

The Brier divergence function becomes the squared norm of the difference between the distributions:
%
\begin{align}
	\divergence{Brier}{P}{Q} &= \expect{x \sim P}\left[\customnorm{Q}{2}^2 - 2Q(x) + 1\right] - \genentropy{Brier}{P} \\
		&= \customnorm{Q}{2}^2 -2 \expect{x \sim P} Q(x) + \customnorm{P}{2}^2 \\
		&= \customnorm{Q}{2}^2 - 2\scalar{P}{Q} + \customnorm{P}{2}^2\\
		&= \customnorm{P-Q}{2}^2. \label{eqn:Brier_divergence}
\end{align}

\cbstart
Interestingly, the Brier divergence is symmetric, and it is analogous to the squared Euclidean distance. For distributions with continuous densities, the Brier divergence is also known as Integrated Squared Error (ISE). This distance metric has been used for decades in studying convergence properties of nonparametric density estimates, such as Parzen window estimates \citep{Parzen62}. In more modern work it has also been applied as a direct optimisation criterion for fitting parametric density models \citep[see \eg][]{Scott99,Mukherjee99,Girolami03}.
\cbend

The value of information under the Brier score becomes the following straightforward quantity:
%
\begin{align}
	\information{Brier}{X}{Y} &= \expect{y \sim P_{Y}} \customnorm{P_X - P_{X\vert Y=y}}{2}^2.
\end{align}

\subsection{Spherical scoring rules}

Another example of strictly proper scoring rules, introduced in \citep{Good1971}, is the spherical scoring rule \citep{Dawid2007,Dawid2012}. The spherical score is defined as follows:
%
\begin{align}
	\score_{spherical}(x,P) &= 1 -\frac{P(x)}{\customnorm{P}{2}}.
\end{align}

This gives rise to the following entropy and divergence functionals:
%
\begin{align}
	\genentropy{spherical}{P} &= 1 -\expect{x \sim P}\frac{P(x)}{\customnorm{P}{2}}\\
		&= 1 -\customnorm{P}{2},\\
	\divergence{spherical}{P}{Q} &= -\expect{x \sim P}\frac{Q(x)}{\customnorm{Q}{2}} + \customnorm{P}{2}\\
		&= \customnorm{P}{2} - \frac{\scalar{Q}{P}}{\customnorm{Q}{2}}\\
		&= \customnorm{P}{2}\left( 1 - \cos(P,Q) \right),
\end{align}
%
where $\cos(P,Q) = \frac{\scalar{P}{Q}}{\customnorm{P}{2}\customnorm{Q}{2}}$ is the cosine similarity between distributions $P$ and $Q$.

An interesting property of the spherical score is that it is agnostic to scaling of $P$. That is $\score_{spherical}(x,c\cdot P) = \score_{spherical}(x,P) $. Similarly, $\divergence{spherical}{P}{c\cdot Q} = \divergence{spherical}{P}{Q}$ and $\divergence{spherical}{c\cdot P}{Q} = c \cdot \divergence{spherical}{P}{Q}$. This means that when approximating a fixed distribution $P$ by $Q$ via minimising $\divergence{spherical}{P}{Q}$ we only need to know $P$ and $Q$ up to a normalising constant.

The value of information under the spherical score is
%
\begin{align}
	\information{spherical}{X}{Y} &= \customnorm{P_X}{2}\expect{y \sim P_{Y}} \left( 1 - \cos(P_X,P_{X\vert Y=y}) \right).
\end{align}

\citet{Gneiting2007} and \citet{Jose2008} also introduce generalisations of the spherical score, where the $L_2$ norm is replaced by a general $L_\gamma$ norm:
%
\begin{align}
	\genentropy{\gamma,pseudospherical}{P} &= -\customnorm{P}{\gamma}.
\end{align}

\subsection{The kernel scoring rule\label{sec:kernel_score}}

The kernel scoring rule first appeared in the statistics literature in \citep{Eaton1996}, although the name \emph{kernel scoring rule} was only used in more recent references \citep{Dawid1994,Dawid2007,Gneiting2007}.

Independently, a related concept, derived from different first principles, has become known in the machine learning community as \emph{maximum mean discrepancy} (MMD, \citep{Sriperumbudur2008}). As we will see, MMD is closely related to the kernel scoring rule. MMD has been adopted in a variety of modern applications in machine learning and statistics, including two sample tests \citep{Gretton2012}, kernel moment matching \citep{Song2008}, embedding of probability distributions \citep{Smola2007} and the kernel-based message passing \citep{Fukumizu2010}.

MMD measures the divergence or distance between two distributions, $P$ and $Q$. It belongs to a rich class of divergences called integral probability metrics \citep{Sriperumbudur2009}, which define the distance between  $P$ and $Q$, with respect to a class of integrand functions $\mathcal{F}$ as follows:
%
\begin{align}
	\divergence{\Fe}{P}{Q} = \sup_{f\in\Fe}\left\vert \expect{x\sim P} f(x) - \expect{x\sim Q} f(x) \right\vert^2.
\end{align}
	
Intuitively, if two distributions are close in the integral probability metric sense, then no matter which function $f$ we choose from the function class $\mathcal{F}$, the difference between the expectation of $f$ under $P$ and $Q$ should be small. This class of divergences include the Wasserstein distance \citep{Barrio1999}, the Dudley metric \citep{Dudley1974} and MMD, which differ only in their choice of the function class $\Fe$.

A particularly interesting case is when the function class $\Fe$ is functions of unit norm from a reproducing kernel Hilbert space (RKHS) $\He$. In this case, the MMD between two distributions can be conveniently expressed using expectations of the associated kernel $k(x, x')$ only \citep{Sriperumbudur2008}:
%
\begin{align}
\mbox{MMD}^2\left(P,Q\right) &= \sup_{\substack{f\in\He\\\Hnorm{f}=1}}\left( \expect{x\sim P} f(x) - \expect{x\sim Q} f(x) \right)^2\label{eqn:rkhs-mmd}\\
	&=  \sup_{\substack{f\in\He\\\Hnorm{f}=1}}\left\vert \expect{x\sim P}\scalar{f}{k(\cdot,x)} - \expect{x\sim Q} \scalar{f}{k(\cdot,x)} \right\vert^2\label{eqn:MMD_reproducing_property}\\
	&=  \sup_{\substack{f\in\He\\\Hnorm{f}=1}}\left\vert \scalar{f}{\expect{x\sim P} k(\cdot,x) - \expect{x\sim Q} k(\cdot,x)}\right\vert^2\label{eqn:MMD_linearity_of_expectation}\\
	&=  \sup_{\substack{f\in\He\\\Hnorm{f}=1}}\scalar{f}{\mu_{P} - \mu_{Q}}^2\\
	&=  \Hnorm{\mu_{P} - \mu_{Q}}^2\label{eqn:MMD_Cauchy_Schwartz}\\
	&=  \expect{x,x'\sim P} k(x,x')	- 2 \expect{x\sim P}\expect{x'\sim Q} k(x,x') + \expect{x,x'\sim Q} k(x,x').\label{eqn:rkhs-mmd-lastline}
\end{align}

In the derivation above we exploited the reproducing property of the kernel to arrive at \eqref{eqn:MMD_reproducing_property} and the linearity of expectation to obtain \eqref{eqn:MMD_linearity_of_expectation}. Step \eqref{eqn:MMD_Cauchy_Schwartz} holds because of the Cauchy-Schwartz inequality. $\mu_P(\cdot) = \expect{x\sim P} k(\cdot,x)$ is called the mean element or RKHS-embedding of the probability distribution $P$ \citep{Smola2007}. The MMD metric is analogous to the Euclidean distance between the mean elements of the two distributions.

The most interesting kernels for the purposes of Hilbert-space embedding of distributions are those called \emph{characteristic} \citep{Sriperumbudur2008}. If the kernel $k$ is characteristic, the mapping from Borel probability measures to mean elements is injective, that is $\mu_P = \mu_Q \iff P = Q$. This also means that for characteristic Hilbert spaces $\divergence{k}{P}{Q} = 0 \iff Q=P$ holds. This is analogous to the strictly proper property of scoring rules and divergences as in Definition \ref{def:strictly_proper}.

The mean embedding $\mu_P$ can be thought of as a generalisation of characteristic functions \citep[see \eg][]{Ord1999}. The characteristic function of a probability distribution $P$ with density $p$ over the real line is defined as follows:
%
\begin{equation}
	\phi_p(t) = \E{x\sim p}{e^{ i t x}} = \int e^{i t x} p(x) dx,
\end{equation}
%
where $i$ is the imaginary number $i=\sqrt{-1}$.

The characteristic function is known to uniquely characterise any Borel probability measure on the real line. Indeed, it corresponds to an RKHS-embedding with the Fourier kernel $k_{Fourier}(x,y) = \exp(ixy)$, which is an example of characteristic kernels. Note, that the final formula \eqref{eqn:rkhs-mmd-lastline} assumed a real valued kernel function, therefore it is not valid for the special case of the Fourier kernel. Other, practically more relevant examples of characteristic kernels include the squared exponential, and the Laplacian kernels \citep[see \eg][]{Rasmussen2006}. As a counterexample, polynomial kernels, and in general kernels corresponding to finite dimensional Hilbert spaces are not characteristic \citep{Sriperumbudur2008}.

The maximum mean discrepancy with characteristic kernels has been applied in various contexts in machine learning. One of the first of these recent applications were two-sample tests. In two-sample testing one is provided i.\,i.\,d.\ samples from two distributions, and one has to determine whether the two distributions are the same or not. \citet{Gretton2012} developed and analysed efficient empirical methods based on the MMD for this problem.

Herding \citep{welling2009herding} and its generalisation kernel herding \citep{Chen2010} have been shown to minimise MMD between a target distribution and the empirical distribution of pseudo-samples. This method is an example of quasi-Monte Carlo methods that are examined in Chapter \ref{sec:approximate_inference}. Lastly, in kernel moment matching \citep{Song2008} MMD is used for density estimation: parameters of a parametric density model are set by minimising MMD from the empirical distribution of data.

The squared MMD in fact conforms to our definition of a generalised divergence in equation \eqref{eqn:def_divergence}, and corresponds to the following scoring rule:
%
\begin{align}
	\score_{k}(x,P) &\defeq k(x,x) - 2 \expect{x'\sim P} k(x,x') + \expect{x',x''\sim P}k(x',x'') \label{eqn:kernel_scoring_rule}\\
		&=  \sup_{\substack{f\in\He\\\Hnorm{f}=1}}\left( f(x)- \expect{x\sim P} f(x) \right)^2.
\end{align}

The equivalence can be seen by applying Definition \ref{def:generalised_divergence} of the generalised divergence:
%
\begin{align}
	\divergence{k}{P}{Q} &\defeq \expect{x\sim P} \score_{k}(x,Q) - \expect{x\sim P} \score_{k}(x,P)\\
		&= \expect{x\sim P} k(x,x) - 2 \expect{x\sim P}\expect{x'\sim Q} k(x,x') + \expect{x',x''\sim Q}k(x',x'')\\
		&\ \ \ \,- \expect{x\sim P} k(x,x) + 2 \expect{x,x'\sim P} k(x,x') - \expect{x',x''\sim P}k(x',x'') \\
		&= \expect{x',x''\sim P}k(x',x'') - 2 \expect{x\sim P}\expect{x'\sim Q} k(x,x') + \expect{x',x''\sim Q}k(x',x'')\\
		&= \mbox{MMD}^2\left(P,Q\right).
\end{align}

The scoring rule in Equation \eqref{eqn:kernel_scoring_rule} is equivalent to the \emph{kernel scoring rule} introduced originally by \citet{Eaton1982}. The term kernel score was later coined by \citet{Dawid2007}. Further references to this scoring rule can be found in \citep{Eaton1996,Gneiting2007}. The original definitions differed from the formula by a factor of two, and they did not have the leading $k(x,x)$ term. These differences do not make any practical effect: scoring rules that are equal up to scaling and an additive term that depends only on $x$ but not on the distribution $P$ give rise to exactly the same generalised entropy and divergence functionals, and are hence equivalent \citep{Dawid2007}. Also, the statistics community defined the scores in terms of negative definite kernels, rather than positive definite ones which is the common convention in machine learning.

It has also been pointed out that the Brier (quadratic) score is a special case of the kernel score when the kernel is chosen to be the trivial $k(x,x') = \delta(x - x')$, where $\delta$ is the Dirac delta function \citep{Dawid2007}.
\cbstart
Thus, methods that minimise the integrated squares error -- \ie the quadratic Brier divergence in Eqn.\ \eqref{eqn:Brier_divergence} -- can also be considered as special cases of kernel moment matching \citep{Scott99,Girolami03,Mukherjee99}.
\cbend

To my knowledge the connection between kernel scores in statistics and maximum mean discrepancy has not been established before. This interpretation allows one to uncover previously unknown connections between existing machine learning methods and to provide a solid theoretical framework for understanding and generalising them. For example, novel asymptotic analysis may established using generalisations of the Fisher information matrix or the sandwitch information matrix.

Depending on the choice of kernel, the kernel score can be strictly proper. \citep{Gneiting2007} provide a proof of the propriety of the kernel scoring rule with respect to all Borel probability measures, given that the expectation $\expect{x,x'\sim P} k(x,x')$ is finite. Using the theory developed to study properties of MMD and characteristic kernels we can also see that the scoring rule is strictly proper whenever the kernel is characteristic \citep{Sriperumbudur2008}. \citet{Gneiting2007} showed that many examples of scoring rules, among them the Brier score (see section \ref{sec:Brier_score}), can be interpreted as special cases of the kernel scoring rule.

The generalised entropy defined by this scoring rule becomes:
%
\begin{align}
	\genentropy{k}{P} &= \expect{x\sim P} \score_{k}(x,P) \\
		&= \expect{x\sim P} k(x,x) - 2 \expect{x,x'\sim P} k(x,x') + \expect{x',x''\sim P}k(x',x'')\\
		&= \expect{x\sim P} k(x,x) - \expect{x,x'\sim P} k(x,x').
\end{align}

The entropy function is concave for all positive definite kernels $k$ and strictly concave whenever the kernel is characteristic. Importantly, it has several favourable properties in comparison to Shannon's entropy.

Firstly, if we assume that the kernel $k$ is bounded, then the entropy functional is also bounded. If we further assume that the kernel satisfies $\forall x,y: k(x,x)\geq k(x,y)$, then the entropy is also non-negative. Thus, in most practical cases the entropy functional is bounded both from above and below. Irrespective of kernel choice, the entropy is exactly zero for delta distributions, that is when the distribution $P$ is concentrated on a single point. If the kernel satisfies the strict inequality $\forall x,y: k(x,x) > k(x,y)$, the entropy is strictly positive for all other probability distributions.

Secondly, the entropy can be computed for any distribution that one can compute expectations over. This means that any probability distribution, and indeed any Borel measure, has a well-defined entropy of this form. This is not true for the Shannon's differential entropy, where the entropy of atomic distributions or mixtures of atomic and continuous distributions is not defined. This property is useful in applications such as quasi-Monte Carlo as discussed in Chapter \ref{sec:approximate_inference}.

Thirdly, the entropy function has the kernel $k$ as a free parameter, which is mixed blessing. On one hand, this provides extra flexibility: even if we commit to a particular family of kernels, like the square exponential, we can fine-tune the entropy function and corresponding divergence to our needs by adjusting parameters, such as the length-scale parameter \citep{Song2008}. On the other hand there is no principled, general way of choosing the kernel or its parameters if we are unsure what it should be. 

We can use the generalised entropy and divergence defined by the kernel scoring rule to define the value of information a random variable provides about another one:
%
\begin{align}
	\information{k}{X}{Y} &= \expect{y\sim P_{Y}} \divergence{k}{P_{X}}{P_{X\vert Y=y}}\\
		&=  \expect{y\sim P_{Y}} \Hnorm{ \mu_{X \vert Y=y} - \mu_X }^2 \label{eqn:kernel_information}\\
		&= k(P_{X},P_{X}) - 2*\expect{y\sim P_{Y}}k(P_{X},P_{X\vert Y=y}) + \expect{y\sim P_{Y}} k(P_{X\vert Y=y},P_{X\vert Y=y})\\
		&= \expect{y\sim P_{Y}} \expect{P_{x_1,x_2 \sim P_{X\vert Y=y}}}k(x_1,x_2) - \expect{x_1,x_2\sim P_{X}}k(x_1,x_2),
\end{align}
%
where I used the shorthand notation $k(P,Q)=\expect{x\sim P,x'\sim Q}k(x,x')$.

To my knowledge, this kernel-based measure of information has not been defined or used in the machine learning or statistics literature before. It is interesting to contrast this to other kernel measures of dependence developed recently in statistics, which are largely based on the cross-covariance operator between Hilbert space embeddings of the two distributions.

\begin{definition}[kernel Cross-covariance operator]
	Let $X$ and $Y$ be two random variables with joint distribution $P\in\probmeasures{\Xe\times\Ye}$, and marginals $P_X$ and $P_Y$. Let $k_{\Xe}:\Xe\times\Xe\mapsto\Complex$ and $k_{\Ye}:\Ye\times\Ye\mapsto\Complex$ be positive definite kernels with associated reproducing kernel Hilbert spaces $\He_\Xe$ and $\He_\Ye$, respectively. Let us define the kernel cross-covariance operator $C_{XY}$ between $X$ and $Y$ so that for all $f\in\He_\Xe$ and $g\in\He_\Ye$
	%
	\begin{equation}
		\scalar{f}{C_{XY}g}_{\He_\Xe} = \expect{(x,y) \sim P} \left( f(x) - \expect{x'\sim P_X} f(x')\right) \left( g(y) - \expect{y' \sim P_Y} g(y')\right).
	\end{equation}
\end{definition}

Based on the cross-covariance operator, one can define various measures of dependence and information. Here I only define the simplest one, the constrained covariance, or COCO:

\begin{definition}[COCO, see \citep{Gretton2005COCO}]
	In the same notation as above let us define define the constrained covariance between $X$ and $Y$, $COCO_{XY}$, as
	%
	\begin{equation}
		COCO_{XY}=\sup_{\substack{f\in\He_\Xe, g\in\He_\Ye \\\customnorm{f}{\He_\Xe}=1,\customnorm{g}{\He_\Ye}=1}} \cov{(x,y)\sim P}{f(x)}{g(y)}.
	\end{equation}
	%
	It can be shown that, $COCO$ is the matrix norm of the cross-covariance operator:
	%
	\begin{equation}
		COCO_{XY} = \customnorm{C_{XY}}{2},
	\end{equation}
	%
	where $\customnorm{\cdot}{2}$ denotes the spectral norm, that is the modulus of largest singular value \citep{Gretton2005COCO}.
\end{definition}

A more robust measure of dependence, the Hilbert Schmidt Information Criterion (HSIC) uses the Hilbert-Schmidt norm of the cross-covariance operator\citep{Gretton2005HSIC}. Kernel measures of dependence like COCO and HSIC have several useful properties. They are symmetric, and can be effectively estimated from empirical data \citep{Gretton2005HSIC}.

However, as with generalisations of Shannon's information in Eqn.\ \eqref{eqn:mutualinfo_generalisations}, COCO and its variants do not have an interpretation as ``the extent to which knowing $Y$ is useful for predicting $X$''. Also, they require a kernel to be defined over both $\Xe$ and $\Ye$, and properties of the functional depend on both choices of kernels. In contrast \eqref{eqn:kernel_information} only requires a single kernel over $\Xe$.

Interestingly, the kernel value of information $\information{k}{X}{Y}$ that I introduced based on the kernel score can also be interpreted in terms of a linear operator in the Hilbert space. I am not aware of any previous use of this operator before, and in referencing it I will use the name diversity operator.

\begin{definition}[Diversity operator]
Given two random variables $X$ and $Y$ with joint distribution $P$, and a positive definite kernel $k:\Xe\times\Xe\mapsto\Complex$ with associated Hilbert space $\He$, let us define the \emph{diversity operator} of Y over X, $D_{X\vert Y}:\He\mapsto\He$ such that for all $f,g\in\He$
%
\begin{align}
	\scalar{f}{D_{X\vert Y}g}_{\He} = \cov{y\sim P_Y}{\expect{X\vert Y=y} f}{\expect{X\vert Y=y} g}.
\end{align}
%
Consequently for all $f\in\He$
%
\begin{align}
	\scalar{f}{D_{X\vert Y}f}_{\He} = \var{y\sim P_Y} \left[ \expect{x\sim P_{X\vert Y=y}} f(x) \right].
\end{align}
\end{definition}

Equivalently, the operator can be defined in terms of mean elements or Hilbert-space embedding of the conditional and marginal distributions as follows:

\begin{statement}[Alternative definition of $D_{X\vert Y}$]
$D_{X\vert Y}$ admits the following equivalent definition
%
\begin{align}
	D_{X\vert Y} = \expect{y\sim P_Y} \left(\mu_{X\vert Y=y} - \mu_{X}\right) \otimes \left(\mu_{X\vert Y=y}  - \mu_{X} \right),
\end{align}
%
where $\otimes$ denotes tensor product, such that $\forall f,g,h\in\He: (f\otimes g)h = \scalar{g}{h}f$.

\begin{proof}
Let $f,g\in\He$, then
%
\begin{align}
	&\scalar{f}{ \left(\expect{y\sim P_Y}\left(\mu_{X\vert Y=y} - \mu_{X}\right) \otimes \left(\mu_{X\vert Y=y}  - \mu_{X} \right)\right) g}\\
	=&\expect{y\sim P_Y}\scalar{f}{ \left(\left(\mu_{X\vert Y=y} - \mu_{X}\right) \otimes \left(\mu_{X\vert Y=y}  - \mu_{X} \right)\right) g}\\
	=& \scalar{f}{\left(\mu_{X\vert Y=y} - \mu_{X}\right)}\scalar{g}{\left(\mu_{X\vert Y=y} - \mu_{X}\right)}\\
	=& \scalar{f}{\left(\expect{x\sim P_{X\vert Y=y}}k(\cdot,x) - \expect{x\sim P_X}k(\cdot,x)\right)}\scalar{g}{\left(\expect{x\sim P_{X\vert Y=y}}k(\cdot,x) - \expect{x\sim P_X}k(\cdot,x)\right)}\\
	=& \expect{y\sim P_Y} \left(\expect{X\vert Y=y} f(x) - \expect{x\sim P_X}f(x)\right)\left(\expect{X\vert Y=y} g(x) - \expect{x\sim P_X}g(x)\right)\label{eqn:diversity_operator_linearity_of_expectation}\\
	=& \cov{y\sim P_Y}{\expect{X\vert Y=y} f}{\expect{X\vert Y=y} g}\\
	=& \scalar{f}{D_{X\vert Y}g}_{\He},
\end{align}
%
where we used the linearity of expectation and the reproducing property of the kernel to obtain step \eqref{eqn:diversity_operator_linearity_of_expectation}.
\end{proof}
\end{statement}

Using this alternative definition it is easy to see that the kernel value of information as defined in Eqn.\ \eqref{eqn:kernel_information} can be expressed as the trace of the diversity operator (which in turn is the same as the Hilbert-Schmidt norm of the square-root of the operator):
%
\begin{align}
	\information{k}{X}{Y} &= \expect{y \sim P_{Y}} \customnorm{\mu_X - \mu_{X\vert Y=y}}{2}^2\\
		&= \expect{y \sim P_{Y}} \trace{\scalar{\mu_X - \mu_{X\vert Y=y}}{\mu_X - \mu_{X\vert Y=y}}}\\
		&= \expect{y \sim P_{Y}} \trace{ ( \mu_X - \mu_{X\vert Y=y}) \otimes (\mu_X - \mu_{X\vert Y=y})}\\
		&= \trace{ \expect{y \sim P_{Y}} ( \mu_X - \mu_{X\vert Y=y}) \otimes (\mu_X - \mu_{X\vert Y=y})} \\
		&= \trace{I_{X\vert Y}}\\
		&= \customnorm{I_{X\vert Y}^{\nicefrac{1}{2}}}{HS}.
\end{align}

It would be interesting future direction to investigate whether this information criterion has any connections to COCO and HSIC, or indeed if it inherits any of their useful properties.

\subsection{The spherical kernel score\label{sec:spherical_kernel_score}}

Seeing how the Brier score is a special case of the kernel scoring rule, one might wonder whether the spherical scoring rule has a similar generalisation. It turns out it does, and it gives rise to a very intuitive divergence. Consider the following scoring rule
%
\begin{align}
	\score_{k,spherical}(x,P) &\defeq \Hnorm{\mu_{\delta_x}} - \frac{\mu_P(x)}{\Hnorm{\mu_P}} \label{eqn:spherical_kernel_score}\\
			& = \Hnorm{\mu_{\delta_x}}\left(1 - \cos(\mu_{\delta_x},\mu_{P})\right)\\
		& =\sqrt{k(x,x)} - \frac{\expect{x'\sim P}k(x,x')}{\sqrt{\expect{x,x'\sim P}k(x,x')}}.
\end{align}

The scoring rule gives rise to the following entropy functional:
%
\begin{align}
	\genentropy{k,spherical}{P} & = \expect{x\sim P}\Hnorm{\mu_{\delta_x}} - \Hnorm{\mu_P}\\
		& = \expect{x\sim P}\sqrt{k(x,x)} - \sqrt{\expect{x,x'\sim P}k(x,x')}.
\end{align}
%
Whenever $k(x,x)=c$ this entropy is non-negative, and bounded from above. For characteristic kernels it is only zero for delta distributions.

The scoring rule leads to the following divergence:
%
\begin{align}
	\divergence{k,spherical}{P}{Q} &= - \expect{x\sim Q}\frac{\mu_P}{\Hnorm{\mu_P}} + \Hnorm{\mu_P}\\
		&= \Hnorm{\mu_P} \left( 1 - \cos(\mu_P, \mu_Q)\right)\\
		&= \sqrt{\expect{x,x'\sim P}k(x,x')} - \frac{\expect{x\sim P}\expect{x'\sim Q}k(x,x')}{\sqrt{\expect{x,x'\sim Q}k(x,x')}}. \label{eqn:spherical_kernel_divergence}
\end{align}

Unlike MMD and $\divergence{k}{\cdot}{\cdot}$, this divergence is asymmetric because of the leading $\Hnorm{\mu_P}$ factor. Also, just like the spherical score, it is agnostic to scaling of $Q$, that is
%
\begin{equation}
	\divergence{k,spherical}{P}{c\cdot Q} = \divergence{k,spherical}{P}{Q}.\label{eqn:spherical_kernel_homogeneity_Q}
\end{equation}
%
Furthermore,
%
\begin{equation}
	\divergence{k,spherical}{c\cdot P}{Q} = c \cdot \divergence{k,spherical}{P}{Q}.\label{eqn:spherical_kernel_homogeneity_P}
\end{equation}
%
This multiplicative scaling behaviour is in general known as \emph{homogeneity}. Whenever the kernel is characteristic, this scoring rule is strictly proper with respect to Borel probability distributions, whose mean embedding $\mu_P(x)$ is bounded.

\begin{theorem}[The spherical kernel score is strictly proper]
Let $k:\Xe\times\Xe\mapsto\Complex$ be a characteristic positive definite kernel. The spherical kernel scoring rule $\score_{k,spherical}$ as defined in Eqn.\ \ref{eqn:spherical_kernel_score} is a strictly proper scoring rule with respect to Borel probability distributions.

\begin{proof}
	Suppose $P\neq Q$, then by the strict propriety of the kernel score
	%
	\begin{align}
		0 &< \divergence{k}{P}{Q}\\
		0 &< \Hnorm{\mu_P}^2 + \Hnorm{\mu_Q}^2 - 2\scalar{\mu_P}{\mu_Q} _{\He}\\
		\scalar{\mu_P}{\mu_Q} &< \frac{1}{2}\left(\Hnorm{\mu_P}^2 + \Hnorm{\mu_Q}^2\right) \leq \Hnorm{\mu_P}\Hnorm{\mu_Q}\\
		\cos(\mu_P,\mu_Q) = \frac{\scalar{\mu_P}{\mu_Q} _{\He}}{\Hnorm{\mu_P}\Hnorm{\mu_Q}} &< 1.
	\end{align}
	Thus,
	\begin{align}
		\divergence{k,spherical}{P}{Q} = \Hnorm{\mu_P} \left( 1 - \cos(\mu_P, \mu_Q)\right) > 0.
	\end{align}
\end{proof}
\end{theorem}

Just as it is the case with the Brier score and the kernel scoring rule, the spherical kernel rule reduces to the spherical score whenever the trivial kernel $k(x,x') = \delta_{x}(x')$ is used.

The spherical kernel score also has an interesting intuitive meaning in terms of test functions Gaussian processes

\begin{proposition}\label{prop:sign_random_function}
Let $P,Q$ be probability distributions over the domain $\Xe$ and $\He$ a RKHS with associated kernel function $k$. Let $GP$ denote a standard Gaussian process in the Hilbert space. Then the following equality holds:
%
\begin{equation}
	 \divergence{k,spherical}{P}{Q} = \Hnorm{\mu_P} \mathbb{P}_{f\sim GP} \left[ \mbox{sign}(\expect{x\sim P} f(x)) \neq \mbox{sign}(\expect{x\sim Q} f(x)) \right]. \label{eqn:sign_random_function}
\end{equation}
\begin{proof}
See Lemma 8 in \citep{Goemans1995}.
\end{proof}
\end{proposition}

Thus, the divergence function \eqref{eqn:spherical_kernel_divergence} is related to the probability that the expectation of a randomly drawn test function $f$ has the same sign when the expectation is taken under $P$ or under $Q$. Intuitively, the more smooth functions one can find whose expectation under $P$ is positive but under $Q$ is negative, the more different $P$ and $Q$ are. The finite dimensional analogue of Proposition \ref{prop:sign_random_function} is exploited in sign-random-projection locality sensitive hashing (SRP-LSH) algorithms \citep{Charikar2002,Ji2012}. Similarly, Eqn.\ \eqref{eqn:sign_random_function} can be exploited in locality sensitive hashing algorithms for probability distributions, even though practical applications of such algorithms are probably limited.

I am not aware of any previous definition or mention of the spherical kernel scoring rule or the associated divergence functional in either the statistics or machine learning literature. It is unclear whether this intuitive divergence function provides any advantages over, say MMD, in practical applications, or whether efficient empirical estimators exist. Exploring practical applications of spherical kernel scores remains an interesting area for future research.

% ##        #######   ######   ######  
% ##       ##     ## ##    ## ##    ## 
% ##       ##     ## ##       ##       
% ##       ##     ##  ######   ######  
% ##       ##     ##       ##       ## 
% ##       ##     ## ##    ## ##    ## 
% ########  #######   ######   ######  

\subsection{Scoring rules and Bayesian decision problems \label{sec:loss_scoring_rule}}

The scoring rule framework is very flexible, in fact for every Bayesian decision problem it is possible to derive a corresponding scoring rule as we will show in this section.

Let us assume we are faced with a decision problem of the following form: We have to decide to take one of several possible actions $\action\in\actionset$. The loss/utility of our action will depend on the state of the environment $\X$, the value of which is unknown to us. If the environment is in state $\X=\x$, and we choose action $\action$, we incur a loss $\loss(\x,\action)$.
Let us assume we have a probabilistic forecast or belief $P$ about the state of the environment $\X$. This belief is usually formed by probabilistic inference. Given our forecast $P$ we can choose an action that minimises the expected loss:
%
\begin{align}
	\action^{*}_{P} = \argmin_{\action\in\actionset} \expect{\x \sim P} \loss(\x,\action).
\end{align}

When we observe the value of $X$ we can score the probabilistic forecast, by evaluating the loss incurred by using this optimal action $\action^{*}_{P}$ in state $\X=\x$:
%
\begin{align}
	\score_{\loss}(\x,P) = \loss(\x,\action^{*}_{P}).\label{eqn:loss_scoring_rule}
\end{align}
%
This function only depends on the true state $\x$ and the forecast $P$, hence it is a scoring rule. The generalised entropy that this scoring rule defines is otherwise known as the Bayes-risk of the decision problem:
%
\begin{align}
	\genentropy{\loss}{P} \defeq &\expect{x\sim P} \loss(\x,\action^{*}_{P})\\
		= &\min_{\action\in\actionset} \expect{x\sim P} \loss(\x,\action)\\
		= &\mathcal{R}_{\loss}(P).
\end{align}

The associated divergence can be interpreted as the excess loss we incur by using the subomptimal action $\action^{*}_{Q}$ computed on the basis of $Q$, when in fact the true distribution of $X$ is $P$:
%
\begin{align}
	\divergence{\loss}{P}{Q} = \expect{x\sim P} \loss(\x,\action^{*}_{Q}) - \min_{\action \in \actionset}\expect{x\sim P} \loss(\x,\action).
\end{align}

Because of the definition of $\genentropy{\loss}{P}$, the divergence is always non-negative, hence the scoring rule defined this way is always proper. In fact, proper scoring rules and Bayesian decision problems are equivalent, inasmuch as every proper scoring rule can be expressed as Bayesian decision problem as in Eqn.\ \eqref{eqn:loss_scoring_rule}. Throughout this thesis I will use the decision theoretic notation ($\loss$, $\actionset$, $\mathcal{R}_{\loss}[\cdot]$) or the scoring rule notation ($\score$, $\genentropy{\score}{\cdot}$, $\divergence{S}{\cdot}{\cdot}$) interchangeably, depending on which one is more natural given the context.

Several scoring rules can be interpreted as special cases of this loss-calibrated framework.

\subsubsection{Logarithmic score and Shannon entropy}

Shannon's entropy has an intuitive operational meaning as minimum description length. We are given a random variable $X$ with distribution $P$ over a finite, discrete dictionary $\Xe$. We would like to encode symbols in $\Xe$ by binary sequences, in such a way, that any sequence composed by concatenating code-words is uniquely decodable. It can be shown that the expected code-length of any uniquely decodable code $f:\Xe\mapsto\{0,1\}^{*}$ under the distribution $P$ is lower bounded by the Shannon entropy of $P$:

\begin{equation}
	\expect{x \sim P} \vert f(x) \vert \geq \frac{1}{\log(2)}\genentropy{Shannon}{P},
\end{equation}
%
where the $\nicefrac{1}{\log(2)}$ is not needed if use base-2 logarithm in the definition of $\genentropy{Shannon}{P}$.
 
Let us consider the following decision problem: Let $\actionset$ be the set of all uniquely decodable binary codes, so that $a:\Xe\mapsto\{0,1\}^{*}$ maps $X$ to a binary code-word of variable length. Let the loss $\loss$ be the length of the code-word assigned to $X$: $\loss(x,a) = \vert a(x)\vert$.

The scoring rule defined by this decision problem is approximately the same as the logarithmic score, and it becomes more exact as the dictionary size increases.

\subsubsection{Kernel scoring rule}

Assume our task is to estimate value of a set of known functions $f\in{\Fe}$ all at the same random point $\X$. The action can be interpreted as a functional $a:\Fe\mapsto\Reals$, that gives an estimated value of $f(\X)$ for any function $f\in\Fe$. We are required to do equally well on all functions, and the loss $\loss$ we incur is equal to the largest squared error we incur on any of these functions:
%
\begin{equation}
	\loss(\x, \action) = \sup_{f\in\Fe}\left( f(\x) - \action(f)\right)^2.
\end{equation}

Given a probabilistic forecast $P$ over $\X$, the Bayes optimal decision $\action^{*}_{P}(f)$ is to compute the mean of $f$ under the forecast distribution $P$:
%
\begin{equation}
	\action^{*}_{P}(f) = \expect{x\sim P} f(x).
\end{equation}

Thus, we can define the following scoring rule $\score$:
%
\begin{equation}
	\score(\x,P) = \loss(x,\action^{*}_{P} )
		= 	\sup_{f\in\Fe}\left( f(\x) - \expect{x\sim P} f(x) \right)^2.
\end{equation}

When $\Fe$ is chosen to be the unit ball in a reproducing kernel Hilbert space $\He$ defined by a positive definite kernel $k$, this scoring rule will be equivalent to the kernel scoring rule for probability distributions (Eqn.\ \eqref{eqn:kernel_scoring_rule}).

As the Brier score is a special case of the kernel scoring rule, it can also be derived from the same decision problem.

\section{Summary}

In this chapter I introduced the framework of scoring rules and strictly proper scoring rules. The framework allows us to define meaningful generalisations of entropy, divergence and the value of information, which are useful in a variety of tasks such as approximate inference and experiment design. I have also shown how the framework of proper scoring rules and Bayesian decision theory are intimately connected.

In addition to the classic examples -- logarithmic, Brier, spherical scores -- I reviewed information quantities that one can define based on reproducing kernel Hilbert spaces: the kernel scoring rule and maximum mean discrepancy. These rich classes of scoring rules have been used by the machine learning community, where they were derived from different first principles without noting the connection to Bregman divergences.

\cbstart
Establishing connections between MMD and strictly proper scoring rules is an important contribution of this chapter. It openned the door to discovering novel concepts such as the kernel value of information or the spherical kernel score. Both of these quantities have appealing properties. The kernel value of information shows similarities to existing kernel measures of information, but has solid theoretical foundation and interpretation in the scoring rule framework. The spherical kernel scoring rule posesses interesting multiplicative scaling properties (Eqns.\ \eqref{eqn:spherical_kernel_homogeneity_Q} and \eqref{eqn:spherical_kernel_homogeneity_P}), which may provide useful to avoid computing intractable marginalisation constants. Exploring practical applications of these concepts is an exciting area for future research.

Complementing the systematic review presented in this chapter, Appendix \ref{sec:information_geometry} presents a visual way to understand the differences between scoring rules reviewed here. There, I present a numerical method based on ISOMAP \citep{Tenenbaum2000} to create approximately correct visualisations of the statistical manifold structures that various scoring rules give rise to.

This chapter has laid the foundation for the rest of this thesis which focuses on applying scoring rule-based concepts to Bayesian analysis.

Scoring rules and associated divergence functionals have long been used in statistical estimation, but their application has largely focussed on score matching. In score matching one seeks to find efficient or computationally appealing estimators to parameters of a statistical model based on \iid samples. The thesis explores their application to two distinct problems in Bayesian statistics, where diveregence functionals play an important role: approximate inference and active learning.

Part \ref{part:2} is concerned with approximate Bayeisian inference, where one seeks to substitute an intractable posterior distribution with a computationally more convenient one. When doing so, one needs to quantify the errors introduced by such approximation. In the approximate inference literature, the KL divergence emerged as the standard objective function to optimise, but as we will see, it is not the only meaningful choice.

In section \ref{sec:approximate_inference} I introduce a theoretical framework for approximate inference motivated by Bayesian decision theory. As it turns out the framework calls for using general Bregman divergences introduced in section \ref{sec:loss_scoring_rule} as a measure of discrepancy between the target and the approximate posterior. In Chapter \ref{sec:herding} I focus on two approximate inference methods, kernel herding and Bayesian quadrature, that both employ the kernel scoring rule-based maximum mean discrepancy.

The focus Part \ref{part:3} is Bayesian active learning, also known as sequential experiment design. In active learning the goal is to select measurements so that the observed outcomes are maximally informative about an unknown quantity one tries to estimate. Quantifying information is central to any active learning algorithm, and in Chapter \ref{sec:active_learning_framework} I argue why information functionals based on proper scoring rules are a particularly appealing choice.

In the last chapters I focus on an active learning algorithm that employs Shannon's information. I chose to concentrate on this specific choice because -- although well motivated -- generalised information quantities are computationally infeasible to work with. I develop an efficient active learning method exploiting the unique symmetry property of Shannon's information, which I call Bayesian Active Learning by Disagreement (BALD).
\cbend