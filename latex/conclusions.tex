%!TEX root = thesis.tex

Machine learning is a mess. This thesis finally brings order and clarity to what otherwise appears as a mix of muddled thoughts. Just kidding.

Machine learning is fragmented. Researchers in the field publish and endless stream of ideas, methods, optimisation problems and probabilistic models. Substantial progress is made when, from this system of thoughts, unifying frameworks and generalisations emerge. Unifying views help researchers establish connections between concepts and techniques they previously thought were unrelated, and develop new ones in a more systematic fashion.

A good example of this unification process is the emergence of probabilistic generative models. By the early 90's, several unsupervised learning algorithms existed. Principal components analysis, Gaussian mixture models, slow feature analysis, Kalman-filters, autoencoders, independent component analysis, Boltzmann machines, factor analysis, hidden Markov models and others co-existed for years or decades, but each of these methods were by large studied in isolation. The relationship between these methods became clear when a unifying framework, based on probabilistic generative models emerged \citep{Lauritzen1996,Roweis1999,Tipping1999,Turner2007}.

Over the past decades, graphical models \citep{Lauritzen1996} have become the standard language for describing complicated, hierarchical probabilistic models. Each new probabilistic model is explained in terms of conditional independence statements represented graphically. The movement introduced the useful distinction between models and algorithms: inference methods are usually developed for general classes of graphical models, rather than in the context of a particular model. The relationship between models is now clear, and it became relatively straightforward to adopt probabilistic models to new problems.

I hope this thesis contributes to the unification of machine learning by presenting a unifying framework for Bayesian machine learning problems based on scoring rules and information geometry. The unifying view allows one to uncover relationships between seemingly unrelated methods. There are four main connections presented in this thesis.

\paragraph{Score matching, approximate inference and active learning} Information geometry provides a common basis for score matching, approximate Bayesian inference and Bayesian active learning. Scoring-rule-based divergences play a central role in all three of these problems. In score matching and approximate inference one tries to minimise divergences, in Bayesian active learning the goal is to maximise them.

\paragraph{Maximum mean discrepancy and kernel scoring rule} A minor contribution in chapter \ref{sec:scoring_rules} is the connection between maximum mean discrepancy \citep{Gretton2012} and the kernel scoring rule \citep{Jose2008}. These related concepts have been developed studied by two distinct communities, derived from different first principles. Establishing this connection allowed me to define a new concepts, such as the \emph{kernel spherical divergence} in Eqn.\ \eqref{Eqn:spherical_kernel_divergence} and the \emph{kernel value of information} in Eqn.\ \eqref{Eqn:kernel_information}.

\paragraph{Kernel herding, Bayesian quadrature and loss-calibrated quasi-Monte Carlo} Herding was originally introduced by \citet{welling2009herding} as a heuristic procedure obtained by ``taking the zero temperature limit of the corresponding maximum likelihood problem''. Although the paper studied several properties of this method, it was unclear whether and how herding is related to existing methods in machine learning. In \citep{Huszar2012herding} we showed that herding is closely related to Bayesian quadrature. In chapter \ref{sec:approximate_inference} I also show that it is a special case of the general procedure I call loss-calibrated quasi-Monte Carlo.

\paragraph{Bayesian optimisation, Bayesian quadrature and active learning} In chaper \ref{sec:active_learning_framework} I introduce a unifying framework for Bayesian active learning. This unifying framework naturally accommodates Bayesian optimisation, Bayesian quadrature, information theoretic active learning and transductive learning. Scoring rules provide a unifying language for these models inasmuch as they can all be described via the scoring rule they use to quantify the usefulness of the Bayesian posterior.

In addition to theoretical contributions, the thesis also presents applications of state-of-the-art information theoretic active learning to binary classification, preference elicitation and quantum tomography.