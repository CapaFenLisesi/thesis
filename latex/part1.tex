\part{Kernel methods and nonparametric inference}
\subsection{Kernel trick introduction}
General introduction with properties, rationale, etc.
\subsubsection{Examples of kernels}
Here, I'll put down the preference learning kernel, and the rotation-invariant squared exponential kernel I have suggested to Gabor Csanyi's work.
\subsection{Conditions for ``kernelisability''}
Orthonormal invariance + strong kernel property: For a family of algorithms, consistent with each other in terms of unitary embeddings. This is a constructive condition, gives an algorithm you can use right away.
\subsection{The kernel trick for Bayesian nonparametrics}
General idea of using kernel trick to define tractable non-parametric Bayesian models.
\subsubsection{Overview of alternative construction tools in BNP}
Direct construction of nonparametric priors, Peter's projective limit, taking the infinite limit of predictive distributions.
\subsubsection{Supervised learning: Gaussian processes, student-t processes}
Derivation of the student-t process, inverse-Wishart process prior using the kernel trick
\subsubsection{Unsupervised learning}
Put the stuff about the density estimation model here. Why it seems to be impossible?\\
