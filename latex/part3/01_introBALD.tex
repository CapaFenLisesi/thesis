%!TEX root = ../main.tex

\paragraph{Summary of contributions} The results presented in this chapter are joint work with Neil M.\ T.\ Houlsby, M\'{a}t\'{e} Lengyel and Zoubin Ghahramani, and are published as an online technical report \citep{Houlsby2011} and as part of \citep{Houlsby2012preference}. NMTH, ML and FH contributed equally to the development of the BALD framework. Unifying active learning, Bayesian optimisation and Bayesian quadrature in the scoring rules framework is original contribution by FH.

\section{Introduction}

In most machine learning applications, a learner passively observes data with which it can make inferences about its environment. It is generally true that as more data becomes available the inferences become more accurate. However, not each and every datapoint is equally useful. Some datapoints will be critically informative, while many more will become redundant given the context and information already learnt from other examples.

It is intuitive to think that, by actively seeking out measurements to be used in inference, the learner can significantly improve the quality of inference using smaller quantities of data. Amongst machine learning researchers this process of choosing which measurements to take is known as active learning; the same problem is called optimal experimental design in the statistics literature. Although active learning has been studied for several decades \citep{lindley1956,jaynes1986}, it is still an active area of research and no general solution exists.

The active learning paradigm is as pertinent now as it has ever been. With the advent and rapid expansion of the Internet, very large amounts of unlabelled data have become available; however, it is relatively costly to obtain labels. Experimental scientists work with ever growing volumes of data, carrying out experiments or labeling datapoints is a time-consuming and costly process for them. Carefully pre-selecting only the most informative experiments can result in substantial improvements in terms of faster processing or reduced costs. Searching for the most useful data in vast spaces of measurements calls for powerful active learning algorithms.

In this chapter I explain how scoring-rule based information quantities described in Chapter \ref{sec:scoring_rules} can be used to formalise the problem of active learning and experiment design. I devise a framework that is flexible enough to accomodate and unify a wide range of existing techniques. Examples include Shannon-information-based active learning \citep{Krause2006,MacKay1992,Houlsby2011} decision theoretic active learning \citep{Kapoor2007,Zhu2003active}, Bayesian optimisation \citep{Hennig2012entropy,Hennig2012newton} and Bayesian quadrature \citep{BZHermiteQuadrature,BZMonteCarlo}.

In the second half of this chapter I focus on a special case of Bayesian active learning that attempts to maximise Shannon's information. I derive a computationally convenient method, called Bayesian Active Learning by Disagreement (BALD) and present multiple applications to binary classification, multi-user preference learning and quantum physics.

% ######## ########     ###    ##     ## ######## ##      ##  #######  ########  ##    ## 
% ##       ##     ##   ## ##   ###   ### ##       ##  ##  ## ##     ## ##     ## ##   ##  
% ##       ##     ##  ##   ##  #### #### ##       ##  ##  ## ##     ## ##     ## ##  ##   
% ######   ########  ##     ## ## ### ## ######   ##  ##  ## ##     ## ########  #####    
% ##       ##   ##   ######### ##     ## ##       ##  ##  ## ##     ## ##   ##   ##  ##   
% ##       ##    ##  ##     ## ##     ## ##       ##  ##  ## ##     ## ##    ##  ##   ##  
% ##       ##     ## ##     ## ##     ## ########  ###  ###   #######  ##     ## ##    ## 
                                                      
\section{A general framework for Bayesian experiment design}

In active learning the goal is to learn about dependence of a \emph{target variable} $\y\in\mathcal{Y}$ on the \emph{input variable} $\x\in\mathcal{X}$ by interactively querying the system with inputs $\x_i\in\mathcal{X}$ and observing the system's response $\y_i$. Ultimately, having observed data $\data = \{(\x_i,\y_i)\}$, our goal is to choose queries such that the observed outcomes provide us with the most information about relevant properties of the system. There are many approaches to active learning that carry out predictions and quantify the value of information in different ways. Here we take a Bayesian discriminative approach, that assumes the existence of some latent parameters $\param$, that directly control the dependence between inputs and outputs, $p(\y\vert\x,\param)$.

Our goal is to infer the value of $\param$ from the observed data $\data = \{(\x_i,\y_i)\}$, which is possible via Bayes' rule
\begin{equation}
p(\param\vert\data) = \frac{p(\data\vert\param)p(\param)}{\int p(\data\vert\param)p(\param) d\param}
\end{equation}
In this chapter I will assume that inference is possible without approximations, and the posterior distribution $p(\param\vert\data)$ is available in closed form. In practice this is rarely the case, and in subsequent chapters I will apply the framework to cases where only approximations to the posterior are available.

A core problem in active learning is to describe how informative data is. In the Bayesian inference framework the posterior $p_\data(\param) := p(\param\vert\data)$ captures and summarises everything there is to know about the parameter $\param$ based on the data $\data$. It therefore makes sense to assess the quality of data $\data$ in terms of the quality of the posterior $p_\data$. Informative data should allow one to construct an accurate prediction $p_\data$ of the parameters $\theta$. Redundant data our estimate $p_\data$ is going to be poor.

The posterior $p_\data$ is a probabilistic estimate, hence it's accuracy can be quantified using a scoring rule $\score(\param,p_\data)$. The goal of the active learner should be to gather data $\data$ so that $p_\data$ minimises this quantity. However, during the process of active learning, the actual parameters $\param$ or indeed the score $\score(\param,p_\data)$ are never explicitly revealed to the learner, otherwise there was no point in learning. The best strategy the learner can follow is to collect data so that their best estimate of this score is minimised. The Bayes estimator to the score $\score(\param,p_\data)$ is the expected posterior score or generalised entropy of the posterior $p_\data$.

\begin{equation}
	\genentropy{\score}{p_\data} = \expect{\param\sim p_\data} \score(\param,p_\data)
\end{equation}

Hence, in the scoring rule framework, the active learner's goal should be to minimise the generalised entropy of the posterior. Recall from section \ref{sec:decision_theory_scoring} that generalised entropy is intimately related to Bayes Risk, so in that context the goal of the learner is to collect data so as to reduce the risk of decisions they have to make in the future.

Having defined a quantitative measure of the information contained in data, I now turn to discussing how this criterion can be exploited to proactively select measurement $x$ to speed up the process of learning. In this thesis I only consider myopic strategies to active learning, whereby the learner optimises the immediate value of information that the next observation provides, as if the next one was the last measurement to perform. This shortsighted strategy is known to lead to suboptimal performance if the learner is allowed to query multiple measurements in a sequence. However, the optimisation problem for non-myopic strategies get quickly intractable because of the combinatorial nature of the problem \citep{Krause2007}.

In myopic active learning, the learner solves the same problem in each step. Having observed some data $\data$, which is the next best measurement $\x$ I should make to minimise the entropy of the posterior. This optimisation problem can be formalised as follows:

\begin{equation}
	\argmax_\x\ \left[ \genentropy{\score}{p(\param\vert\data)} - \expect{y\sim p(\y\vert \x, \data)}{\genentropy{\score}{p(\param\vert \x,\y,\data)} }\right ]
	\label{eqn:active_learning_optimisation_criterion}
\end{equation}

The first term is the entropy of the posterior the learner currently has. This is a function of data $\data$ that is already observed, hence it is constant and could be neglected. The second term is the entropy of the posterior after making measurement $\x$ and observing outcome $\y$. Because at the point of choosing the next measurement the outcome is unknown, the learner has to rely on an approximation again and take an expectation with respect to its current predictive model of the outcome $p(\y \vert \x, \data)$. Note that here, the latent parameter $\param$ has been integrated out.

Recall Definition \ref{def:conditional_value_of_information} of the generalised conditional value of information. Using that definition we can also say that the learner's goal is to maximise the conditional value of intormation of the measurement $\x$ with respect to $\param$ given $\data$.

\begin{equation}
	\argmax_\x\ \conditionalinformation{\score}{\param}{\x}{\data}
\end{equation}

Yet another equivalent formulation of the criterion expresses the learner's goal as maximising the expected divergence between the current and new posterior after observing the outcome of the next measurement:

\begin{equation}
	\argmax_\x\  \expect{y \sim p(\y\vert \x, \data)}\divergence{S}{P_{\data}}{P_{\data\cup\{\x,\y\}}}
\end{equation}

The equivalence between these three criteria has been noted in the context of the Shannon's information in \citep{MacKay1992}. The framework presented here is a generalisation of \citeauthor{MacKay1992}'s work, and allows one to specify the goal of Bayesian active learning as a scoring rule for the posterior distribution. In the following section I will review a wide range of Bayesian methods in the literature that all fall within the scope of this approach.

% ######## ##     ##    ###    ##     ## ########  ##       ########  ######  
% ##        ##   ##    ## ##   ###   ### ##     ## ##       ##       ##    ## 
% ##         ## ##    ##   ##  #### #### ##     ## ##       ##       ##       
% ######      ###    ##     ## ## ### ## ########  ##       ######    ######  
% ##         ## ##   ######### ##     ## ##        ##       ##             ## 
% ##        ##   ##  ##     ## ##     ## ##        ##       ##       ##    ## 
% ######## ##     ## ##     ## ##     ## ##        ######## ########  ######  

\section{Examples and special cases}

\subsection{Shannon's entropy}

The most commonly used special case of the scoring-rule-based Bayesian active learning framework uses the logarithmic score and hence, Shannon's entropy. 

Sannon's entropy is used in multiple practical methods \citep{MacKay1992,Lawrence2004,Krause2006,Ji2008,Settles2010,Houlsby2011,Huszar2012quantum}

\subsection{Transductive active learning}

The goal of supervised learning is to build a model that accurately predicts the outcome $\y$ for a test input $\x$. Supervised learning can be divided into inductive and transductive approaches, based on the precise goals that one tries to achieve. In induction, the learner's goal is to induce general rules from the data, that can later be used in a variety of decision tasks. Information theoretic active learning based on Shannon's information is a good example of induction.

In contrast, in transductive machine learning the learner's goal is to predict, from the observed data, the outcomes $\y$ for a specific, pre-defined set of test examples. Generalising the solution to outside this training set is not important. Transduction was introduced to machine learning by Vladimir Vapnik \citep[see \eg][]{Gammerman1998}, who argued that when solving a specific problem of predicting labels on a test set, one should not try to solve a more complicated problem first.

Transductive learning can be expressed in a Bayesian framework \citep{Graepel1999}, and so can transductive active learning as a special case of the scoring-rule based active learning framework. Consider the following scoring rule.

\begin{align}
	\score_{\mbox{trans}}(\param,p_\data) &= \expect{\x\sim p_{\mbox{test}}}{\expect{p(\y\vert\x,\param)}{S(\y,p(\y\vert\x,\data))}}
\end{align}
where $p_{\mbox{test}}$ is the distribution of test examples, $p(\y\vert\x,\data)=\int p_{\data}(\param) p(\y\vert\x,\param) d\param $ is the posterior predictive distribution and $\score$ is an arbitrary scoring rule over $\Ye$. In transduction, $p_{\mbox{test}}$ is often a uniform mixture of point masses at particular test locations $p_{\mbox{test}} = \nicefrac{1}{N_{\mbox{test}}}\sum_{n=1}^{N_{\mbox{test}}} \delta(\x - \x_n)$, in which case the criterion can be written as follows.

\begin{align}
	\score_{\mbox{trans}}(\param,p_\data) &= \frac{1}{N_{\mbox{test}}}\sum_{n=1}^{N_{\mbox{test}}} \expect{p(\y\vert\x_n,\param)}{S(\y,p(\y\vert\x_n,\data))}
\end{align}

When $\score$ is chosen to be the logarithmic score, the active learning criterion \eqref{eqn:active_learning_optimisation_criterion} becomes the average mutual information between the newly unveiled label and the unknown labels of test examples. This objective function was considered in prior work by \citet{MacKay1992}.

One may also consider a loss-based scoring rule from section \ref{sec:loss_scoring_rule}

\begin{align}
	\score_{\mbox{trans},\loss}(\param,p_\data) &= \expect{\x\sim p_{\mbox{test}}}\left[ \min_{\hat{y}} \left\{\expect{\y\sim p(\y\vert\x,\param)} \loss(\hat{\y},\y) \right\} \right]\label{eqn:transductive_loss_scoring}
\end{align}
where $\loss(\hat{y},\y)$ is the loss incurred for predicting $\hat{y}$ when the true outcome is $\y$.

Transductive active learning with the loss-based criterion \eqref{eqn:transductive_loss_scoring} is often referred to as decision theoretic active learning. Exact decesion theoretic active learning is computationally demanding to achieve in practice. \citet{Kapoor2007} provide details of such an algorithm in the context of Gaussian process models. \citet{Zhu2003active} use a similar objective function to perform graph-based transductive active learning based using harmonic functions.

\subsection{Bayesian optimisation}

Numerical optimisation techniques are an important tool for many applications in engineering and computational science.
The goal is to find the minimum of an objective function $f$ by probing the function at a sequence of locations $x_n$. At each step, the function value $f(\x_n)$, and often local gradients are revealed. Many of these search and optimisation algorithms can be interpreted as active learning algorithms that try to learn about the objective function. This interpretation allowed for the development of novel class of probabilistic optimisation approaches which explicitly model the objective function, and perform probabilistic inference as part of the optimisation procedure. Many of these modern methods use Gaussian processes to model complex surfaces.

The first generation of Bayesian optimisation algorithms were typically based on the concept of improvement: the aim is to evaluate the objective function at a sequence of points, such that that subsequent function values become lower and lower. Several heuristics have been developed around this idea, including the popular expected improvement \citep{Mockus1982,Jones1998, Frean2008}, probability of improvement \citep{Jones2001,Lizotte2008} and upper confidence bounds \citep{Srinivas2009}. These algorithms are prone to getting stuck in local minima because they attempt to collect low function values, rather than to learn about the location of the optimum \citep{Hennig2012entropy}.

The newest class of algorithms \citep{Hennig2012entropy}, called information-greedy algorithms separate the problem of learning about the function and providing an estimate to the minimum. The sequence of function evaluations does not necessarily converge, or indeed it does not generally decrease.

Entropy search, and information-greedy Bayeisan optimisation algorithm presented in \citep{Hennig2012entropy} is a special case of the scoring-rule-based active learning framework. It employs a scoring rule of the following form:

\begin{equation}
	\score_{\mbox{argmin}}(f,p_\data) = \score(\argmin_{\x}f,p_{\argmin}) \label{eqn:Bayesian_argmin_scoring_rule}
\end{equation}
,where $f$ is the objective function which takes the role of $\param$ as the parameter of interest. $p_\data$ is a posterior measure over objective functions -- in most recent work this is a Gaussian process measure. $p_{\argmin} \in \probmeasures{\Xe}$ is the probability distribution that $p_\data$ implies over the location of the minimum $\argmin_{x}f$. Note that in \citep{Hennig2012entropy} $p_{\argmin}$ is called $p_{\min}$, I use a different notation for consistency. The scoring rule $\score$ could be any strictly proper scoring rule, \citeauthor{Hennig2012entropy} use the logarithmic loss which they derive approximations for.

Equation \eqref{eqn:Bayesian_argmin_scoring_rule} implies that the goal of optimisation is to find the exact location of the global minimum. In many applications however this is not required, instead, it is sufficient to learn about the value at the minimum. In this case, one could use a scoring rule of the following form.

\begin{equation}
	\score_{\mbox{min}}(f,p_\data) = \score(\min_{\x}f,p_{\min}), \label{eqn:Bayesian_argmin_scoring_rule}
\end{equation}
where $\min_{\x}f$ is the minimal value of the objective function, $p_{\min}$ the measure $p_\data$ induces over the minimal value of $f$.

Lastly, it is possible that neither the location nor the exact value of the minimum is important. The goal is simply to find a location $\x^{*}$ where the function $f(\x^{*})$ is as small as possible. In most modern applications of numerical optimisation this is a more reasonable requirement than finding the exact location of the global optimum. This goal can be expressed in the scoring rule framework using the following scoring rule.

\begin{equation}
	\score_{\mbox{best}}(f,p_\data) = f(\argmin_{\x}\expect{f\sim p_\data}{f(\x)}) \label{eqn:Bayesian_optimisation_scoring_rule}
\end{equation}

Note that $\score_{\mbox{best}}$ is also a special case of a Bayesian decision score $\score_{\loss}$ (section \ref{sec:loss_scoring_rule}, Equation \eqref{eqn:loss_scoring_rule}) with actions $\actionset=\Xe$ and loss function $\loss(f,a)=f(a)$.


Information-greedy Bayesian optimisation methods are still in their infancy. Interpreting them in the general scoring-rule-based framework allows one to understand the underlying goals, and propose straightforward extensions. I am not aware of either \eqref{eqn:Bayesian_argmin_scoring_rule} or \eqref{eqn:Bayesian_optimisation_scoring_rule} used in previous work in the context of Bayesian optimisation.

\subsection{Sequential Bayesian quadrature}

Sequential Bayesian quadrature, described in detail in chapter \ref{sec:herding} is also an example of scoring-rule-based active learning. In that case, one seeks to approximate the expectation of a function $f$ under a probability distribution $p$

Since the integral is intractable to calculate in closed form, the integral is approximated by weighted sum of function evaluations at particular sampling locations $\x_n$. The active learning problem involves finding the next sample $x_1$ point given the points already querried.

\begin{equation}
	\score(f,p_\data) = \left(Z_{p,f} - \hat{Z}_\data \right)^2
\end{equation}

Using this scoring rule in the scoring-rule based Bayesian active learning framework is equivalent to minimising the expected reduction in posterior variance of the quadrature estimate as in eqn.\ \eqref{eqn:foo}.

\section{Bayesian active learning by Disagreement (BALD)}

Foo bar