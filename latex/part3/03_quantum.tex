%!TEX root = ../thesis.tex

\begin{summarycontributions}
 The material presented in this chapter is joint work with Neil M.\ T.\ Houlsby. Both authors contributed equally to all aspects of research. The work forms the basis of the journal article `Adaptive Bayesian Quantum Tomography' \citep{Huszar2012quantum}. Figures \ref{fig:Bloch_disk}, \ref{fig:qubit_results} and \ref{fig:two_qubit_results} illustrating experimental results are taken from this paper as published, or with only slight modification, with consent from the co-author.
\end{summarycontributions}

\section{Introduction}

Quantum computing and quantum communication are rapidly exploding areas of modern computer science. Exploiting quantum physical phenomena such as entanglement and quantum teleportation, these computers and communication protocols radically extend the scope of efficient computing. Over the past couple of decades it has been shown that large classes of practically important algorithms, such as integer factorisation or discrete logarithm, can be implemented in polynomial time using quantum computers \citep{Shor1994,Shor1997}. For these problems, no efficient classical (non-quantum) algorithms are known.

However, there is an important experimental limitation which is a barrier to progress towards studying large quantum computers: state reconstruction. At the hearth of this problem lies the fact that the end result of a quantum computation is not simply a series of bits, but instead a quantum state. And quantum states cannot be directly observed. In order to figure out what state a quantum computer produced as the result of computation one has to perform \emph{measurements} on it.

Measurements in quantum physics have two surprising properties	:

Firstly, the outcome of a quantum measurement is generally non-deterministic. That is, even if the state of the system being measured is fully known, the outcome of the measurement is random. Therefore, in most cases, a single measurement doesn't provide full information about the state of the system, so repeated measurements and statistical inference are needed.

Secondly, a measurement destroys -- or at least modifies -- the quantum state of the system itself. The phenomenon is known as wave function collapse. This means that repeated measurements on the same system are pointless, as the first measurements destroy the state. These two properties of quantum measurements imply that one can (nearly) never obtain full information about a quantum state in any single experiment, because the state is destroyed before sufficient information is extracted.

To overcome these problems physicists studying quantum systems usually produce several independent copies of the same system (equivalent to ``running'' a quantum computer several times), and make measurements on each of the independent copies. Reconstructing the state from this sequence of non-deterministic measurement outcomes is a statistical inference problem known generally as \emph{state reconstruction} or \emph{quantum state tomography} \citep{Artiles2005}.

Technological and implementational constraints aside, a barrier to studying large, multipartite quantum systems today is that the number of independent copies required to accurately reconstruct the state via quantum tomography grows at least exponentially with the size (number of qubits) of the system. So even though classically hard algorithms can be implemented using polynomial number of quantum operations, reading out the result can still take exponentially long.\footnote{Fortunately, in many practical applications of quantum computers, such as finding prime factors, rich prior information is available about the structure of the results, which can be exploited to speed up the tomography process}

In current experimental quantum physics, when researchers study properties a new physical implementation of a quantum gate, they have to demonstrate that in multiple situations their equipment produces a state predicted by theory. Often due to noise and other environmental factors these implementations are imperfect and the produced state does not turn out exactly as desired. The degree of discrepancy between the theoretical predictions and the actually produced state is called infidelity. To be able to measure infidelity, experimenters often have to perform full quantum tomography. Therefore any method that speeds these processes up may be of great practical importance.

In this chapter I explore the possibility of speeding up quantum tomography by using state-of-the art Bayesian active learning. I will first introduce the mathematical framework for studying quantum tomography, and provide a Bayesian analysis of the problem. Then I present an active learning method to perform quantum tomography, that we call Adaptive Bayesian Quantum Tomographu (ABQT). I present simulated experimental results that show that active learning can speed up the process of tomography by orders of magnitude.

% #### ##    ## ######## ########   #######  
%  ##  ###   ##    ##    ##     ## ##     ## 
%  ##  ####  ##    ##    ##     ## ##     ## 
%  ##  ## ## ##    ##    ########  ##     ## 
%  ##  ##  ####    ##    ##   ##   ##     ## 
%  ##  ##   ###    ##    ##    ##  ##     ## 
% #### ##    ##    ##    ##     ##  #######  

\section{A primer to quantum statistics}

The central concept in quantum physics is the quantum state of a system. The state describes the system's behaviour when measurements are carried out. An example of a simple, two-dimensional quantum state is the polarisation state of a single photon. Photons are indeed one of the most widely used quantum physical model systems. Throughout this introduction I will use photons as an example to illustrate physical analogues of mathematical formalism. Other examples of quantum systems include spins of electrons, nuclear spins, quantum dot pairs, atomic spins. For recent reviews on the state of experimental quantum computing see \citep{Ladd2010} and references therein.

Even a simple system like a photon can be in a variety of states. For example, the photon can be linearly polarised. The angle of linear polarisation can be either horizontal (denoted as $\vert H\rangle$) or vertical ($\vert V \rangle$) or any angle in between. Light can also be circularly		 polarised. The direction of circular polarisation can be left (denoted $\vert L\rangle$) right ($\vert R\rangle$), or anything in between. In addition, the system can be in any superposition of linear and circular polarisation which is generally called elliptic polarisation.

Mathematically, the polarisation state of a photon is fully described by a two-dimensional unit-length complex vector. In this chapter I use the so called Dirac or bra-ket notation \citep{Dirac1939} for complex valued vectors $\vert\x\rangle$, to clearly distinguish them from real valued vectors and variables that appeared in other sections of the thesis. In this notation $\langle\x\vert$ denotes the conjugate transpose of $\vert\x\rangle$, $\langle\x\vert\y\rangle$ the scalar product between $\vert\x\rangle$ and $\vert\y\rangle$. The scalar product between complex vectors is defined as $\langle\x\vert\y\rangle = \sum_{d=1}^{D} x_d \overline{y_d}$, where $\overline{y_d}$ is complex conjugation.

A two-dimensional unit-length complex vector $\vert \phi \rangle$ can be parametrised using three continuous parameters $\theta$, $a_x$ and $a_y$ as follows:

\begin{equation}
	\vert \phi \rangle = \left(\begin{array}{c}
		\cos(\theta)\exp(ia_x)\\
		\sin(\theta)\exp(ia_y)\end{array}\right)
\end{equation}

This complex vector may be used to represent the quantum state of the photon. Parameter $\theta$ can be interpreted as the angle of linear polarisation relative to horizontal, with $\theta = 0^{\circ}$ meaning horizontal, $\theta=90^{\circ}$ vertical polarisation:

\begin{equation}
	\vert H \rangle = \left(\begin{array}{c}
		1\\
		0\end{array}\right)
	\label{eqn:quantum_photon_horizontal}
\end{equation}
and
\begin{equation}
	\vert V \rangle = \left(\begin{array}{c}
		0\\
		1\end{array}\right)
	\label{eqn:quantum_photon_vertical}
\end{equation}

Any real valued linear combination of these two basis vectors (as long as unit-norm constraint is preserved) describes a general linearly polarised state at a certain angle. To represent circular polarisation, we need to use complex vectors:

\begin{equation}
	\vert R \rangle = \frac{1}{\sqrt{2}}\left(\begin{array}{c}
		1\\
		i\end{array}\right)
	\label{eqn:quantum_photon_right}
\end{equation}
and
\begin{equation}
	\vert L \rangle = \frac{1}{\sqrt{2}}\left(\begin{array}{c}
		1\\
		-i\end{array}\right)
	\label{eqn:quantum_photon_left}
\end{equation}

To see how these abstract states relate to real-world observable phenomena, one has to understand how measurements work.

\subsection{Measurements}

The quantum state of a system cannot be directly observed, only via \emph{measurements} performed on the system. Measurements in quantum physics have two distinctive features: the outcome is non-deterministic and performing a measurement alters the state of the system on which the measurement was performed.

An example of a measurement in case of a photon would be letting it pass through a linear polarising filter (like the ones used in LCD displays). Mathematically, the polarising filter is described by a two-dimensional unit-length complex vector $\vert\psi\rangle$, just like a quantum state. Depending on the state of the photon $\vert\phi\rangle$ and the measurement describing the filter $\vert\psi\rangle$, the photon either `bounces back' from the filter or with a certain probability passes trough. By placing a photodetector after the polar filter one can record which one of these two outcomes happened. The probability of the two outcomes is governed by the state of the photon and the measurement itself and follows the following rules.

\begin{align}
	\mathbb{P}\left( \mbox{pass}\ \vert\ \vert\phi\rangle , \vert\psi\rangle \right) &= \left\vert\langle\psi\vert\phi\rangle\right\vert^2 &= \trace\left(\vert\phi\rangle\langle\phi\vert\right)\left(\vert\psi\rangle\langle\psi\vert\right) \\
	\mathbb{P}\left( \mbox{bounce}\ \vert\ \vert\phi\rangle , \vert\psi\rangle \right) &= 1 - \left\vert\langle\psi\vert\phi\rangle\right\vert^2 &= \left(\trace\vert\phi\rangle\langle\phi\vert\right)\left(I - \vert\psi\rangle\langle\psi\vert\right)
\end{align}

The outcome probabilities are expressed as straightforward scalar products, or in general, quadratic forms. The unit norm constraint on $\vert\phi\rangle$ and $\vert\psi\rangle$ ensures that the outcome probabilities are aplways non-negative and sum to one.

Let us look at the outcome probabilities in concrete examples.

\begin{example}
A linearly polarised photon with polarisation angle $\theta = 30^{\circ}$	 is measured using a horizontally polarised filter. The photon's state is $\vert\phi\rangle = \cos(\theta)\vert H \rangle + \sin(\theta)\vert V \rangle$. The filter is described as $\vert\psi\rangle = \vert H \rangle$. Since $\langle H \vert V \rangle = 0$, we can see that the probability that the photon passes the filter is

\begin{equation}
	\mathbb{P}\left( \mbox{pass}\ \vert\ \vert\phi\rangle , \vert\psi\rangle \right) = \cos(\theta)\langle H \vert H \rangle + \sin(\theta) \langle H \vert V \rangle = \cos(\theta) = \frac{\sqrt{3}}{2}
\end{equation}\end{example}

\begin{example}
A linearly polarised photon with polarisation angle $\theta = 30^{\circ}$ from the horizontal is measured using a right circularly polarised filter. The photon's state is $\vert\phi\rangle = \cos(\theta)\vert H \rangle + \sin(\theta)\vert V \rangle$. The filter is described as $\vert\psi\rangle = \vert R \rangle$. Using Eqns.\ \eqref{eqn:quantum_photon_horizontal},\eqref{eqn:quantum_photon_vertical} and \eqref{eqn:quantum_photon_right} one can see that $\langle V \vert R \rangle = \nicefrac{1}{\sqrt{2}}$ , $\langle H \vert R \rangle = \nicefrac{i}{\sqrt{2}}$. Thus, the probability of the photon passing through is
\begin{equation}
	\mathbb{P}\left( \mbox{pass}\ \vert\ \vert\phi\rangle , \vert\psi\rangle \right) = \frac{1}{2}\left\vert\cos(\theta) + i\sin(\theta)\right\vert^2 = \frac{1}{2}.
\end{equation}

So the photon passes or bounces back completely randomly with a probability $0.5$ irrespective of the angle of its linear polarisation. Thus, performing a measurement using a circular polarisation filter provides no information about the linear polarisation of the photon. This example highlights the importance of choosing the right measurement to perform in quantum tomography.
\end{example}

Crucially, measuring a quantum system alters the state.	This phenomenon is called wave function collapse. In quantum tomography we assume that after only a single measurement on a system, its state gets completely destroyed and we cannot use it anymore for the purposes of inference. After each measurement, once the outcome is recorded, the measured system is discarded, and a new, independent copy of the system is generated and measured.

There are alternative approaches that use sequences of measurements which only partially destroy the state; these approaches are referred to as continuous weak measurements and quantum control \citep{Smith2006}. Weak measurements also have a high importance in quantum cryptanalysis \citep{Pryde2004}. Because the algorithm presented in this chapter does not make use of weak measurements, I will not give more details on what exactly happens to the state after a measurment is performed. The interested reader is referred to \citep{Petz2008}, a comprehensive textbook on quantum statistics.

\subsection{Density matrices}

In the previous paragraphs we have seen that quantum measurements are inherently non-deterministic. But in some cases there is another source of uncertainty effecting the outcome of measurements. These can be due to environmental noise, lack of information, and various other reasons. We will call these other sources of uncertainty classical uncertainty, as opposed to quantum uncertainty which is the inherent uncertainty in measurements. When both kinds of uncertainty are present, we will say that the quantum system is in a \emph{mixed state}. The states we have discussed so far are called \emph{pure states}/. In pure states, no classical uncertainty is present.

\begin{example}[Observationally equivalent sources]
Consider two noisy sources of photons. Source A produces a horizontally polarised photon with probability $0.5$ or a vertically polarised one with probability $0.5$. Source B produces the state $\frac{1}{\sqrt{2}}\vert H \rangle + \frac{1}{\sqrt{2}} \vert V \rangle$ with probability $0.5$ or $\frac{1}{\sqrt{2}}\vert H \rangle - \frac{1}{\sqrt{2}} \vert V \rangle$ with probability $0.5$. These correspond to $45^{\circ}$ and $-45^{\circ}$ polarisation angles respectively.

Let us see what happens if we perform a measurement $\langle \psi \vert$ on the two noisy systems. First, for system A the probability is given by
\begin{equation}
\mathbb{P}_{A}\left( \mbox{pass} \vert\ \vert\psi\rangle \right) = \frac{1}{2} \langle\psi\vert H\rangle^2 + \frac{1}{2} \langle\psi\vert V\rangle^2
\end{equation}

For system B we get
\begin{align}
\mathbb{P}_{B}\left( \mbox{pass} \vert\ \vert\psi\rangle \right) &= \frac{1}{2} \left(\frac{1}{\sqrt{2}}\langle\psi\vert H\rangle + \frac{1}{\sqrt{2}}\langle\psi\vert V \rangle\right)^2 + \frac{1}{2} \left(\frac{1}{\sqrt{2}}\langle\psi\vert H\rangle - \frac{1}{\sqrt{2}}\langle\psi\vert V \rangle\right)^2 \\
	&= \frac{1}{2} \langle\psi\vert H\rangle^2 + \frac{1}{2} \langle\psi\vert V\rangle^2 + \frac{1}{2}\langle\psi\vert H\rangle\langle\psi\vert V\rangle - \frac{1}{2}\langle\psi\vert H\rangle\langle\psi\vert V\rangle \\
	&= \frac{1}{2} \langle\psi\vert H\rangle^2 + \frac{1}{2} \langle\psi\vert V\rangle^2
\end{align}

In both cases the probabilities of observing the photon passing the filter or bouncing back are exactly the same. The probability is only a function of the measurement, but it does not depend on which source the photon came from. So no matter what measurement we make, the two sources are indistinguishable experimentally. I will call these two sources \emph{observationally equivalent}.\footnote{Incidentally, in this example, because $\nicefrac{1}{2} \langle\psi\vert H\rangle^2 + \nicefrac{1}{2} \langle\psi\vert V\rangle^2 = \nicefrac{1}{2}$, the measurement outcome does not even depend on the measurement.}
\label{empl:two_noisy_systems}
\end{example}

As Example \ref{empl:two_noisy_systems} highlights, when classical and quantum uncertainty are both present in a system, they cannot be disambiguated by measuring the system. There are examples of quantum systems which are different, but which cannot be told apart experimentally. This concept is intimately related to the issue of non-identifiable likelihood models \citep{Teicher1961} in statistics: if two or more parameters define exactly the same likelihood model, the true parameter cannot be identified based on data.

We can define equivalence classes of systems, and the goal of quantum tomography becomes to infer the equivalence class, or \emph{the mixed state} of the system. These mixed states can be parametrised via the so called density matrix $\quantumparam$ \citep{Fano1957}.

\begin{definition}[Density matrix]
Consider a random quantum system that is in pure state $\vert\phi\rangle$ with probability $P_{\vert\psi\rangle}$. The density matrix describing this system is defined as the following linear operator:
\begin{equation}
	\quantumparam = \expect{\vert\psi\rangle\sim P}{\vert\phi\rangle\langle\phi\vert}
\end{equation}
The density matrix is Hermitian and has unit trace.
\end{definition}

As we have seen, the two noisy systems in Example \ref{empl:two_noisy_systems} were equivalent, and indeed they are both described by the same density matrix $\quantumparam=\frac{1}{2}I$. In the context of photon sources, a light source, whose density matrix is $\quantumparam=\frac{1}{2}I$ is called \emph{unpolarised}. There are several `different', but observationally equivalent unpolarised light sources, and in fact, randomly mixing unpolarised light with unpolarised light remains unpolarised.

Pure states are special cases of mixed states. A system with pure state $\vert \phi \rangle$ can be described by its density matrix $\quantumparam = \vert \phi \rangle \langle \phi \vert$. This follows from the definition, using a probability distribution that concentrates on the single pure state $\vert \phi \rangle$. Density matrices corresponding to pure states are therefore always rank-one.

Similarly to mixed states, measurements also have a more general form. The most general class of measurements considered in this thesis are Positive Operator Valued Measures (POVMs). A POVM is defined by a set, $\mathbb{M}$, of Hermitian operators $M_{\outcome}$, indexed by possible outcomes $\outcome\in\{1,\ldots,D\}$, satisfying $\sum_{\outcome=1}^{D} M_{\outcome} = I$. These Hermitian operators determine the probability of observing outcome $\outcome$ in configuration $\config$ when the measured system is in state $\quantumparam$ via Born's rule:

\begin{equation}
	\mathbb{P}\left(\outcome\vert\quantumparam,\mathbb{M}\right) = \text{tr}\left\{M_{\outcome}\quantumparam \right\}
\end{equation}

The condition $\sum_{\outcome=1}^{D} M_{\outcome} = I$ ensures that the probabilities sum to 1, and they are always non-negative because all operators involved are Hermitian.

So far we have only considered probably the simplest two-dimensional model system, the polarised photon. In quantum computing, two-dimensional quantum systems are called qubits. As a generalisation of horizontal and vertical polarisation $\vert H \rangle$ and $\vert V \rangle$ the notation $\vert 0 \rangle$ and $\vert 1 \rangle$ is often used to denote the two most important states.

For general $D$-dimensional systems, $\quantumparam$ is a $D \times D$ Hermitian matrix, satisfying $\trace\quantumparam = 1$. Quantum computers use $m$-qubit systems with $D=2^m$, and the notation $\vert 000 \rangle$, $\vert 001 \rangle$, \ldots $\vert 111 \rangle$ is used to denote important states. There also exist quantum systems where $D\neq 2^m$, such as qutrits where $D=3$, however because of their practical applications, multi-qubit systems are by far the most widely studied.

Multipartite systems are quantum systems composed of multiple subsystems. Multi-qubit systems are an example of these. The subsystems of a multipartite system can be separable or entangled. Separability is a property analogous to statistical independence and it means that the subsystems behave statistically independently of each other under any separate measurement of them. Separability and entanglement are a difficult topic and beyond the scope of this general introduction. The interested readers are referred to \citep[Chapter 4]{Petz2008}.

% #### ##    ## ######## ######## ########  ######## ##    ##  ######  ######## 
%  ##  ###   ## ##       ##       ##     ## ##       ###   ## ##    ## ##       
%  ##  ####  ## ##       ##       ##     ## ##       ####  ## ##       ##       
%  ##  ## ## ## ######   ######   ########  ######   ## ## ## ##       ######   
%  ##  ##  #### ##       ##       ##   ##   ##       ##  #### ##       ##       
%  ##  ##   ### ##       ##       ##    ##  ##       ##   ### ##    ## ##       
% #### ##    ## ##       ######## ##     ## ######## ##    ##  ######  ######## 

\section{Quantum Tomography as Bayesian Inference}

Quantum state tomography involves determining from experimental data the quantum state, $\quantumparam$, of a system by performing measurements on several identical copies. As $\quantumparam$ is Hermitian and have unit trace, $D^2-1$ real degrees of freedom must be estimated. Note that the number of degrees of freedom is exponential in the number of qubits in a multi-qubit system.

The apparatus for a tomographic experiment may be configured to perform several different measurements; we use $\config\in\configset$ to index all accessible configurations Each measurement configuration $\config$ is characterised by a positive operator-valued measure (POVM). For each configuration, a measurement results in observing one of $D$ distinguishable outcomes\footnote{In general the number of outcomes does not have to correspond to the dimension of the system, but we can make this assumption without loss of generality }. A POVM is defined by the set, $\mathbb{M}_{\config}$, of Hermitian operators $M_{\config\outcome}$, indexed by possible outcomes $\outcome\in\{1,\ldots,D\}$, satisfying $\sum_{\outcome=1}^{D} M_{\config\outcome} = I$. These POVMs jointly constitute our tomographic model $\mathcal{M}=\{\mathbb{M}_{\config}:\config\in\configset\}$ and determine the probability of observing outcome $\outcome$ in configuration $\config$ when the measured system is in state $\quantumparam$ via Born's rule:

\begin{equation}
	\mathbb{P}\left(\outcome\vert\quantumparam,\config;\mathcal{M}\right) = \trace\left\{M_{\config\outcome}\quantumparam \right\}\label{eqn:quantum_Born}
\end{equation}

State reconstruction has been approached with several methods, the most popular being maximum likelihood estimation (MLE). MLE finds a physically feasible state $\quantumparam$ that is most likely to have produced the observed data, $\mathcal{D}$, by maximising the likelihood, which can be rewritten as:

\begin{equation}
	\label{eqn:quantum_lik}
	\mathcal{L}(\quantumparam;\data)=\prod_{n=1}^{N} \mathbb{P}\left(\outcome_n\vert\quantumparam,\config_n\right) = \prod_{\config \in \configset}n_{\config}!\prod_{\outcome=1}^{D}\frac{\mbox{tr}\{M_{\alpha\outcome}\quantumparam\}^{n_{\config\outcome}}}{n_{\config\outcome}!},
\end{equation}
where $n_{\config}$ is the number of times configuration $\config$ was used, $n_{\config\outcome}$ is the number of times outcome $\outcome$ was observed in configuration $\config$. All probabilities are conditional on $\mathcal{M}$, for brevity this is omitted.

A well-known drawback of maximum likelihood estimation is that it often yields rank-deficient estimates of $\quantumparam$, and thus assigns zero predictive probability to certain observations\,\citep{BayesianTomography}. This seems an unreasonable conclusion on the basis of a finite sample. This phenomenon is not unusual in maximum likelihood methods, and is related to \emph{over-fitting} in statistics \citep{Hawkins2004}. Additionally, MLE provides no measure of uncertainty in its point estimate.

More sophisticated methods for quantum tomography use Bayesian inference and suffer from neither of these problems\,\cite[][and refs.]{BayesianTomography}. In Bayesian inference a prior probability density, $p(\quantumparam)$, over feasible states is specified. This prior is then augmented with the likelihood from Eqn.\,\eqref{eqn:quantum_lik} using Bayes' rule to yield a posterior distribution:

\begin{equation}
	\label{eqn:quantum_bayes}
	p(\quantumparam\vert\data)\propto \mathcal{L}(\quantumparam;\data)p(\quantumparam)
\end{equation}

Should we want a point estimate, we may report, say, the Bayesian mean estimate (BME) which is known to maximise expected operational divergences\,\citep{BayesianTomography,BayesianOptimality}. But importantly, Bayesian inference also provides \emph{error bars}, and more: the posterior captures richly our remaining uncertainty in the true state having seen the data $\data$. 

For Bayesian inference one has to provide the prior $p(\quantumparam)$, which is typically chosen to be non-informative or uniform. Here we adopt the representation and prior introduced in\,\citep{BayesianTomography}, that treats the original system of interest as part of a larger, $D \times K$ dimensional bipartite system. Our prior over the mixed state $\quantumparam$ is then defined as the measure induced by the uniform (Haar) measure over pure states in $D\times K$ dimensions. It is easy to see that, tracing out\footnote{`tracing out' is an operation on the density matrix of a multi-partitite quantum system, which is analogous to computing the marginal of multivariate probability distributions. For further details please refer to \citep{BayesianTomography}.} the $K$ dimensional ancillary part leaves us with a prior distribution that concentrates on rank-$K$ density matrices. By tuning parameter $K$ one can trade off between computational efficiency and estimation accuracy, in a similar manner to compressed sensing\,\citep{CompressedSensing}.

Unfortunately, normalisation of the posterior distribution (Eqn.\,\eqref{eqn:quantum_bayes}) becomes analytically intractable, and therefore one have to approximate it. There has not been much prior work on approximate Bayesian inference in quantum tomography, most existing approaches have used a variant of Markov chain Monte Carlo\,\cite[][and refs.\ therein]{BayesianTomography}. These methods are not particularly fast and require evaluation of the full likelihood \eqref{eqn:quantum_lik}, which has $\mathcal{O}(n)$ cost with the number of different configurations used so far.

If we want to implement active learning, inference has to be performed after each measurement or after mini-batches of measurements. Therefore, MCMC methods whose cost increases with the number of measurements are inappropriate. To address this problem we developed a fast sequential importance sampling (SIS) algorithm, with $\mathcal{O}(1)$ likelihood evaluation cost. See \,\citep{SMCBook} for a thorough overview of these techniques.

In SIS, one keeps track of a number of samples, often called particles, $\quantumparam_s,\, (s=1\ldots S)$ and corresponding weights $w_s, \, \left( \sum_s w_s = 1 \right)$  which are updated sequentially, every time a new measurement is made. Assume that after $n$ measurements, having observed data $\data_n$, the particles and weights  $w^{(n)}_s$ constitute an approximation to the posterior:	

\begin{equation}
	p(\quantumparam\vert\data_n)\approx \sum_{s=1}^{S}w^{(n)}_s\delta(\quantumparam-\quantumparam_s)\label{eqn:quantum_SISapprox}
\end{equation}

Using this approximation, and Bayes' rule, one can derive an approximation to the next posterior, after observing a new outcome $\outcome_{n+1}$ in configuration $\config_{n+1}$, as:

\begin{align}
	p(\quantumparam\vert\config_{n+1}&,\outcome_{n+1},\data_n) = \frac{\mathbb{P}(\outcome_{n+1}\vert\quantumparam,\config_{n+1})p(\quantumparam\vert\data_n)}{\int \mathbb{P}(\outcome_{n+1}\vert\quantumparam,\config_{n+1})p(\quantumparam\vert\data_n) d\quantumparam}\label{eqn:quantum_SISupdate}\\
	&\approx \sum_{s=1}^{S}\underbrace{\frac{\mathbb{P}(\outcome_{n+1}\vert\quantumparam_s,\config_{n+1})w^{(n)}_s}{\sum_{r=1}^{S}\mathbb{P}(\outcome_{n+1}\vert\quantumparam_r,\config_{n+1})w^{(n)}_r}}_{w^{(n+1)}_s}\delta(\quantumparam-\quantumparam_s)\notag
\end{align}

The new weights $w^{(n+1)}_s$ are the re-normalised product of our current weights $w^{(n)}_s$ and observation probabilities $\mathbb{P}(\outcome_{n+1}\vert\quantumparam_s,\config_{n+1})$. This update is fast, and only requires computing one term of the full likelihood, thus its complexity is independent of how many configurations have been tried before. This computational efficiency comes at a price; as time progresses, several weights decay to almost zero, and thus the quality of our approximation drops. This issue can be detected and handled by monitoring the effective sample size and resampling appropriately\,\citep{SMCBook}.

%    ###     ######  ######## #### ##     ## ######## 
%   ## ##   ##    ##    ##     ##  ##     ## ##       
%  ##   ##  ##          ##     ##  ##     ## ##       
% ##     ## ##          ##     ##  ##     ## ######   
% ######### ##          ##     ##   ##   ##  ##       
% ##     ## ##    ##    ##     ##    ## ##   ##       
% ##     ##  ######     ##    ####    ###    ######## 

\section{Active learning in Quantum Tomography}

Most existing approaches to optimal experiment design for quantum tomography determine, prior to collecting data, an optimal set of measurements to be used throughout the experiment. In this sense, whenever they exist, mutually unbiased bases (MUBs) are known to be optimal\,\citep{MUBFirst,MUBExperiment}. Research since has focused mainly on proving or disproving existence of, and implementing MUBs in various dimensions\,\citep{DimensionSix,MUBQutrit,MUBExperiment}.

Other work,\,\citep{OEDFirst,OEDAverage} considered OED based on the Cram\'{e}r-Rao bound. These approaches, including MUBs, are not active learning algorithms. They provide only a partial solution to the problem of optimal experiment design inasmuch as they do not take partial data into account. If we are allowed to revise our choice of measurements during the experiment based on data collected so far, we may be in a better position to reduce redundancy.

In the context of quantum tomography, the active learning approach has been referred to as self-learning measurements \citep{SelfLearning, SelfLearningExperimental}. However, due to the expensive computations that are involved, these methods have been restricted to two dimensional pure quantum states, or experiments with very few measurements. The BALD methodology presented in the previous chapter combined with sequential importance sampling allow us to build a fast, online algorithm that allows self-learning in arbitrary dimensions with many measurements. 

Recall the BALD criterion for information theoretic active learning from Chapter \ref{sec:active_learning_framework}. Now, the mixed state density matrix $\quantumparam$ takes the role of parameters $\param$
\begin{equation}
\argmax_{\config\in\configset} \left\{ \mathbb{H}\left[\mathbb{P}(\outcome|\config,\data)\right] - \mathbb{E}_{p(\quantumparam\vert\data)}\left[\mathbb{H}\left[ \mathbb{P}(\outcome|\config,\quantumparam) \right] \right]\right\}\label{eqn:quantum_rearrangement}
\end{equation}

To compute this objective, we can combine Born's rule in Eqn.\ \eqref{eqn:quantum_Born} with the sequential importance sampling approximation from Eqn.\ \eqref{eqn:quantum_SISapprox}:

\begin{equation}
\argmax_{\config\in\configset} \left\{ \mathbb{H}\left[\sum_{s=1}^{S} w^{(n)}_s \trace\left\{\quantumparam M_{\config,\bullet} \right\} \right] - \sum_{s=1}^{S} w^{(n)}_s \mathbb{H}\left[ \trace\left\{\quantumparam M_{\config,\bullet} \right\} \right] \right\}\label{eqn:quantum_objective_SIS}
\end{equation}

The above objective function can be evaluated in $\mathcal{DK}$, where $K$ is the rank parameter used to define the prior over $\quantumparam$.

In summary, we propose the following algorithm, called Adaptive Bayesian Quantum Tomography \citep[ABQT, ][]{Huszar2012quantum}. After each single measurement, ABQT updates its approximate posterior via sequential Monte Carlo using Eqn.\ \eqref{eqn:quantum_SISupdate}, then chooses the next measurement by direct numerical maximisation of the BALD information theoretic objective in Eqn.\ \eqref{eqn:quantum_objective_SIS}.

Within the Bayesian framework, one could consider other measures of uncertainty about the Quantum state. \citep{SelfLearning} seeks to minimise the Bayes risk using fidelity of $\quantumparam$ as the loss function. Although this loss is theoretically attractive, only the log loss allows the particular analytic reformulation to Eqn.\ \eqref{eqn:quantum_rearrangement} that permits efficient online computations. Other loss functions require one full posterior update for every possible outcome for each measurement under consideration, ABQT requires only one update per complete cycle. Therefore, if one does not use log loss online computation is infeasible; in \citep{SelfLearningExperimental} experimental designs for all $2^N$ possible experimental outcome successions are pre-computed, they are therefore limited to very short experiments ($< 20$ measurements). Combining Eqn. \eqref{eqn:quantum_rearrangement} with our SIS Bayesian update scheme allows for fast online experimental design.

% ########  ########  ######  ##     ## ##       ########  ######  
% ##     ## ##       ##    ## ##     ## ##          ##    ##    ## 
% ##     ## ##       ##       ##     ## ##          ##    ##       
% ########  ######    ######  ##     ## ##          ##     ######  
% ##   ##   ##             ## ##     ## ##          ##          ## 
% ##    ##  ##       ##    ## ##     ## ##          ##    ##    ## 
% ##     ## ########  ######   #######  ########    ##     ######  

\section{Results}

\begin{figure}
	\resizebox{\columnwidth}{!}{
	\includegraphics{figs/BALD/quantum/Bloch_disk.pdf}
	}

	\caption[Illustration of adaptive tomography on a single-photon system]{Adaptive selection of measurements based of partial data. Scatter plots show 400 samples from current posterior. Shaded circles around the `Bloch disk' show relative value of the objective in Eqn.\,\eqref{eqn:quantum_rearrangement} for different measurement directions (lighter is higher). Pairs of arrows show the most informative next measurement. Circular histograms show the number of times measurement directions have been used. \textbf{(a)}  Initially, no observations are made, samples shown are from the uniform prior. All measurements are equally informative, we chose to start with $\{\left\vert H\right\rangle,\left\vert V\right\rangle\}$. \textbf{(b)}  After one measurement, the posterior is updated, the next best measurement is mutually unbiased w.r.t.\ the first one. It is now $\{\left\vert D\right\rangle,\left\vert A\right\rangle\}$. \textbf{(c)} After two observations, the next best measurement is equally biased to the first two bases. \textbf{(d)} Posterior after 1000 observations concentrates around true state. The method tries a range of measurements, with a tendency to point towards the solution.
	\label{fig:Bloch_disk}}
\end{figure}

\begin{figure}
	\input{figs/BALD/quantum/one_qubit_combined.tikz}
	\caption[Single-qubit adaptive tomography using projective measurements.]{One qubit tomography using projective measurements. \textbf{(a)}  Improvement of mean posterior fidelity as the experiment progresses. Results are shown for uniformly sampled measurements (\ref{leg:rand}), uniformly sampled Pauli measurements ( \ref{leg:MUB}), ABQT selecting adaptively amongst the 3 Pauli measurements (\ref{leg:aMUB}) and ABQT picking general measurements (\ref{leg:aFlex}). Adaptive optimisation of measurements allows for an almost $n^{-1}$ rate of convergence, while other methods are more consistent with a $n^{-\frac{1}{2}}$ rate. \textbf{(b)}  Final value of the mean posterior infidelity after 6000 measurements using the four methods as before, as a function of purity of the state to be estimated. The advantage of ABQT is greatest for purer states. \label{fig:qubit_results}}
\end{figure}

\begin{figure}[th]
	\begin{center}
		\input{figs/BALD/quantum/case1.tikz}
		\input{figs/BALD/quantum/case2.tikz}\\
		\input{figs/BALD/quantum/case3.tikz}
		\input{figs/BALD/quantum/case5b.tikz}
	\end{center}
	\caption[Performance of the adaptive Bayesian method in two-qubit tomography]{Two qubit QST with uniformly chosen amongst MUB (\ref{leg:MUB}) or SSQT bases (\ref{leg:SSQT}) and AB QT picking from the same set of MUBs (\ref{leg:aMUB}),  SSQT bases (\ref{leg:aSSQT}) or a more flexible set of 81 separable bases (\ref{leg:fSSQT}). Cases (a)-(c) are the same as those in\,\citep{MUBExperiment}, (d) shows average results over 20 randomly generated entangled pure states. \textbf{(a)} As expected, for the maximally mixed state the choice of measurement strategy has little effect. \textbf{(b)} On the entangled state $(\vert HH\rangle+\vert VV\rangle)/\sqrt{2}$ MUB outperforms SSQT when uniformly sampled, but by allowing for adaptivity we can close the performance gap. \textbf{(c)} SSQT outperforms MUBs on the separable state $\vert HV \rangle$, but again, picking measurements adaptively the two sets perform similarly. \textbf{(d)} For random pure states a large improvement in performance is made when performing ABQT with the flexible set of separable measurements. Using this set, ABQT only needs $10^4$ measurements to achieve $\approx98.7\%$ mean fidelity for which MUB needs $10^5$ measurements. \label{fig:two_qubit_results}}
\end{figure}

\subsection{Single qubit tomography}

In our first simulated experiments we study tomography of single qubits ($D=2$).

Mixed state qubits have three real degrees of freedom and can be conveniently visualised. All physically feasible quantum states -- when $\quantumparam$ is Hermitian and have unit trace -- can be injectively mapped to points within a three dimensional ball of unit raduis, called the Bloch sphere. The surface of the Bloch sphere corresponds to pure states, the centre to the maximally mixed state $\quantumparam = \nicefrac{1}{2}I$. By convention the pure states $\vert 1 \rangle$ and $\vert 0 \rangle$ are at the North and South pole respectively. For further explanation of the Bloch sphere see \eg \citep[Chapter 2]{Petz2008}.

For illustration purposes we omit the third dimension, and only infer two remaining parameters, which will therefore lie in a unit (Bloch) disk, a slice of the original Bloch sphere. This corresponds to determining the angle of linear polarisation of a photon, assuming that the circular polarisation is zero. We allow for arbitrary projective measurements with binary ($D = 2$) outcomes. These measurements are represented as pairs of antipodal points on the perimeter of the Bloch disk. Now $\config\in[0,\pi)$ codes for the orientation of the linear filter.

Fig.\ \ref{fig:Bloch_disk} shows the progression of measurement bases chosen by ABQT. We start with a uniform prior over all allowed $\quantumparam$. The first two measurements are perpendicular. In quantum statistics these states are called mutually unbiased \citep{MUBExperiment}. The popular MUB technique for experiment design would precompute these two measurements and use them throughout the whole experiment. The third measurement is equally biased with respect to both previous bases, demonstrating that using a fixed MUB set is suboptimal in the adaptive framework. Throughout the rest of the experiment the algorithm explores a wide range of measurements, and proactively narrows down the posterior around the true underlying solution.

Fig.\ \ref{fig:qubit_results} shows that the posterior mean fidelity -- this time inferring all three coordinates in the full Bloch sphere -- improves at a faster rate when measurements are adaptively optimised. The rate is more consistent with a $n^{-1}$ law rather than $n^{-\frac{1}{2}}$ as predicted for non-adaptive methods\,\cite[][and refs.]{MUBExperiment}. Fig.\ \ref{fig:qubit_results}.b shows an even larger advantage for states of high purity (defined as sum of squared eigenvalues).

\subsection{Separable vs.\ MUB tomography of two qubits}

In multipartite systems, such as $m$-qubit registers, there are two fundamentally different classes of measurements one can apply: separable or entangling. Separable tomographic experiments are straightforward and cheap to implement, while entangling measurements are statistically more thought to be statistically more powerful. Notably, entanglement is required for implementing MUBs. These differences are discussed extensively in\,\citep{MUBExperiment}.

To investigate this trade-off in the light of adaptive tomography, we reproduce and extend the experiments in\,\citep{MUBExperiment}. This study showed that MUB outperfoms separable tomography, when measurements are selected uniformly from a predefined pool. We hope to show that when active learning is used, the advantage of MUB disappears.

Results or simulated experiments in two-qubit systems are shown in Fig.\ \ref{fig:two_qubit_results}. We compare Adaptive Bayesian Quantum Tomography to non-adaptive methods with measurements uniformly chosen amongst MUB (\ref{leg:MUB}) or standard separable quantum tomography (SSQT) bases (\ref{leg:SSQT}). For ABQT, we tested three different pools of available configurations $\configset$: picking from the same set of MUBs (\ref{leg:aMUB}), SSQT bases (\ref{leg:aSSQT}) or a more flexible set of 81 separable bases (\ref{leg:fSSQT}).

When not using active learning, we managed to reproduce the results reported in \citep{MUBExperiment}. Notably, all substantial differences between MUB and SSQT indeed vanish as we allow for adaptivity (Fig.\ \ref{fig:two_qubit_results}.a--c). Furthermore, for random pure states, we are able to realise a ten-fold improvement over MUBs when using flexible separable measurements (Fig.\ \ref{fig:two_qubit_results}.d). The results indicate that allowing for adaptivity with an imperfect, but flexible set of measurements offers greater advantages than using a fixed set of optimised measurements.

\section{Conclusions and outlook}

In summary, we have presented a new adaptive optimal experimental design framework for quantum tomography based on the BALD methodology. We showed that mutually unbiased bases, widely accepted as \emph{the} optimal measurements, represent only a partial solution and are suboptimal in the adaptive framework. Moreover, the adaptive framework applies regardless of dimensionality, and can be applied to spaces where MUBs do not even exist\,\citep{DimensionSix,ExactInformation}. This motivates a shift in experimental focus from implementing complex entangling measurements to implementing quickly reconfigurable simpler measurements.
	
The results presented here are based on simulations. In real quantum experiments involving photons, adaptive Bayesian tomography can be feasibly achieved via mechanically or electronically controlled liquid crystal wave plates. These are optical elements that can alter the polarisation state of photons that pass through them. In follow-up work, \citet The elements are rotated via electronically controlled step-motors giving rise to a large set of possible configurations $\Xe$.{Kravtsov2013} successfully implemented and tested ABQT for single-qubit photon-polarisation states using two mechanically controlled wave plates. The elements are rotated via electronically controlled step-motors giving rise to a large set of possible configurations $\Xe$ covering the whole spectrum of projective measurements.

Although our algorithm demonstrated a substantial leap forward in terms of empirical performance, it is important to keep in mind that it still does not resolve the curse of dimensionality: the size of the parameter space still scales exponentially with the number of qubits in question. Other successful approaches address the question of dimensionality by restricting the search space. Compressed sensing \citep{CompressedSensing} constrains estimation onto a lower-dimensional manifold of rank-deficient states. These simplifying assumptions and smoothness constraints can be incorporated into a Bayesian framework by ways of the prior distribution.